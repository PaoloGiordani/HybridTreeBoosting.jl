var documenterSearchIndex = {"docs":
[{"location":"Examples/#Learning-HTBoost-via-examples","page":"Examples (Julia)","title":"Learning HTBoost via examples","text":"","category":"section"},{"location":"Examples/","page":"Examples (Julia)","title":"Examples (Julia)","text":"The examples are Julia scripts that you can run. Some as similar to the tutorials, others explore additional aspects of HTBoost.","category":"page"},{"location":"Examples/","page":"Examples (Julia)","title":"Examples (Julia)","text":"A good way to familiarize yourself with HTBoost and compare (its performance to LigthGBM) is to study and run the following examples:","category":"page"},{"location":"Examples/","page":"Examples (Julia)","title":"Examples (Julia)","text":"Basic use (main options, cv, savings and loading results, variable importance and more post-estimation analysis)\nLogistic (binary classification)\nGlobal Equity Panel (time series and panels/longitudinal data, with various options for cv)\nCategoricals (how HTBoost handles categorical features)\nMissing data  (HTBoost excels at handling missing data)\nSpeeding up with large n (strategies to reduce computing time for large n)","category":"page"},{"location":"Examples/","page":"Examples (Julia)","title":"Examples (Julia)","text":"The other examples explore more specific aspects of HTBoost: ","category":"page"},{"location":"Examples/","page":"Examples (Julia)","title":"Examples (Julia)","text":"Understanding hybrid trees ","category":"page"},{"location":"Examples/","page":"Examples (Julia)","title":"Examples (Julia)","text":"Hybrid trees (how HTBoost can escape local minima of smoothtrees)\nProjection pursuit regression (an example where adding a single index model to each tree (the default in HTBoost) improves forecasting)","category":"page"},{"location":"Examples/","page":"Examples (Julia)","title":"Examples (Julia)","text":"Other distributions (loss functions)","category":"page"},{"location":"Examples/","page":"Examples (Julia)","title":"Examples (Julia)","text":"Multiclass (multiclass classification)\nZero inflated y (y≥0, continuous except for positive mass at 0)\nGammaPoisson (aka negative binomial for count data)  \nHuber and t unbiased (outlier robust losses in HTBoost and lightGBM)\nt distribution (the recommended robust loss in HTBoost)\nGamma distribution (discusses options if min(y)>0)","category":"page"},{"location":"Examples/","page":"Examples (Julia)","title":"Examples (Julia)","text":"Others","category":"page"},{"location":"Examples/","page":"Examples (Julia)","title":"Examples (Julia)","text":"Offset (exposure) (how to add an offset, common in e.g. insurance, biology ...)\nSparsity penalization (how HTBoost improves forecasting by feature selection when p is large)\nSpeedups with sparsevs (how HTBoost speeds up feature selection when p is large)","category":"page"},{"location":"tutorials/Multiclass/#Multiclass-classification-in-HTBoost","page":"-","title":"Multiclass classification in HTBoost","text":"","category":"section"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"Key points:","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"y is a vector: only one outcome is possible. (For multilabel classification, fit a sequence of :logistic models.)\nElements of y can be numerical or string. Numericals do not need to be in the format (0,1,2,...). e.g. (1,2,3,...), (22,7,48,...)   (\"a\",\"b\",\"c\",...), (\"Milan\",\"Rome\",\"Naples\",...) are all allowed.\nnum_class is detected automatically, not a user input.  \nThe output from HTBpredict( ) is:  yf,classvalues,ymax = HTBpredict(xtest,output). yf is a (ntest,numclass) matrix of probabilities, with yf[i,j] the probability that observation i takes value classvalue[j]. classvalues is a (numclass) vector of the unique values of y, sorted from smallest to largest. ymax is a (ntest) vector the most likely outcome.\nHTBoost employs a one-vs-rest approach (not a multinomial logistic).   \nThe training time is proportional to num_classes, and it can therefore be high. There is room for future improvements though, since the one-vs-rest approach can be made embarassingly parallel.  ","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"Comparison with LightGBM","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"In this example we draw simulated data from a multinomial with 3 classes, and compare HTBoost and LightGBM. The smoother the function, the more HTBoost will outperform. ","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"number_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Statistics\nusing LightGBM\n","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"Options to generate data ","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"\nRandom.seed!(12)\n\n# Options for data generation \nn         = 10_000\np         = 5      # p>=4. Only the first 4 variables are relevant\n\n# Multinomial logistic to draw data. Functions vary from very smooth (exp(1*...))\n# to only moderately smooth (exp(8*...)). \n\nf_1(x,b)    = b./(1.0 .+ (exp.(1.0*(x .- 1.0) ))) .- 0.1*b \nf_2(x,b)    = b./(1.0 .+ (exp.(2.0*(x .- 0.5) ))) .- 0.1*b \nf_3(x,b)    = b./(1.0 .+ (exp.(4.0*(x .+ 0.0) ))) .- 0.1*b\nf_4(x,b)    = b./(1.0 .+ (exp.(8.0*(x .+ 0.5) ))) .- 0.1*b\n\nb1v = [0.0,1.0,1.0,0.3,0.3]   # coefficients first class\nb2v = [0.0,0.3,0.3,1.0,1.0]   # coefficients second class \n","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"Options for HTBoost. loss = :multiclass. The number of classes is not a user's input.  We set nfold=1 and nofullsample=true for fair comparison to LightGBM. ","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"\nloss      = :multiclass \nmodality  = :compromise   # :accurate, :compromise (default), :fast, :fastest\n\nnfold     = 1             \nnofullsample = true \nverbose     = :Off \n","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"Simulate data.","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"\n# generates data from a multinomials with 3 classes \nfunction rnd_3classes(x,b1v,b2v)\n\n  class_values = [0.0,1.0,2.0]\n#  class_values = [1.0,2.0,3.0]     # HTBoost works. LightGBM needs converting to 0,1,2 ...\n#  class_values = [\"orange\",\"apple\",\"mango\"]     # HTBoost works. LightGBM needs converting to 0,1,2 ...\n\n  b0,b1,b2,b3,b4 = b1v[1],b1v[2],b1v[3],b1v[4],b1v[5]\n  d1 = b0 .+ f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4)\n\n  b0,b1,b2,b3,b4 = b2v[1],b2v[2],b2v[3],b2v[4],b2v[5]\n  d2 = b0 .+ f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4)\n\n  sumexpd = @. exp(d1) + exp(d2) + 1\n  prob1 = @. exp(d1)/sumexpd \n  prob2 = @. exp(d2)/sumexpd \n  \n  n  = size(x,1)\n  y  = Vector{eltype(class_values)}(undef,n)\n  u  = rand(n)\n\n  for i in eachindex(y)\n    u[i] < prob1[i] ? y[i] = class_values[1] : (u[i] < prob1[i]+prob2[i] ? y[i] = class_values[2] : y[i] = class_values[3]) \n  end   \n\n  return y \nend \n\n\nn_test = 100_000\nx,x_test = randn(n,p), randn(n_test,p)\ny        = rnd_3classes(x,b1v,b2v)\ny_test   = rnd_3classes(x_test,b1v,b2v)\n","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"Fit HTBoost. Notice the additional output of HTBpredict( ) Compare predictions using the (minus) multinomial log-likelihood and the hit rate.","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"\n# fit HTBoost\nparam  = HTBparam(loss=loss,nfold=nfold,nofullsample=nofullsample,verbose=verbose,modality=modality)\ndata   = HTBdata(y,x,param)\noutput = HTBfit(data,param)\n\nyf,class_values,ymax = HTBpredict(x_test,output) \nhit_rate = mean(ymax .== y_test)\n\nloss     = HTBmulticlass_loss(y_test,yf,param.class_values)\n\nprintln(\"\\n HTBoost hit rate $hit_rate and loss $loss \")\n","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"Fit LightGBM. Notice the additional output of HTBpredict( )","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"\n# lightGBM \nestimator = LGBMClassification(   # LGBMRegression(...)\n    objective = \"multiclass\",\n    num_class = 3,\n    categorical_feature = Int[],\n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    metric = [\"multi_logloss\"],\n    num_threads = number_workers\n )\n\nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n\n# parameter search over num_leaves and max_depth\nsplits = (collect(1:n_train),collect(1:min(n_train,100)))  \n\nparams = [Dict(:num_leaves => num_leaves,\n           :max_depth => max_depth) for\n      num_leaves in (4,16,32,64,127,256),\n      max_depth in (2,3,5,6,8)]\n\nlightcv = LightGBM.search_cv(estimator,x,y,splits,params,verbosity=-1)\n\nloss_cv = [lightcv[i][2][\"validation\"][estimator.metric[1]][1] for i in eachindex(lightcv)]\nminind = argmin(loss_cv)\n\nestimator.num_leaves = lightcv[minind][1][:num_leaves]\nestimator.max_depth  = lightcv[minind][1][:max_depth]\n\n# fit at cv parameters\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\nyf_gbm = LightGBM.predict(estimator,x_test)   # (n_test,num_class) \n\n# from probabilities, compute the most likely outcome. \nymax_gbm = zeros(eltype(class_values),n_test)\n\nfor i in eachindex(ymax)\n  ymax_gbm[i] = class_values[argmax(yf_gbm[i,:])]\nend     \n\nhit_rate = mean(ymax_gbm .== y_test)\nloss     = HTBmulticlass_loss(y_test,yf_gbm,param.class_values)\n\nprintln(\"\\n LightGBM hit rate $hit_rate and loss $loss \")\n","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"HTBoost in this case has a substantially lower loss, which is not surprising given that the simulated data has substantial smoothness. ","category":"page"},{"location":"tutorials/Multiclass/","page":"-","title":"-","text":"HTBoost hit rate 0.5263 and loss 96175.13\nLightGBM hit rate 0.52407 and loss 96276.94","category":"page"},{"location":"examples/Speedups_with_sparsevs/#How-HTBoost-speeds-up-training-with-large-p","page":"How HTBoost speeds up training with large p","title":"How HTBoost speeds up training with large p","text":"","category":"section"},{"location":"examples/Speedups_with_sparsevs/","page":"How HTBoost speeds up training with large p","title":"How HTBoost speeds up training with large p","text":"Short description:","category":"page"},{"location":"examples/Speedups_with_sparsevs/","page":"How HTBoost speeds up training with large p","title":"How HTBoost speeds up training with large p","text":"We explore how sparsevs affects speed and accuracy.","category":"page"},{"location":"examples/Speedups_with_sparsevs/","page":"How HTBoost speeds up training with large p","title":"How HTBoost speeds up training with large p","text":"Extensive description: ","category":"page"},{"location":"examples/Speedups_with_sparsevs/","page":"How HTBoost speeds up training with large p","title":"How HTBoost speeds up training with large p","text":"Sparsevs is used by HTBoost to speed up HTBoost with large number of features (p>>100). The idea is to to store, at predetermined intervals (a Fibonacci sequence in default), the  ten (or other number: param.numberbestfeatures) features that had the lowest loss in each split of the tree. For example, for a tree of depth 4, between 10 and 40 features will be stored in this group of bestfeatures at the first update (10 if the same features have the lowest loss at each split). In the next tree, only the features in this group will be considered as candidates for splitting, saving time for large p. At the next predetermined update, the best features are added to this group. Dichotomous features (i.e. dummies, taking only two values) are always included, since much faster. Since features never leave the group of bestfeatures, this group can get large if the environment is dense, and will stay small if the environment is sparse. Large speed-ups gains are therefore not guaranteed, but the forecasting accuracy should not be strongly affected except in extreme cases where several hundred features are needed to accurately fit the data. To prevent loss of fit, an automatic warning is issued if the size of the group of bestfeatures is over 50% the largest theoretical size (output.ratioactual_max>0.5).","category":"page"},{"location":"examples/Speedups_with_sparsevs/","page":"How HTBoost speeds up training with large p","title":"How HTBoost speeds up training with large p","text":"The speed-ups gains are typically smaller than may be expected, due to the fact that i) parallelization becomes more efficient for larger p/number_workers (where p is the number of candidate features at a given split, and p*<p if sparsevs = :On ), and ii) there is a fixed cost for refineOptim  (refine otimization of μ,τ,m given i). Speed-ups are larger for very large p (e.g. p=2000)","category":"page"},{"location":"examples/Speedups_with_sparsevs/","page":"How HTBoost speeds up training with large p","title":"How HTBoost speeds up training with large p","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Statistics\nusing LightGBM\n\n# USER'S OPTIONS \nRandom.seed!(123)\n\n# Options for data generation \nn         = 1_000\np         = 1_000        # number of features \nstde      = 1            \n\n# Options for HTBoost: modality is the key parameter guiding hyperparameter tuning and learning rate.\n# :fast and :fastest only fit one model at default parameters, while :compromise and :accurate perform\n# automatic hyperparameter tuning. \n\nsparsevs         = :On\nmodality         = :compromise   # :accurate, :compromise (default), :fast, :fastest\n\nnumber_best_features = 10  # Default 10. <10 to consider fewer features in each split (larger speed gains, less precision)\nfrequency_update     = 1       # Default 1. >1 to update less frequently (larger speed gains, less precision)\n\nnfold            = 1     # 1 for fair comparison with LightGBM\nnofullsample     = true  # true for fair comparison with LightGBM\n\nverbose          = :Off\nwarnings         = :On\n\nntrees           = 1000   # maximum number of trees \n\n# function: Friedman of linear, with pstar relevant features.\n# Increasing the number of relevant features increases the difficulty of the problem and can be\n# used to evaluate speed gains and accuracy losses of sparsevs.\n\nFriedman_function(x) = 10.0*sin.(π*x[:,1].*x[:,2]) + 20.0*(x[:,3].-0.5).^2 + 10.0*x[:,4] + 5.0*x[:,5]\n\np_star    = 10       # number of relevant features \nβ = randn(p_star)\ndgp(x)  = x[:,1:length(β)]*β\n\n# END USER'S INPUTS \n\nif dgp==Friedman_function\n    x,x_test = rand(n,p), rand(200_000,p)    # Friedman function on U(0,1)\nelse \n    x,x_test = randn(n,p), randn(200_000,p)    \nend     \n\nf       = dgp(x)\ny      = f + stde*randn(n)\nf_true = dgp(x_test)\n\n# LightGBM\n\n# Create an estimator with the desired parameters—leave other parameters at the default values.\nestimator = LGBMRegression(   # LGBMRegression(...)\n    objective = \"regression\",\n    categorical_feature = [],  # or [1,2,3,5,6,7,8], treating date as a category, as probably Lightgbm would.\n    num_iterations = ntrees,   # default 100\n    learning_rate = 0.1,      # default 0.1\n    early_stopping_round = 50,  # default 0, i.e. Inf\n    bagging_fraction = 1.0,\n    metric = [\"l2\"],\n    num_threads = number_workers\n)\n\nsharevalidation = 0.3\nn_train = Int(round((1-sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = y[1:n_train]\nx_val   = x[n_train+1:end,:]; y_val = y[n_train+1:end]\n\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\nyf_gbm = LightGBM.predict(estimator,x_test)\n\n\n# HTBoost\n\nparam   = HTBparam(modality=modality,ntrees=ntrees,sparsevs=sparsevs,\n                    frequency_update=frequency_update,number_best_features=number_best_features,\n                    nfold=nfold,verbose=:Off,warnings=:On)\n\ndata  = HTBdata(y,x,param)\n\nprintln(\"\\n n = $n, p = $p, modality = $modality, sparsevs = $sparsevs, frequency_update = $frequency_update\")\nprintln(\" time to fit \")\n\n@time output = HTBfit(data,param);\nyf = HTBpredict(x_test,output,predict=:Ey)  # predict\n\nprintln(\"\\n RMSE of HTBoost from true E(y|x)                   \", sqrt(mean((yf-f_true).^2)) )\nprintln(\" RMSE of LightGBM (default param) from true E(y|x)     \", sqrt(mean((yf_gbm-f_true).^2)) )\n\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data,verbose=false);\nprintln(\" HTBoost number of included features in final model $(sum(fi.>0))\")\n\nif param.sparsevs == :On\n    @info \" sparsevs is :On. Repeat with sparsevs=:Off to track speed gains and any accuracy loss. Speed gains will be smaller if many features are relevant.\"\nelse \n    @info \" sparsevs is :Off. Repeat with sparsevs=:On to track speed loss and any accuracy gains. Speed gains will be smaller if many features are relevant.\"\nend     \n","category":"page"},{"location":"tutorials_R/Basic_use/#Basic-use","page":"-","title":"Basic use","text":"","category":"section"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Summary","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Illustrates use of the main functions on a regression problem with simulated data. \nparam.modality as the most important user's choice, depending on time budget. \nIn default modality, HTBoost performs automatic hyperparameter tuning.\nThe R bindings use JuliaConnectoR, which requires Julia to be installed. Please read Installation in R.\nA few tutorials are provided for R. For more tutorials and examples, see Julia tutorials and Julia examples, using the guidelines in Installation in R to adapt the code to R. ","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Main points ","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"default loss is \"L2\". Other options for continuous y are \"Huber\", \"t\" (recommended in place of \"Huber\"), \"gamma\", \"gammaPoisson\", \"L2loglink\". For zero-inflated continuous y, options are \"hurdleGamma\", \"hurdleL2loglink\", \"hurdleL2\"   \ndefault is block cross-validation with nfolds=4: use randomizecv = TRUE to scramble the data. See Global Equity Panel for further options on cross-validation (e.g. sequential cv, or generally controlling the training and validation sets).","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"fit, with automatic hyperparameter tuning if modality is :compromise or :accurate\nsave fitted model (upload fitted model)\naverage τ (smoothness parameter), which is also plotted. (Smoother functions ==> larger gains compared to other GBM)\nfeature importance\npartial effects plots","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Install and load required packages. More detailed explanations in steps 1-4 in Installation in R. (Note: Julia must be installed )","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"\n# ensure that R can find the path to julia.exe\njulia_path = \"C:\\\\Users\\\\.julia\\\\juliaup\\\\julia-1.11.2+0.x64.w64.mingw32\\\\bin\"  # replace with your path to Julia\nSys.setenv(JULIA_BINDIR = julia_path)\n\nif (!require(JuliaConnectoR)) {\n  install.packages(\"JuliaConnectoR\")\n}\n\nlibrary(JuliaConnectoR)\n\n# install packages in Julia (if needed) without leaving R\njuliaEval('using Pkg; Pkg.add(\"Distributed\")')\njuliaEval('using Pkg; Pkg.add(\"DataFrames\")')\n#To install HTBoost from the registry, use the following command.\n#juliaEval('using Pkg; Pkg.add(\"HTBoost\")')\n# Alternatively, this will work even if the package is not in the registry.\njuliaEval('using Pkg; Pkg.add(\"https://github.com/PaoloGiordani/HTBoost.jl\")')\n\n# load packages \nHTBoost = juliaImport(\"HTBoost\")\nDataFrames = juliaImport(\"DataFrames\")   \n\n","category":"page"},{"location":"tutorials_R/Basic_use/#Set-the-desired-number-of-workers-(cores)-to-be-used-in-parallel.","page":"-","title":"Set the desired number of workers (cores) to be used in parallel.","text":"","category":"section"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"This step is not required by other GMBs, which rely on shared parallelization.   Note: the first run of HTBoost (or any julia script) includes compile time, which increases in the number of workers and can be quite high. It is 80'' for 8 workers on my machine. HTBoost parallelizes well up to 8 cores, and quite well up to 16 if p/#cores is sufficiently high. ","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"juliaEval('\n           number_workers  = 8  # desired number of workers, e.g. 8\n           using Distributed\n           nprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n           @everywhere using HybridTreeBoosting\n           ')","category":"page"},{"location":"tutorials_R/Basic_use/#End-of-preliminary-steps-(required-in-all-scripts).-Now-we-generate-data.","page":"-","title":"End of preliminary steps (required in all scripts). Now we generate data.","text":"","category":"section"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"y is the sum of six additive nonlinear functions, plus Gaussian noise.","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"\nn      = 10000\np      = 6\nstde   = 1.0\nn_test = 100000\n\nf_1 = function(x,b)  b*x + 1 \nf_2 = function(x,b)  2*sin(2.5*b*x)  \nf_3 = function(x,b)  b*x^3\nf_4 = function(x,b)  b*(x < 0.5) \nf_5 = function(x,b)  b/(1.0 + (exp(4.0*x )))\nf_6 = function(x, b) {b * (x > -0.25 & x < 0.25)}\n\nb1  = 1.5\nb2  = 2.0\nb3  = 0.5\nb4  = 4.0\nb5  = 5.0\nb6  = 5.0\n\nx      = matrix(rnorm(n*p),nrow = n,ncol = p)\nx_test = matrix(rnorm(n_test*p),nrow = n_test,ncol = p)\nf      = f_1(x[,1],b1) + f_2(x[,2],b2) + f_3(x[,3],b3) + f_4(x[,4],b4) + f_5(x[,5],b5) + f_6(x[,6],b6)\nf_test = f_1(x_test[,1],b1) + f_2(x_test[,2],b2) + f_3(x_test[,3],b3) + f_4(x_test[,4],b4) + f_5(x_test[,5],b5) + f_6(x_test[,6],b6)\ny      = f + rnorm(n)*stde\n\nfnames = c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\")\n","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"When y and x are numerical matrices, we could feed them to HTBoost directly. However, a more general procedue (which allows strings), is to transform the data into a dataframe and then into a Julia DataFrame. This is done as follows (here only for x, but the same applies to y if it is not numerical).","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"\ndf     =  data.frame(x)   # create a R dataframe\ndf_test = data.frame(x_test)\nfnames = colnames(df)\n\nx = DataFrames$DataFrame(df)\nx_test = DataFrames$DataFrame(df_test)\nDataFrames$describe(x)","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Options for HTBparam( ).  ","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"I prefer to specify parameter settings separately (here at the top of the script) rather than directly in HTBparam( ), which is of course also possible.   modality is the key parameter: automatic hyperparameter tuning if modality is \"compromise\" or \"accurate\", no tuning (except of #trees) if \"fast\" or \"fastest\".     In HTBoost, it is not recommended that the user performs  hyperparameter tuning by cross-validation, because this process is done automatically if modality is \"compromise\" or \"accurate\". The recommended process is to first run in modality=\"fast\" or \"fastest\", for exploratory analysis and to gauge computing time, and then switch to \"compromise\" (default) or \"accurate\". For a tutorial on user-controlled cross-validation, see User's controlled cross-validation.","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"\nloss      = \"L2\"        # :L2 is default. Other options for regression are :L2loglink (if y≥0), :t, :Huber\nmodality  = \"fast\"      # \"accurate\", \"compromise\" (default), \"fast\", \"fastest\". The first two perform parameter tuning internally by cv.\npriortype = \"hybrid\"    # \"hybrid\" (default) or \"smooth\" to force smoothness \nnfold     = 1           # number of cv folds. 1 faster (single validation sets), default 4 is slower, but more accurate.\nnofullsample = TRUE     # if nfold=1 and nofullsample=TRUE, the model is not re-fitted on the full sample after validation of the number of trees\nrandomizecv = FALSE     # FALSE (default) to use block-cv. \nverbose     = \"Off\"","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Options for cross-validation:","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"While the default in other GBM is to randomize the allocation to train and validation sets, the default in HTBoost is block cv, which is suitable for time series and panels. Set randomizecv=true to bypass this default.  See Timeseriesand_panels for further options on cross-validation (e.g. sequential cv, or generally controlling the training and validation sets).","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"randomizecv = FALSE       # false (default) to use block-cv. \n","category":"page"},{"location":"tutorials_R/Basic_use/#Set-up-HTBparam-and-HTBdata,-then-fit.-Optionally,-save-the-model-(or-load-it).-Predict.-Print-some-information.","page":"-","title":"Set up HTBparam and HTBdata, then fit. Optionally, save the model (or load it). Predict. Print some information.","text":"","category":"section"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"param = HybridTreeBoosting$HTBparam(loss=loss,priortype=priortype,randomizecv=randomizecv,nfold=nfold,verbose=verbose,modality=modality,nofullsample=nofullsample)\n\ndata  = HybridTreeBoosting$HTBdata(y,x,param,fnames=fnames)   # fnames is optional\noutput = HybridTreeBoosting$HTBfit(data,param)\n\n# save (load) fitted model. NOTE: use the same filename as the name of the object that is being saved.\n#save(output, file = \"output\")\n#load(\"output\")\n\nyhat = HybridTreeBoosting$HTBpredict(x, output)      # Fitted values\nyf = HybridTreeBoosting$HTBpredict(x_test, output)   # Predicted values\n\ncat(\"modality =\", param$modality, \", nfold =\", nfold, \"\\n\")\ncat(\"depth =\", output$bestvalue, \", number of trees =\", output$ntrees, \"\\n\")\ncat(\"out-of-sample RMSE from truth\", sqrt(sum((yf - f_test)^2)/n_test), \"\\n\")\n","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"which prints","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"modality = fast , nfold = 1 \ndepth = 5 , number of trees = 293 \nout-of-sample RMSE from truth 0.3563147\n","category":"page"},{"location":"tutorials_R/Basic_use/#Feature-importance-and-average-smoothing-parameter-for-each-feature.","page":"-","title":"Feature importance and average smoothing parameter for each feature.","text":"","category":"section"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"tau is the smoothness parameter; lower values give smoother functions, while tau=Inf is a sharp split (tau is truncated at 40 for this function).   avgtau is a summary of the smoothness of f(x), with features weighted by their importance. avgtau_a is a vector array with the importance weighted tau for each feature.  ","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"The Julia functions do not always print nearly in R. To control printing, create a R dataframe  ","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"tuple = HybridTreeBoosting$HTBweightedtau(output,data,verbose=FALSE)  # output is a Julia tuple, which can be converted to a R list\nlist = juliaGet(tuple)\n\n# Create a data frame: features from 1 to p \ndf <- data.frame(\n  feature = list$fnames,\n  importance = list$fi,\n  avgtau = list$avgtau_a\n)\n\n# Create a data frame: features sorted by importance\ndf_sorted <- data.frame(\n  sortedindx = list$sortedindx,\n  sorted_feature = list$fnames_sorted,\n  sorted_importance = list$fi_sorted,\n  sorted_avgtau = list$avgtau_a[list$sortedindx]\n)\n\n#  print(\"\\n Variable importance and smoothness, from first to last feature (not sorted)\")\n#  print(df)\nprint(\"\\n Variable importance and smoothness, from most to least important feature\")\nprint(df_sorted)\n\ncat(\"\\n Average smoothing parameter τ is\", round(list$gavgtau, digits = 1), \".\\n\")\ncat(\"\\n In sufficiently large samples, and if modality is 'compromise' or 'accurate':\\n\")\ncat(\" - Values above 20-25 suggest very little smoothness in important features. HTBoost's performance may slightly outperform or slightly underperform other gradient boosting machines.\\n\")\ncat(\" - At 10-15 or lower, HTBoost should outperform other gradient boosting machines, or at least be worth including in an ensemble.\\n\")\ncat(\" - At 5-7 or lower, HTBoost should strongly outperform other gradient boosting machines.\\n\")","category":"page"},{"location":"tutorials_R/Basic_use/#Create-a-plot-to-visualize-the-average-smoothness-of-the-splits","page":"-","title":"Create a plot to visualize the average smoothness of the splits","text":"","category":"section"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"The plot gives an idea of the average (importance weighted) smoothness across all splits.... ","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"\n# Create a plot to visualize the average smoothness of the splits\nlibrary(ggplot2)\nlibrary(gridExtra)\n\ndf <- data.frame(x = list$x_plot, g = list$g_plot)\n\np <- ggplot(df, aes(x = list$x_plot, y = list$g_plot)) +\n  geom_line() +\n  labs(title = \"avg smoothness of splits\", x = \"standardized x\", y = \"\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 30, face = \"bold\"),\n    axis.title.x = element_text(size = 20),\n    axis.title.y = element_text(size = 20),\n    axis.text = element_text(size = 20)\n  )\n\nprint(p)\n\n","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"The plot gives an idea of the average (importance weighted) smoothness across all splits.... ","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"<img src=\"../assets/avgtau.png\" width=\"400\" height=\"250\">","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"... which in this case is a mix of very different values across features: approximate linearity for x1, smooth functions for x3 and x5, and essentially sharp splits for x2, x4, and x6. Note: Variable (feature) importance is computed as in Hastie et al., \"The Elements of Statistical Learning\", second edition, except that the normalization is for sum=100.  ","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":" Row │ feature  importance  avgtau     sorted_feature  sorted_importance  sorted_avgtau \n     │ String   Float32     Float64    String          Float32            Float64\n─────┼──────────────────────────────────────────────────────────────────────────────────\n   1 │ x1          14.5666   0.458996  x3                        19.0633       3.30638\n   2 │ x2          12.9643  19.6719    x5                        18.6942       3.72146\n   3 │ x3          19.0633   3.30638   x6                        17.9862      35.1852\n   4 │ x4          16.7254  36.0846    x4                        16.7254      36.0846\n   5 │ x5          18.6942   3.72146   x1                        14.5666       0.458996\n   6 │ x6          17.9862  35.1852    x2                        12.9643      19.6719\n\n Average smoothing parameter τ is 7.3.\n\n In sufficiently large samples, and if modality=:compromise or :accurate\n\n - Values above 20-25 suggest very little smoothness in important features. HTBoost's performance may slightly outperform or slightly underperform other gradient boosting machines.\n - At 10-15 or lower, HTBoost should outperform other gradient boosting machines, or at least be worth including in an ensemble.\n - At 5-7 or lower, HTBoost should strongly outperform other gradient boosting machines.","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Some examples of smoothness corresponding to a few values of tau (for a single split) help to interpret values of avgtau","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"<img src=\"../assets/Sigmoids.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"On simulated data, we can evaluate the RMSE from the true f(x), exluding noise:","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Hybrid trees outperform both smooth and standard trees","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Here is the output for n=10k (nfold=1, nofullsample=true).  Hybrid trees strongly outperform both smooth trees and standard symmetric (aka oblivious) trees. (Note: modality = :sharp is a very inefficient way to run a symmetric tree; use CatBoost or EvoTrees instead!)","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"  modality = fastest, nfold = 1, priortype = hybrid\n depth = 5, number of trees = 141, gavgtau 7.3\n out-of-sample RMSE from truth 0.3136\n\nmodality = fastest, nfold = 1, priortype = smooth \n depth = 5, number of trees = 121, gavgtau 4.5\n out-of-sample RMSE from truth 0.5751\n\n modality = fastest, priortype = sharp\ndepth = 5, number of trees = 183, avgtau 40.0\n out-of-sample RMSE from truth 0.5320","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Partial dependence plots","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Partial dependence assumes (in default) that other features are kept at their mean.","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data,verbose=false);\nq,pdp  = HTBpartialplot(data,output,[1,2,3,4,5,6]) # partial effects for the first 6 variables \n\n# plot partial dependence\ntuple = HybridTreeBoosting$HTBpartialplot(data,output,c(1,2,3,4,5,6))\nlist = juliaGet(tuple)   #  this is not necessary in most cases (e.g. tuple$q works fine)\n\nq  = list$q     \npdp = list$pdp\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Assuming q and pdp are matrices and f_1, f_2, ..., f_6 are functions, b1, b2, ..., b6 are parameters\nf_list <- list(f_1, f_2, f_3, f_4, f_5, f_6)\nb_list <- list(b1, b2, b3, b4, b5, b6)\n\npl <- vector(\"list\", 6)\n\nfor (i in 1:length(pl)) {\n  df <- data.frame(\n    x = q[, i],\n    HTB = pdp[, i],\n    true = f_list[[i]](q[, i], b_list[[i]]) - f_list[[i]](q[, i] * 0, b_list[[i]])\n  )\n  \n  pl[[i]] <- ggplot(df, aes(x = x)) +\n    geom_line(aes(y = HTB, color = \"HTB\"), size = 1.5) +\n    geom_line(aes(y = true, color = \"true\"), linetype = \"dotted\", size = 1) +\n    labs(title = paste(\"PDP feature\", i), x = \"x\", y = \"f(x)\") +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(size = 15),\n      legend.position = \"bottom\",\n      legend.title = element_blank(),\n      legend.text = element_text(size = 12)\n    ) +\n    scale_color_manual(values = c(\"HTB\" = \"blue\", \"true\" = \"red\"))\n}\n\n# Arrange the plots in a 3x2 grid layout\ngrid.arrange(grobs = pl, ncol = 3, nrow = 2)```\n","category":"page"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"Partial plots for n = 1k,10k,100k, with modality = :fastest and nfold = 1.    Notice how plots are smooth only for some features. ","category":"page"},{"location":"tutorials_R/Basic_use/#n-1_000","page":"-","title":"n = 1_000","text":"","category":"section"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"<img src=\"../assets/Minimal1k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"tutorials_R/Basic_use/#n-10_000","page":"-","title":"n = 10_000","text":"","category":"section"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"<img src=\"../assets/Minimal10k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"tutorials_R/Basic_use/#n-100_000","page":"-","title":"n = 100_000","text":"","category":"section"},{"location":"tutorials_R/Basic_use/","page":"-","title":"-","text":"<img src=\"../assets/Minimal100k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"examples/Offset/#Incorporating-an-offset","page":"Incorporating an offset","title":"Incorporating an offset","text":"","category":"section"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"Short description:","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"offset is added to γ (NOT multiplied by E(y|x)), where γ = link(E(y|x)). It should therefore be in logs for loss in [:L2loglink,:gamma,:Poisson,:gammaPoisson,:hurdleGamma, :hurdleL2loglink], and logit for loss in [:logistic].","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"For example, if loss = :gamma and E(y|x) = offset*f, then the offset that is fed into HTBdata() should be log(offset), data   = HTBdata(y,x,param,offset=log(offset))    ","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"Instructions ","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"Make sure that the offset vector has the same length as y and that it is appropriately transformed (so it can be added to the link-transformed E(y|x)).","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"\n    if loss in [:gamma,:L2loglink,:Poisson,:gammaPoisson,:hurdleGamma, :hurdleL2loglink]\n        γ = log.(offset)\n    else\n        γ = offset\n    end","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"Add the offset as an argument in HTBdata( )","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"data   = HTBdata(y,x,param,offset=γ)","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"If predicting, add any offset as an argument in HTBpredict( )","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"yhat  = HTBpredict(x_test,output,predict=:Ey,offset=γ_test)","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"Warning!:","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"Categorical features with more than two categories are not currently handled correctly (by the mean targeting transformation)","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"with offsets. The program will run but categorical information will be used sub-optimally, particularly if the average offset differs across categories. If categorical features are important, it may be better to omit the offset from HTBdata(), and instead model y/offset with a :L2loglink loss instead of a :gamma, :Poisson or :gammaPoisson. ","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"Some advice: ","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"Unless you are absolutely sure that an offset enters exactly with coeff 1, I recommend also adding it  as a feature. HTBoost will then be able to capture possibly subtle nonlinearities and interaction effects in E(y|offset).  ","category":"page"},{"location":"examples/Offset/","page":"Incorporating an offset","title":"Incorporating an offset","text":"\n\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Plots,Statistics\nimport Distributions \n\n# USER'S OPTIONS \n\nRandom.seed!(12)\n\n# Some options for HTBoost\nloss      = :gamma          \nmodality  = :fast       # :accurate, :compromise (default), :fast, :fastest \n\npriortype = :hybrid      # :hybrid (default) or :smooth to force smoothness \nnfold     = 1             # number of cv folds. 1 faster (single validation sets), default 5 is slower, but more accurate.\nnofullsample = true       # if nfold=1 and nofullsample=true, the model is not re-fitted on the full sample after validation of the number of trees\n \nrandomizecv = false       # false (default) to use block-cv. \nverbose     = :On\nwarnings    = :On\n\n# options to generate data.\nk           = 10         # shape parameter\nn,p,n_test  = 10_000,4,100_000\n\nf_1(x,b)    = b*x  \nf_2(x,b)    = -b*(x.<-0.5) + b*(x.>=0.5)   \nf_3(x,b)    = b*x.*(x.<0)\nf_4(x,b)    = b*(x.>=0.5)\n\nb1,b2,b3,b4 = 0.2,0.2,0.2,0.2\n\n# generate data\nx,x_test = randn(n,p), randn(n_test,p)\n\nc        = -2  \nf        = c .+ f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4)\nf_test   = c .+ f_1(x_test[:,1],b1) + f_2(x_test[:,2],b2) + f_3(x_test[:,3],b3) + f_4(x_test[:,4],b4)\n\noffset  = exp.(2*(randn(n)*std(f) .+ 0.2))  # offset here in terms of E(y|x), to be transformed later.\n\nif loss in [:gamma,:L2loglink,:Poisson,:gammaPoisson,:hurdleGamma, :hurdleL2loglink]\n    γ = log.(offset)\nelse\n    γ = offset\nend\n\nμ        = @. exp(f)*offset     # conditional mean \nμ_test   = exp.(f_test)          \n\nscale    = μ/k\nscale_test = μ_test/k\ny       = zeros(n)\n\nfor i in eachindex(y)\n    y[i]  = rand(Distributions.Gamma.(k,scale[i]))\nend \n\nhistogram(y)\n@show [mean(y), std(y), std(μ), maximum(y)]\n\n# set up HTBparam and HTBdata, then fit and predit\n\n# coefficient estimated internally. \nparam  = HTBparam(loss=loss,priortype=priortype,randomizecv=randomizecv,nfold=nfold,\n                   verbose=verbose,warnings=warnings,modality=modality,nofullsample=nofullsample)\ndata   = HTBdata(y,x,param,offset=γ)\n\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output,predict=:Ey)\n#yhat  = HTBpredict(x_test,output,predict=:Ey,offset=γ_test)\n\nprintln(\" \\n loss = $loss, modality = $(param.modality), nfold = $nfold \")\nprintln(\" depth = $(output.bestvalue), number of trees = $(output.ntrees) \")\nprintln(\" out-of-sample RMSE from truth, μ     \", sqrt(sum((yf - μ_test).^2)/n_test) )\n\nprintln(\"\\n true shape = $k, estimated = $(exp(output.bestparam.coeff_updated[1][1])) \")\nprintln(\"\\n For information about coefficients, use HTBcoeff(output) \")\nHTBcoeff(output)\n\n\n# Plot \n\nq,pdp  = HTBpartialplot(data,output,[1,2,3,4],predict=:Egamma)\n\n# plot partial dependence in terms of the natural parameter \npl   = Vector(undef,4)\nf,b  = [f_1,f_2,f_3,f_4],[b1,b2,b3,b4]\n\nfor i in 1:length(pl)\n        pl[i]   = plot( [q[:,i]],[pdp[:,i] f[i](q[:,i],b[i]) - f[i](q[:,i]*0,b[i])],\n           label = [\"HTB\" \"dgp\"],\n           legend = :bottomright,\n           linecolor = [:blue :red],\n           linestyle = [:solid :dot],\n\n           linewidth = [5 5],\n           titlefont = font(15),\n           legendfont = font(12),\n           xlabel = \"x\",\n           ylabel = \"f(x)\",\n           )\nend\n\ndisplay(plot(pl[1], pl[2], pl[3], pl[4], layout=(2,2), size=(1300,800)))  # display() will show it in Plots window.\n","category":"page"},{"location":"examples/Multiclass/#Multiclass-classification","page":"Multiclass classification","title":"Multiclass classification","text":"","category":"section"},{"location":"examples/Multiclass/","page":"Multiclass classification","title":"Multiclass classification","text":"Multiclass classification in HTBoost. Key points:","category":"page"},{"location":"examples/Multiclass/","page":"Multiclass classification","title":"Multiclass classification","text":"y is a vector: only one outcome possible. (For multilabel classification, fit a sequence of :logistic models.)\nElements of y can be numerical or string. Numericals do not need to be in the format (0,1,2,...). e.g. (1,2,3,...), (22,7,48,...)   (\"a\",\"b\",\"c\",...), (\"Milan\",\"Rome\",\"Naples\",...) are all allowed.\nnum_class is detected automatically, not a user input.  \nThe output from HTBpredict() is:  yf,classvalues,ymax = HTBpredict(xtest,output). yf is a (ntest,numclass) matrix of probabilities, with yf[i,j] the probability that observation i takes value classvalue[j]. classvalues is a (numclass) vector of the unique values of y, sorted from smallest to largest. ymax is a (ntest) vector the most likely outcome.\nThe training time is proportional to num_classes, and it can therefore be high. There is room for future improvements though, since the one-vs-rest approach is embarassingly parallel.  ","category":"page"},{"location":"examples/Multiclass/","page":"Multiclass classification","title":"Multiclass classification","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Statistics\nusing LightGBM\n\n# USER'S OPTIONS \n\nRandom.seed!(12)\n\n# Options for data generation \nn         = 10_000\np         = 5      # p>=4. Only the first 4 variables are used in the function f(x) below \n\n# Options for HTBoost: modality is the key parameter guiding hyperparameter tuning and learning rate.\n# :fast and :fastest only fit one model at default parameters, while :compromise and :accurate perform\n# automatic hyperparameter tuning. \n\nloss      = :multiclass \nmodality  = :compromise   # :accurate, :compromise (default), :fast, :fastest\npriortype = :hybrid\n\nnfold     = 1             # nfold=1 and nofullsample=true for fair comparison to LightGBM\nnofullsample = true \n\nverbose    = :Off \nwarnings   = :On\n\n# Multinomial logistic to draw data.\n\nf_1(x,b)    = b./(1.0 .+ (exp.(1.0*(x .- 1.0) ))) .- 0.1*b \nf_2(x,b)    = b./(1.0 .+ (exp.(2.0*(x .- 0.5) ))) .- 0.1*b \nf_3(x,b)    = b./(1.0 .+ (exp.(4.0*(x .+ 0.0) ))) .- 0.1*b\nf_4(x,b)    = b./(1.0 .+ (exp.(8.0*(x .+ 0.5) ))) .- 0.1*b\n\nb1v = [0.0,1.0,1.0,0.3,0.3]   # coefficients first class\nb2v = [0.0,0.3,0.3,1.0,1.0]   # coefficients second class \n\n# END USER'S OPTIONS  \n\n# generates data from 3 classes \nfunction rnd_3classes(x,b1v,b2v)\n\n  class_values = [0.0,1.0,2.0]\n#  class_values = [1.0,2.0,3.0]     # HTBoost works. LightGBM needs converting to 0,1,2 ...\n#  class_values = [\"orange\",\"apple\",\"mango\"]     # HTBoost works. LightGBM needs converting to 0,1,2 ...\n\n  b0,b1,b2,b3,b4 = b1v[1],b1v[2],b1v[3],b1v[4],b1v[5]\n  d1 = b0 .+ f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4)\n\n  b0,b1,b2,b3,b4 = b2v[1],b2v[2],b2v[3],b2v[4],b2v[5]\n  d2 = b0 .+ f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4)\n\n  sumexpd = @. exp(d1) + exp(d2) + 1\n  prob1 = @. exp(d1)/sumexpd \n  prob2 = @. exp(d2)/sumexpd \n  \n  n  = size(x,1)\n  y  = Vector{eltype(class_values)}(undef,n)\n  u  = rand(n)\n\n  for i in eachindex(y)\n    u[i] < prob1[i] ? y[i] = class_values[1] : (u[i] < prob1[i]+prob2[i] ? y[i] = class_values[2] : y[i] = class_values[3]) \n  end   \n\n  return y \nend \n\n# generate data\nn_test = 100_000\nx,x_test = randn(n,p), randn(n_test,p)\ny        = rnd_3classes(x,b1v,b2v)\ny_test   = rnd_3classes(x_test,b1v,b2v)\n\n# fit HTBoost\nparam  = HTBparam(loss=loss,priortype=priortype,nfold=nfold,\n                   verbose=verbose,warnings=warnings,modality=modality,nofullsample=nofullsample)\n    \ndata   = HTBdata(y,x,param)\n\noutput = HTBfit(data,param)\nyf,class_values,ymax = HTBpredict(x_test,output)\nhit_rate = mean(ymax .== y_test)\n\nloss     = HTBmulticlass_loss(y_test,yf,param.class_values)\n\nprintln(\"\\n HTBoost hit rate $hit_rate and loss $loss \")\n\n# lightGBM \nestimator = LGBMClassification(   # LGBMRegression(...)\n    objective = \"multiclass\",\n    num_class = 3,\n    categorical_feature = Int[],\n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    metric = [\"multi_logloss\"],\n    num_threads = number_workers\n )\n\nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n\n# parameter search over num_leaves and max_depth\nsplits = (collect(1:n_train),collect(1:min(n_train,100)))  # goes around the problem that at least two training sets are required by search_cv (we want the first)\n\nparams = [Dict(:num_leaves => num_leaves,\n           :max_depth => max_depth) for\n      num_leaves in (4,16,32,64,127,256),\n      max_depth in (2,3,5,6,8)]\n\nlightcv = LightGBM.search_cv(estimator,x,y,splits,params,verbosity=-1)\n\nloss_cv = [lightcv[i][2][\"validation\"][estimator.metric[1]][1] for i in eachindex(lightcv)]\nminind = argmin(loss_cv)\n\nestimator.num_leaves = lightcv[minind][1][:num_leaves]\nestimator.max_depth  = lightcv[minind][1][:max_depth]\n\n# fit at cv parameters\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\nyf_gbm = LightGBM.predict(estimator,x_test)   # (n_test,num_class) \n\nymax_gbm = zeros(eltype(class_values),n_test)\n\nfor i in eachindex(ymax)\n  ymax_gbm[i] = class_values[argmax(yf_gbm[i,:])]\nend     \n\nhit_rate = mean(ymax_gbm .== y_test)\nloss     = HTBmulticlass_loss(y_test,yf_gbm,param.class_values)\n\nprintln(\"\\n LightGBM hit rate $hit_rate and loss $loss \")\n\n","category":"page"},{"location":"examples/Sparsity_penalization/#Finding-sparsity","page":"Finding sparsity","title":"Finding sparsity","text":"","category":"section"},{"location":"examples/Sparsity_penalization/","page":"Finding sparsity","title":"Finding sparsity","text":"Short description.","category":"page"},{"location":"examples/Sparsity_penalization/","page":"Finding sparsity","title":"Finding sparsity","text":"HTBoost has an in-built penalization encouraging a sparse representation. This penalization is very mild in modality = :fast, which is close to standard boosting.  When modality=:compromise or modality=:accurate, the amount of penalization is automatically cross-validated, ranging from none (standard boosting) to quite strong (to capture very sparse representations.) Note: this cv is not performed if the effective sample size (which depends on n, loglikvide, and var(yhat)/var(y)) is large compared to the number of features p.","category":"page"},{"location":"examples/Sparsity_penalization/","page":"Finding sparsity","title":"Finding sparsity","text":"Further notes and comments.","category":"page"},{"location":"examples/Sparsity_penalization/","page":"Finding sparsity","title":"Finding sparsity","text":"HTB's approach to sparsity builds on Xu et al. 2019, \"Gradient Boosted Feature Selection\", designed for the n>>p case,  implemented by penalizing the introduction of any feature not previously selected, with some important innovations.  Since HTBoost is much slower than other GBMs, it is essential for cross-validation to require only a handful of evaluations. This is not possible in the representation of Xu et al., where the penalization has no obvious range, and a grid from 0.125 to 500 is used. HTBoost normalizes the penalization by taking into account the number of features as well as their nature (continuous or binary), which allows for a more focused search of a few values in a narrow range. ","category":"page"},{"location":"examples/Sparsity_penalization/","page":"Finding sparsity","title":"Finding sparsity","text":"In the example below, only a small subset p_star of features are relevant. All features are dichotomous, and they don't interact, so there is no smoothness for  HTBoost to take advantage of, but HTBoost outperforms a version without sparsity penalization (especially in modality=:compromise or :accurate).","category":"page"},{"location":"examples/Sparsity_penalization/","page":"Finding sparsity","title":"Finding sparsity","text":"Sparsity penalization is most useful with small n, very large p, or low SNR, as GBM are already quite good at selecting away redundant features. ","category":"page"},{"location":"examples/Sparsity_penalization/","page":"Finding sparsity","title":"Finding sparsity","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Statistics\nusing LightGBM\n\n# USER'S OPTIONS \nRandom.seed!(1)\n\n# Options for data generation \nn         = 1_000\np         = 500       # number of features \np_star    = 20        # number of relevant features (p_star < p )\nstde      = 1            \n\n# Options for HTBoost\nmodality = :compromise  \n\ndepth     = 2     # depth fixed to speed up computations\nnfold     = 4     # nfold=1 is not adequate in a small n situation. Leave at default (4)\n\nverbose          = :Off\nwarnings         = :On\n\nβ = randn(p_star)\nf_dgp(x,β) = x[:,1:length(β)]*β\n\n# END USER'S INPUTS \n\nx,x_test = randn(n,p),randn(200_000,p)    \n\ny      = f_dgp(x,β) + stde*randn(n)\nf_true = f_dgp(x_test,β)\n\n# LightGBM\n\nestimator = LGBMRegression(   # LGBMRegression(...)\n    objective = \"regression\",\n    num_iterations = 1000,   # default 100\n    learning_rate = 0.1,    # default 0.1\n    early_stopping_round = 100,  \n    metric = [\"l2\"],\n    num_threads = number_workers,\n    max_depth = depth         # setting it to 1 in this experiment only \n)\n\nsharevalidation = 0.3\nn_train = Int(round((1-sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = y[1:n_train]\nx_val   = x[n_train+1:end,:]; y_val = y[n_train+1:end]\n\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\n\nyf_gbm = LightGBM.predict(estimator,x_test)[:,1]   # (n_test,num_class) \n\n# HTBoost\n\nparam   = HTBparam(modality=modality,verbose=:Off,warnings=:On,depth=depth,nfold=nfold)\ndata  = HTBdata(y,x,param)\n\noutput = HTBfit(data,param,cv_grid=[depth]);\nyf = HTBpredict(x_test,output,predict=:Ey)  # predict\n\nprintln(\"\\n RMSE of HTBoost from true E(y|x)                   \", sqrt(mean((yf-f_true).^2)) )\nprintln(\" RMSE of LightGBM (default param) from true E(y|x)     \", sqrt(mean((yf_gbm-f_true).^2)) )\n\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data,verbose=false);\nprintln(\" HTBoost number of included features in final model $(sum(fi.>0))\")\n\n# HTBoost without scarcity penalization\n\nparam   = HTBparam(modality=modality,verbose=:Off,warnings=:On,depth=depth,nfold=nfold,\n                    sparsity_penalization = 0)\n\noutput = HTBfit(data,param,cv_grid=[depth],cv_sparsity=false);\nyf = HTBpredict(x_test,output,predict=:Ey)  # predict\n\nprintln(\"\\n RMSE of HTBoost without sparsity penalization      \", sqrt(mean((yf-f_true).^2)) )\n\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data,verbose=false);\nprintln(\" HTBoost without sparsity penalization: #included features  $(sum(fi.>0))\")\n","category":"page"},{"location":"Parameters/","page":"Parameters","title":"Parameters","text":"Note: all Julia symbols can be replaced by strings. e.g. loss=:L2 can be replaced by loss=\"L2\".","category":"page"},{"location":"Parameters/#Parameters-more-likely-to-be-modified-by-user","page":"Parameters","title":"Parameters more likely to be modified by user","text":"","category":"section"},{"location":"Parameters/","page":"Parameters","title":"Parameters","text":"loss             [:L2] Supported distributions:\n:L2 (Gaussian)\n:logistic (binary classification)\n:multiclass (multiclass classification)\n:t (student-t, robust alternative to :L2)\n:Huber \n:gamma \n:Poisson (count data)\n:gammaPoisson (aka negative binomial, count data)\n:L2loglink (alternative to :L2 if y≥0)\n:lognormal (L2 on log(y))\n:hurdleGamma (zero-inflated y)\n:hurdleL2loglink (zero-inflated y)\n:hurdleL2 (zero-inflated y)\nSee the examples and tutorials for uses of each loss function. Fixed coefficients (such as shape for :gamma, dispersion and dof for :t, and overdispersion for :gammaPoisson) are computed internally by maximum likelihood. Inspect them using HTBcoeff().","category":"page"},{"location":"Parameters/","page":"Parameters","title":"Parameters","text":"In HTBpredict(), predictions are for E(y) if predict=:Ey (default), while predict=:Egamma forecasts the fitted parameter ( E(logit(prob) for :logistic, log(E(y)) for :gamma etc ... )","category":"page"},{"location":"Parameters/","page":"Parameters","title":"Parameters","text":"modality         [:compromise]  Options are: :accurate, :compromise (default), :fast, :fastest.                      :fast and :fastest run only one model, while :compromise and :accurate cross-validate the most important parameters.                    :fast runs only one model (only cv number of trees) at values defined in param = HTBparam().                     :fastest runs only one model, setting lambda=0.2, nfold=1 and nofullsample=true (does not re-estimate on the full sample after cv).                     Recommended for faster preliminary analysis only.                     In most cases, :fast and :fastest also use the quadratic approximation to the loss for large samples.                     :accurate cross-validates 7-10 models (at the most important parameters (see HTBfit() for details),                     then stacks all the cv models. :compromise cv 4-7 models.  \nrandomizecv       [false] default is block-cv (aka purged cv); a time series or panel structure is automatically detected (see HTBdata())                           if a date column is provided. Set to true for standard cv.\nnfold              [4] n in n-fold cv. Set nfold = 1 for a single validation set (by default the last param.sharevalidation share of the sample).                           nfold, sharevalidation, and randomizecv are disregarded if train and test observations are provided by the user.\nsharevalidation:        [0.30] Can be: a) Integer, size of the validation set, or b) Float, share of validation set.                           Relevant only if nfold = 1. If randomizecv = false (default), the validation set is compromised of the last x% observations, else a random subsample. \nindtrain_a:Vector{Vector{I}} [ ] for user's provided array of indices of train sets. e.g. vector of 5 vectors, each with indices of train set observations\nindtest_a:Vector{Vector{I}}  [ ] for user's provided array of indices of test sets. e.g. vector of 5 vectors, each with indices of train set observations. \nnofullsample      [false] if true and nfold=1, HTBoost is not re-estimated on the full sample after validation.                           Reduces computing time by roughly 60%, at the cost of a modest loss of accuracy.                           Useful for very large datasets, in preliminary analysis, in simulations, and when instructions specify a train/validation                           split with no re-estimation on full sample. Activated by default when modality=:fastest.     \ncat_features            [ ] vector of indices of categorical features, e.g. [2,5], or vector of names in DataFrame,                           e.g. [:wage,:age] or [\"wage\",\"age\"]. If empty, categoricals are automatically detected as non-numerical features.\noverlap:            [0] number of overlaps in time series and panels. Typically overlap = h-1, where y(t) = Y(t+h)-Y(t). Used for purged-cv.\nverbose         [:Off] verbosity :On or :Off\nwarnings        [:On] or :Off","category":"page"},{"location":"Parameters/#Parameters-less-frequently-modified-by-user","page":"Parameters","title":"Parameters less frequently modified by user","text":"","category":"section"},{"location":"Parameters/","page":"Parameters","title":"Parameters","text":"priortype               [:hybrid] Options are: :hybrid, :smooth, :disperse.  ","category":"page"},{"location":"Parameters/","page":"Parameters","title":"Parameters","text":":hybrid encourages smoothness, but allows both smooth and sharp splits, :smooth forces smooth splits,                             :disperse is :hybrid but with no penalization encouraging smooth functions (not recommended in most cases).                             Set to :smooth if you want to force derivatives to be defined everywhere, but note that this disengages hybrid trees and can lead to substantial loss of accuracy if the function is not smooth everywhere. ","category":"page"},{"location":"Parameters/","page":"Parameters","title":"Parameters","text":"lambda           [0.1 or 0.2] Learning rate. 0.1 for (nearly) best performance. 0.2 can be almost as accurate, particularly if the function is smooth and p is small.                    The default is 0.1, except in modality = :fastest, where it's 0.2.                    Consider 0.05 if tiny improvements in accuracy are important and computing time is not a concern (and possibly increase ntrees to 4000)\ndepth              [5] tree depth. Unless modality = :fast or :fastest, this is over-written as depth is cross-validated. See HTBfit() for more options.\nweights                 NOTE: weights for weighted likelihood are set in HTBdata(), not in HTBparam().\noffset                  NOTE: offsets (aka exposures) are set in HTBdata(), not in HTBparam(). See offset tutorial and offset example    \nsparsity_penalization   [0.3] positive numbers encourage sparsity. The range [0.0-1.5] should cover most scenarios.                            Automatically cv in modality=:compromise and :accurate. Increase to obtain a more parsimonious model, set to 0 for standard boosting.\ndepth              [5] tree depth. Unless modality = :fast or :fastest, this is over-written as depth is cross-validated. See HTBfit() for more options.\nntrees             [2000] Maximum number of trees. HTBfit() will automatically stop when cv loss stops decreasing.\nsharevs                 [1.0] row subsampling in variable selection phase (only to choose feature on which to split.) Default is no subsampling.                           sharevs = :Auto sets the subsample size to min(n,50k*sqrt(n/50k)).                           At high n, sharevs<1 speeds up computations, but can reduce accuracy, particularly in sparse setting with low SNR.         \nsubsampleshare_columns  [1.0] column subsampling (aka feature subsampling) by tree.\nmin_unique              [:default] sharp splits are imposed on features with less than min_unique values (default is 5 for modality=:compromise or :accurate, else 10)\nmixed_dc_sharp          [false] true to force sharp splits on discrete and mixed discrete-continuous features (defined as having over 20% obs on a single value)\ndelete_missing          [false] true to delete rows with missing values in any feature, false to handle missing internally (recommended).\ntheta                   [1]  numbers larger than 1 imply tighter penalization on β (final leaf values) compared to default.\nmeanlntau        [1.0] prior mean of log(τ). Set to higher numbers to suggest less smooth functions.        \nmugridpoints       [11] number of points at which to evaluate μ during variable selection. 5 is sufficient on simulated data with normal or uniform distributions, but actual data may benefit from more (due to with highly non-Gaussian features).                           For extremely complex and nonlinear features, more than 10 may be needed. This number is automatically reduced at deep levels of the tree (higher than 5 in default).       \nforce_sharp_splits      [ ] optionally, a p vector of Bool, with j-th value set to true if the j-th feature is forced to enter with a sharp split.\nforce_smooth_splits     [ ] optionally, a p vector of Bool, with j-th value set to true if the j-th feature is forced to enter with a smooth split (high values of τ not allowed).\ncat_representation_dimension  [3] 1 for mean encoding, 2 also adds frequency, 3 also adds variance.\nlosscv                  [:default] loss function for cross-validation. The default is the same loss type used for the training set. Other options are (:mse,:mae,:logistic,:sign).\nn_refineOptim      [10^6] maximum number of observations to use fit μ and τ (split point and smoothness).                           Lower numbers can provide speed-ups with very large n at some cost in terms of fit.\nloglikdivide         [1.0] Higher numbers increase the strength or all priors. The defaults sets it internally using HTBloglikdivide(),                           when it detects a dates series in HTBdata().\ntau_threshold         [10.0] lowest threshold for imposing sharp splits. Lower numbers give more sharp splits.\nmultiplier_stdtau    [5.0] The default priors suggest smoother splits on features whose unconditional distribution (appropriately transformed according to the link function) is closer to the unconditional distribution of y or, when not applicable, to a Gaussian. To disengage this feature, set multiplier_stdtau = 0","category":"page"},{"location":"Parameters/","page":"Parameters","title":"Parameters","text":"Additional parameters to control the parameter tuning process can be set in HTBfit(), but keeping the defaults is generally encouraged.","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/#Installation-and-Introduction-to-the-R-bindings-of-HTBoost.jl","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/#Install-Julia","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Install Julia","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"Ensure Julia is installed.  Julia can be downloaded from (https://julialang.org/downloads/)","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/#Each-script-should-start-with-the-following-4-steps","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Each script should start with the following 4 steps","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/#1.-Ensure-that-the-Julia-executable-is-in-the-system-search-path.","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"1. Ensure that the Julia executable is in the system search path.","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"In most cases this should not be necessary, but if R cannot find the path:  ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"To find the path to the Julia executable, run the following command in Julia: julia Sys.BINDIR\nSet the path at the start of your R script (only the path, excluding julia.exe)","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"julia_path = \"C:\\\\Users\\\\.julia\\\\juliaup\\\\julia-1.11.2+0.x64.w64.mingw32\\\\bin\"  # replace with your path\nSys.setenv(JULIA_BINDIR = julia_path)","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/#2.-Install-JuliaConnectoR-package-(if-not-already-installed)-and-load-it","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"2. Install JuliaConnectoR package (if not already installed) and load it","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"if (!require(JuliaConnectoR)) {\n  install.packages(\"JuliaConnectoR\")\n}\n\nlibrary(JuliaConnectoR)","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/#3.-Install-the-Julia-packages-Distributed,-DataFrames,-HTBoost-(if-not-already-installed),-and-load-them.","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"3. Install the Julia packages Distributed, DataFrames, HTBoost (if not already installed), and load them.","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"Installation is needed only once, and can be done from Julia or in R using the JuliaConnectoR package as follows:","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"juliaEval('using Pkg; Pkg.add(\"Distributed\")')\njuliaEval('using Pkg; Pkg.add(\"DataFrames\")')","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"To install HTBoost from the registry, use the following command.","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"juliaEval('using Pkg; Pkg.add(\"HybridTreeBoosting\")')","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"Alternatively, this will work even if the package is not in the registry.","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"juliaEval('using Pkg; Pkg.add(\"https://github.com/PaoloGiordani/HybridTreeBoosting.jl\")')","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"Load the packages ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"HTBoost = juliaImport(\"HybridTreeBoosting\")\nDataFrames = juliaImport(\"DataFrames\")   \n#RData = juliaImport(\"RData\")    # not required by HTBoost, can be convenient to work with R datasets in Julia","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/#4.-Set-the-desired-number-of-workers-(cores)-to-be-used-in-parallel.","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"4. Set the desired number of workers (cores) to be used in parallel.","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"juliaEval('\n           number_workers  = 8  # desired number of workers, e.g. 8\n           using Distributed\n           nprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n           @everywhere using HybridTreeBoosting\n           ')","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/#Running-HTBoost","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Running HTBoost","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"The most important functions in HTBoost are","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"param  = HybridTreeBoosting$HTBparam()           # defines the model     \ndata   = HybridTreeBoosting$HTBparam(y,x,param)  # defines the data, including optional features such as names, weights, time\noutput = HybridTreeBoosting$HTBfit(data,param)   # fits the model \nyf     = HybridTreeBoosting$HTBpredict(x_predict,output)\nt      = HybridTreeBoosting$HTBweightedtau(output,data)  # variable importance and smoothness\nt      = HybridTreeBoosting$HTBpartialplot(data,output,c(1,2))  # partial dependence plots, here for the first two features  ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/#Further-notes-on-using-HybridTreeBoosting-in-R","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Further notes on using HybridTreeBoosting in R","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/#To-translate-the-Julia-tutorials-in-R","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"To translate the Julia tutorials in R","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"Apologies to all the R purists for often using = everywhere instead of distinguishing between functions and assignments (= and <-).\nMy versioning of Julia code to R is probably amateurish. Feel free to improve it. \nChange Julia symbols to R strings. e.g. :modality to \"modality\", :On to \"On\"\nChange Julia true/false to R TRUE/FALSE \nChange Julia vectors [1,2,3] to R vectors c(1,2,3)\nThe output of each function in Julia is a named tuple. This corresponds to a list in R, whose elements can be accessed in the usual way, e.g. ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"  ntrees = output$ntrees  ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"Should this fail, we can use juliaGet() to translate the Julia object into a proper R list, e.g.  ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"  list = juliaGet(output)\n  ntrees = list$ntrees  ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"In the tutorials the output of a function is sometimes given by several variables, e.g. (see Zeroinflatedy.md) in Julia ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"    yf,prob0,yf_not0     = HTBpredict(x_test,output)","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"This will not work in R, but it is always possible to work with the list, so R becomes","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"    t = HybridTreeBoosting$HTBpredict(x_test,output)\n    prob0 = t$prob0 ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/#Getting-your-data-from-R-to-HTBoost","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Getting your data from R to HTBoost","text":"","category":"section"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"If y and x are numerical matrices, they can be taken into HTBdata() directly, e.g. ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"n      = 1000 \np      = 2  \nx      = matrix(rnorm(n*p),nrow = n,ncol = p)\ny      = x[,1] + rnorm(n)\n\nparam   = HybridTreeBoosting$HTBparam(modality=\"accurate\")  \ndata   = HybridTreeBoosting$HTBdata(y,x,param)\noutput = HybridTreeBoosting$HTBfit(data,param)","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"This will work even if there are missing data (NA in R, NaN or missing in Julia) (see Missing data) for how HTBoost deals with missing data internally, delivering superior accuracy if the underlying function is at least partially smooth.  ","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"When y and/or x contain strings (categorical features), we must translate our R dataframe into a Julia DataFrame, which is then fed to HTBdata(), e.g. (continuing from the previous example)","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"x_string =  sample(c(\"v1\", \"v2\", \"v3\"), n, replace = TRUE)   # create a categorical with 3 values\ndf       = data.frame(x,x_string)                          # R dataframe \nx        = DataFrames$DataFrame(df)                          # x is a Julia dataframe\ndata     = HybridTreeBoosting$HTBdata(y,x,param,fnames=colnames(df))    # pass the column names \noutput   = HybridTreeBoosting$HTBfit(data,param)                        \n","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"Columns of string values are automatically interpreted by HTBoost as a categorical. If some categorical features are represented by numerical values, it is necessary to list them in param (in which case all categorical features, even strings, must be listed). This can be done either with a vector of their column positions, or with their names, if fnames (an optional argument) is provided to HTBdata()","category":"page"},{"location":"tutorials_R/Installation_and_use_in_R/","page":"Installation and Introduction to the R bindings of HTBoost.jl","title":"Installation and Introduction to the R bindings of HTBoost.jl","text":"# either \nparam = HybridTreeBoosting$HTBparam(cat_features=c(3))\ndata  = HybridTreeBoosting$HTBdata(y,x,param)    # passing the column names is optional\n\n# or\nparam = HybridTreeBoosting$HTBparam(cat_features=c(\"x_string\"))\ndata  = HybridTreeBoosting$HTBdata(y,x,param,fnames=colnames(df))    # passing the column names is required\n","category":"page"},{"location":"examples/Speeding_up_with_large_n/#Speeding-up-training-with-large-n","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"","category":"section"},{"location":"examples/Speeding_up_with_large_n/#Some-options-to-speed-up-training-for-HTBoost,-particularly-with-large-n-and-large-p.","page":"Speeding up training with large n","title":"Some options to speed up training for HTBoost, particularly with large n and large p.","text":"","category":"section"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"HTBoost runs much faster (particularly with large n) with multiple cores than with one, after the initial one-off cost. The improvements in speed are roughly linear in the number of cores, up to 8 cores, and still good up to 16 cores, particularly when p/#cores is large. Gains from 16 to 32 cores are modest. ","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"1) modality = :fast, nfold = 1, nofullsample = true, disengage variable selection. ","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"The easiest way to speed up training is by setting nfold = 1 (a single validation set), and modality=:fast or :fastest. These modalities do not perform cv.  :fast will typically still produces a competitive model in terms of accuracy, particularly if n/p is large. If nfold=1, setting nofullsample=true further reduces computing time by 60% at the cost of fitting the model on a smaller sample. modality = :fastest automatically sets nfold=1, nofullsample=true, and also lambda = 0.2 instead of 0.1. lambda = 0.2 can perform almost as well if the function is smooth and n/p is large.","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"2) Use a coarser grid for feature selection at deeper levels of the tree. (Can be combined with any value of modality) ","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"Examples of use: ","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"\n    param = HTBparam(depth_coarse_grid =4,depth_coarse_grid2=5,modality=:fast)\n    param = HTBparam(depth_coarse_grid =4,depth_coarse_grid2=5,modality=:compromise)","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"Replacing the defaults (5,7) with (4,5) may speed up computations by 25-33%, with no or little loss of fit in most cases. ","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"3) Don't allow forcing sharp splits (in combination with modality=:fast). Warning: potential for decreased performance if f(x) is not smooth! ","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"In situations where some features may require imposing sharp splits, the model is estimated twice. To avoid this, the recommended option is to run ","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"output = SMARTfit(data,param,cv_hybrid=false)","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"(in combination with modality=:fast or :fastest) then cuts computing times in half. The loss of fit is typically modest, but can be substantial in some cases.","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"4) Be aware of how time increases with depth in HTBoost.","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"The cost of fitting a tree of depth 6 (7) is roughly twice the cost of fitting a tree of depth 5 (6). Hence going from depth 5 to depth 7 could increases computing time by a factor of 4. In most cases the actual increase is smaller, as less trees are needed, but still substantial.","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"5) Cross-validate on a sub-sample, then one run best model on full sample.","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"Setting modality = :fast fits the model at default parameters. If some cross-validation is desired,  the following strategy can be used to speed up cross-validation, typically with only small deterioration in performance if n is large and n/p is large. ","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"When n is very large, and it takes too long to fit HTBoost in modality = :compromise or :accurate, one way to proceed is to cv on a subsample of the data (say 20%) and then fit only one model on the full sample, using the  best parameters found in the subsample, except for the number of trees. If the subsample is large enough, the best parameters found in the subsample will be close to the best parameters in the full sample. (Of course the subset is more noisy and will prefer simpler models, but the difference should be modest if n is large. The number of trees should not be taken from the subset of course.) This can be accomplished as follows:","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"Set modality=:compromise or :accurate, take a subsample of the data (20%), and run output=HTBfit() on that.\nSet param=output.bestparam, and then param.modality=:fast, and run HTBfit() on the full data.","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"An example is given below: ","category":"page"},{"location":"examples/Speeding_up_with_large_n/","page":"Speeding up training with large n","title":"Speeding up training with large n","text":"number_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Statistics\n\n# USER'S OPTIONS \nRandom.seed!(123)\n\n# Options for data generation \nn         = 500_000\np         = 100         # number of features \ndummies   = true        # if true if x, x_test are 0-1 (much faster training).\nstde      = 1            \n\n# Options for HTBoost: modality is the key parameter guiding hyperparameter tuning and learning rate.\n# :fast only fits one model at default parameters, while :compromise and :accurate perform\n# automatic hyperparameter tuning. \n\nrandomsubset      = 0.2          # e.g. 0.2. Share of observations in the first sub-set \nmodality_subs     = :compromise  # :accurate or :compromise (default)\nmodality_full     = :fast        # :fast\n\nnfold_subs       = 1             # number of cv folds. 1 sufficient if the sub-sample is sufficiently large \nnfold_full       = 1         \nnofullsample_full = true         # if nfold=1 and nofullsample=true, the model is not re-fitted on the full sample after validation          \nrandomizecv       = false       # false (default) to use block-cv.\n\nverbose          = :Off\nwarnings         = :On\n\n# simple f(x), with pstar relevant features.\n\np_star    = 10       # number of relevant features \nβ         = randn(p_star)    # draw linear coefficients from a Gaussian distribution\ndgp(x)    = x[:,1:length(β)]*β\n\n# END USER'S INPUTS \n\nif dummies\n    x,x_test = randn(n,p),randn(200_000,p) \n    x,x_test = Float64.(x .> 0), Float64.(x_test .> 0)\nelse\n    x,x_test = randn(n,p), randn(200_000,p)    \nend     \n\ny       = dgp(x) + stde*randn(n)\nf_test  = dgp(x_test)\n\n# HTBoost on a sub-sample \nparam_subs   = HTBparam(modality=modality_subs,nfold=nfold_subs,nofullsample=true,randomizecv=randomizecv,\n                verbose=verbose,warnings=warnings)\ndata         = HTBdata(y,x,param_subs)\nn            = length(data.y)\n\nind       = randperm(n)[1:convert(Int,round(randomsubset*n))]\ndata_subs = HTBdata(y[ind],x[ind,:],param_subs)\n\noutput_subs = HTBfit(data_subs,param_subs) # performs cv on subset\n\n# HTBoost on full sample \nparam          = output_subs.bestparam        # sets param at best configuration in subset, then modify where appropriate\n\nparam.ntrees   = 2_000                        # number of trees should not be from subsample! Early stopping must be on full sample.\nparam.modality = modality_full      \nparam.nfold    = nfold_full\nparam.nofullsample = nofullsample_full\n\ndata  = HTBdata(y,x,param)\n\nprintln(\"\\n n = $n, p = $p, dummies=$dummies, modality = $(param.modality)\")\n\nprintln(\"\\n Time to train the full model.\")\n@time output = HTBfit(data,param);\n\nyf = HTBpredict(x_test,output,predict=:Ey)  # predict\nprintln(\"\\n RMSE of HTBoost from true E(y|x) \", sqrt(mean((yf-f_test).^2)) )\n","category":"page"},{"location":"Tutorials_R/#HTBoost-Tutorials","page":"Tutorials (R)","title":"HTBoost Tutorials","text":"","category":"section"},{"location":"Tutorials_R/","page":"Tutorials (R)","title":"Tutorials (R)","text":"For R users, I recommend starting with Installation and use in R and Basic use.  \nOnly a few tutorials are provided for R. For more tutorials and examples, see Julia tutorials and Julia examples, using the guidelines in Installation and use in R to adapt the code to R. ","category":"page"},{"location":"Tutorials_R/#R-tutorials","page":"Tutorials (R)","title":"R tutorials","text":"","category":"section"},{"location":"Tutorials_R/","page":"Tutorials (R)","title":"Tutorials (R)","text":"Installation and use in R\nBasic use (main options, cv, savings and loading results, variable importance and more post-estimation analysis)\nTime series and panels (Data Frames, time series and panels/longitudinal data, with various options for cv)\nCategorical features (how HTBoost handles categorical features; comparison with LightGBM and CatBoost)","category":"page"},{"location":"examples/Missing_data/#Internal-handling-of-missing-data-in-HTBoost","page":"Internal handling of missing data in HTBoost","title":"Internal handling of missing data in HTBoost","text":"","category":"section"},{"location":"examples/Missing_data/","page":"Internal handling of missing data in HTBoost","title":"Internal handling of missing data in HTBoost","text":"HTBoost handles missing values automatically. Imputation is optional. ","category":"page"},{"location":"examples/Missing_data/","page":"Internal handling of missing data in HTBoost","title":"Internal handling of missing data in HTBoost","text":"This example reproduces the set-up in the simulations i) Experiment 1 and ii) Model 2 and Model 3 in Experiment 2 in the paper: \"On the consistency of supervised learning with missing values\" by Josse et al., 2020.  Data can be missing at random, or missing not at random as a function of x only, or missing not at random as a function of E(y).","category":"page"},{"location":"examples/Missing_data/","page":"Internal handling of missing data in HTBoost","title":"Internal handling of missing data in HTBoost","text":"The approach to missing values in HTBoost is Block Propagation (see Josse et al.).\nThere is a however a a key difference compared to lightGBM and other GBM when the split is soft (τ < Inf): the value m at which to set all missing is estimated/optimized at each node. With standard trees (sharp splits), it only matters whether missing are sent to the left or right branch, but in HTBoost the allocation of missing values is also smooth (as long as the split is smooth), and the proportion to which missings are sent left AND right is decided by a new split parameter m, distinct from μ.  The result is more efficient inference with missing values. When the split is sharp or the feature takes only two values, HTBoost assigns missing in the same way as LigthGBM (by Block Propagation).\nThe native procedure to handle missing values is Bayes consistent (see Josse et al.), i.e. efficient in large samples, and is convenient in that it can handle data of mixed types (continuous, discrete, categorical). When feasible, a good imputation of missing values + mask (see Josse et al. ) can perform better, particularly in small samples, high predictability of missing values from non-missing values, linear or quasi-linear f(x), and missing at random (in line with the results of Josse et al.)  ","category":"page"},{"location":"examples/Missing_data/","page":"Internal handling of missing data in HTBoost","title":"Internal handling of missing data in HTBoost","text":"The comparison with LightGBM is biased toward HTBoost if the function generating the data is smooth in some features. LightGBM is cross-validated over maxdepth and numleaves, with the number of trees set to 1000 and found by early stopping.","category":"page"},{"location":"examples/Missing_data/","page":"Internal handling of missing data in HTBoost","title":"Internal handling of missing data in HTBoost","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing DataFrames, Random, Statistics\nusing LinearAlgebra,Plots, Distributions\nusing LightGBM\n\nRandom.seed!(1)\n\n# Options to generate data. y is the of four additive nonlinear functions + Gaussian noise(0,stde^2)\n\nExperiment      = \"1\"     # \"1\" or \"2 Friedman\" or \"2 Linear\"  (see Josse et al., 2020, experiment 1, and experiment 2, Friedman or Linear dgp)\nmissing_pattern = 1       # Only relevant for Experiment = \"1\". 1 for MCAR (missing at random), 2 for Censoring MNAR (at 1), 3 for Predictive missingness \n\nn,p,n_test  = 10_000,10,100_000  # n=1000, p= 9 or 10 in paper. Since this is one run, consider larger n. \nstde        = 0.1                # 0.1 in paper\nρ           = 0.5                # 0.5 in paper, cross-correlation of features \n\n# Some options for HTBoost\npriortype = :hybrid        # :hybrid (default) or :smooth or :sharp\nmodality  = :compromise    # :accurate, :compromise, :fast, :fastest \n\nnfold           = 1        # nfold cv. 1 faster (single validation sets), default 4 is slower, but more accurate. Here nfold = 1 for fair comparison with LightGBM.\n\nplot_results = false\nmask_missing = false     # default = false. True to introduce an additional feature, a dummy with value 'true' if x is missing. Not necessary.\n\n# END USER'S OPTIONS\n\n# generate data\n\n# Missing at random (MCAR)\nfunction model1_missingpattern1(x) \n\n    prob = 0.2  # probability of miss. 0.2 in their experiments \n    α,β = 1,2  # α,β = 1,2 in their experiments\n    i   = 1    # i=1 in their experiments. Which feature is x^2 (miss is for 1st)\n    ind = rand(size(x,1)) .< prob\n    f   = α*x[:,i].^β  \n    x[ind,1] .= NaN\n\n    return f,x\nend \n\n\nfunction model1_missingpattern2(x) \n\n    prob = 0.2                    # probability of miss. 0.2 in their experiments \n    q  = quantile(x[:,1],1-prob)\n    α,β = 1,2  # α,β = 1,2 in their experiments\n    i   = 1    # i=1 in their experiments. Which feature is x^2 (miss is for 1st)\n    ind = x[:,1] .< q\n    f   = α*x[:,i].^β  \n    x[ind,1] .= NaN\n\n    return f,x\nend \n\n\n\nfunction model1_missingpattern3(x) \n\n    prob = 0.2   # probability of miss. 0.2 in their experiments \n    α,β = 1,2  # α,β = 1,2 in their experiments\n    i   = 1    # i=1 in their experiments. Which feature is x^2 (miss is for 1st)\n    ind = rand(size(x,1)) .< prob\n    f   = α*x[:,i].^β  + 3*ind \n    x[ind,1] .= NaN\n\n    return f,x\nend     \n\n\nfunction Friedman(x) \n\n    prob = 0.2   # probability of miss. 0.2 in their experiments \n    n,p  = size(x)\n    f    = @. 10*sin(π*x[:,1]*x[:,2]) + 20*(x[:,3] - 0.5)^2 + 10*x[:,4] + 5*x[:,5]\n    MissData = rand(n,p) .< prob\n    x[MissData] .= NaN\n\n    return f,x\nend     \n\n\n\nfunction Linear(x) \n\n    prob = 0.2   # probability of miss. 0.2 in their experiments \n    n,p  = size(x)\n    f    =  x[:,1] + 2*x[:,2] - x[:,3] + 3*x[:,4] - 0.5*x[:,5] - x[:,6] + 0.3*x[:,7] + 1.7*x[:,8] \n            + 0.4*x[:,9] - 0.3*x[:,10] \n    MissData = rand(n,p) .< prob\n    x[MissData] .= NaN\n\n    return f,x\nend     \n\n\n\nfunction missing_function(missing_pattern)\n\n    if missing_pattern==1\n        return model1_missingpattern1\n    end \n    \n    if missing_pattern==2\n        return model1_missingpattern2\n    end \n\n    if missing_pattern==3\n        return model1_missingpattern3\n    end \n\nend \n\n\nif Experiment == \"1\"\n    f_pattern = missing_function(missing_pattern)\nelseif Experiment == \"2 Friedman\" \n    f_pattern = Friedman\nelseif Experiment == \"2 Linear\" \n    f_pattern = Linear\nelse \n    @error \"Experiment misspelled.\"    \nend \n\n\n#cross-correlated data\nu = ones(p)\nμ = ones(p)\nV = ρ*(u*u') + (1-ρ)*I\nd = Distributions.MvNormal(μ,V)\nx = copy(rand(d,n)')            # copy() because LightGBM does not copy with Adjoint type  \nx_test = copy(rand(d,n_test)')\n\nf,x            = f_pattern(x)\nf_test,x_test  = f_pattern(x_test) \n\ny      = f + stde*randn(n)\ny_test = f_test + stde*randn(n_test)\n\n# set up HTBparam and HTBdata, then fit and predit\nparam  = HTBparam(priortype=priortype,randomizecv=true,nfold=nfold,modality=modality )\ndata   = HTBdata(y,x,param)\n\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)  # predict\n\nyf  = HTBpredict(x_test,output)  # predict\n\n# Evaluate predictions at a few points\nprintln(\"\\n E(y|x) with x1 = 1 or missing, and x2 = 0 or 1\") \n\nfor ov in [0.0,1.0]\n    x_t    = fill(ov,size(x,2),p)\n    x_t[1,1] = 1.0\n    yf1     = HTBpredict(x_t,output)  # predict\n    x_t[1,1] = NaN\n    yf2     = HTBpredict(x_t,output)  # predict\n\n    println(\" prediction at x1 = 1 and at x = miss, other variables at $ov \", [yf1[1],yf2[1]])\nend \n\n# LigthGBM \n\nestimator = LGBMRegression(\n    objective = \"regression\",\n    metric = [\"l2\"],         \n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    num_threads = number_workers\n    )\n\nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n\n# parameter search over num_leaves and max_depth\nsplits = (collect(1:n_train),collect(1:min(n_train,100)))  # goes around the problem that at least two training sets are required by search_cv (we want the first)\n\nparams = [Dict(:num_leaves => num_leaves,\n           :max_depth => max_depth) for\n      num_leaves in (4,16,32,64,127,256),\n      max_depth in (2,3,5,6,8)]\n\nlightcv = LightGBM.search_cv(estimator,x,y,splits,params,verbosity=-1)\n\nloss_cv = [lightcv[i][2][\"validation\"][estimator.metric[1]][1] for i in eachindex(lightcv)]\nminind = argmin(loss_cv)\n\nestimator.num_leaves = lightcv[minind][1][:num_leaves]\nestimator.max_depth  = lightcv[minind][1][:max_depth]\n\n# fit at cv parameters\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\nyf_gbm = LightGBM.predict(estimator,x_test)   # (n_test,num_class) \n\nprintln(\"\\n Experiment = $Experiment, missing_pattern = $missing_pattern, n = $n\")\nprintln(\"\\n out-of-sample RMSE from truth, HTBoost, modality=:modality  \", sqrt(sum((yf - f_test).^2)/n_test) )\nprintln(\" out-of-sample RMSE from truth, LigthGBM cv                     \", sqrt(sum((yf_gbm - f_test).^2)/n_test) )\n","category":"page"},{"location":"tutorials/Zero_inflated_y/#Strategies-for-zero-inflated-data","page":"-","title":"Strategies for zero-inflated data","text":"","category":"section"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"<img src=\"../assets/ZeroInflated.png\" width=\"400\" height=\"250\">","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"We distinguish three cases of zero-inflated data:  ","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"A discrete distribution whose support includes 0 (e.g. Poisson) has excess probability mass at y=0. \nAn otherwise continuous distribution for y on (0,∞) has positive mass at y=0.\nAn otherwise continuous distribution for y on (-∞,∞) has positive mass at y=0 or at some other value. ","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"**Case 1): zero-inflated discrete *y***","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"HTBoost does not have specialized settings for this scenario. I am not aware of any GBM package with a specialized loss for this case, but the statistical literature has several approaches that could be adapted to GBMs. Meanwhile, for HTBoost users, loss = :gammaPoisson is an option, with loss = :hurdleL2loglink (see case 2) a possible alternative.","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"Case 2): zero-inflated continuous y on [0,∞)","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"LightGBM, XGBoost and CatBoost propose the Tweedie loss for this case. The Tweedie loss allows (for the key parameter in the rage (1-2), with default set at 1.5 in those packages) for positive mass at y=0, where otherwise y>0.   HTBoost has two loss functions specific to this case:     :hurdleGamma, :hurdleL2loglink The :hurdleGamma is closest to the Tweedie distribution in LightGBM, XGB, and CatBoost. The hurdleL2loglink loss can be a strong alternative, if the gamma assumption is incorrect.","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"Hurdle models in HTBoost build two separate models, one with logistic loss to predict the occurence of a zero, and a second model with loss gamma or L2 or L2loglink to predict E(y|y≠0). Compared to a Tweedie regression, hurdle models have richer parametrization but far weaker constraints on the distribution, implying higher variance and smaller bias. My reading of the statistical literature is that hurdle models typically outperform Tweedy in terms of forecasting, particularly in data-rich environments, which should be the most relevant case for ML.  ","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"Case 3): zero-inflated continuous y on (-∞,∞)   While :hurdleGamma and :hurdleL2loglink require y≥0, a :hurdleL2 loss can be used if some y are negative. A hurdleL2 loss could therefore also be used if an otherwise continuous y has positive mass at some value v other than zero, by setting y = y-v and loss = :hurdleL2.  ","category":"page"},{"location":"tutorials/Zero_inflated_y/#Example","page":"-","title":"Example","text":"","category":"section"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"See Zero inflated y for a fully worked-out example, including a comparison with LightGBM. ","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"From a tutorial's perspective, the only points of note are the loss ....","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"param=HTBparam(loss=:hurdleGamma)    # :hurdleGamma, :hurdleL2loglink, :hurdleL2 ","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"and the output from HTBpredict( ), which takes the form ","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"yf,prob0,yf_not0 = HTBpredict(x_test,output)","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"where","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"prob0 = prob(y=0|x)   yfnot0 = E(y|x,y≠0)   yf = E(y|x) = (1-prob0)*yfnot0","category":"page"},{"location":"tutorials/Zero_inflated_y/","page":"-","title":"-","text":"```","category":"page"},{"location":"tutorials/Offset/#Offset-(exposure)","page":"-","title":"Offset (exposure)","text":"","category":"section"},{"location":"tutorials/Offset/","page":"-","title":"-","text":"HTBoost provides the option to include an offset (exposure), a practice common in many fields, including biology and insurance. ","category":"page"},{"location":"tutorials/Offset/","page":"-","title":"-","text":"The offset is added to γ (NOT multiplied by E(y|x)), where γ = E(link(y)|x). It should therefore be in logs for loss ∈ [:L2loglink, :gamma, :Poisson, :gammaPoisson, :hurdleGamma, :hurdleL2loglink], and logit for loss =:logistic.","category":"page"},{"location":"tutorials/Offset/","page":"-","title":"-","text":"For example, if loss = :gamma and E(y|x) = offsetf(x), then the offset that is fed into *HTBdata( ) should be logged. ","category":"page"},{"location":"tutorials/Offset/","page":"-","title":"-","text":"loss     = :gamma \n\nexposure = df[1:n_train,:duration]\nexposure_test = df[n_train+1:end,:duration]\n\nif loss in [:gamma,:L2loglink,:Poisson,:gammaPoisson,:hurdleGamma,:hurdleL2loglink]\n   offset = log.(exposure)\n   offset_test = log.(exposure_test)\nelse\n    offset = exposure \n    offset_test = exposure_test\nend\n\nparam    =  HTBparam(loss=loss)              \ndata     =  HTBdata(y,x,param,offset=offset)    \nyf       =  HTBpredict(x_test,output,predict=:Ey,offset=offset_test)","category":"page"},{"location":"tutorials/Offset/#Some-advice","page":"-","title":"Some advice","text":"","category":"section"},{"location":"tutorials/Offset/","page":"-","title":"-","text":"Unless you are absolutely sure that an offset enters exactly with coeff 1, I recommend also adding it  as a feature. HTBoost will then be able to capture possibly subtle nonlinearities and interaction effects in E(y|offset).  ","category":"page"},{"location":"tutorials/Offset/#Warning!-Offset-does-not-work-well-with-categorical-features","page":"-","title":"Warning! Offset does not work well with categorical features","text":"","category":"section"},{"location":"tutorials/Offset/","page":"-","title":"-","text":"Categorical features with more than two categories are not currently handled correctly (by the mean targeting transformation) with offsets. The program will run but categorical information will be used sub-optimally, particularly if the average offset differs across categories. If categorical features are important, it may be better to omit the offset from HTBdata( ), and instead model y/offset with a :L2loglink loss instead of a :gamma, :Poisson or :gammaPoisson. ","category":"page"},{"location":"tutorials/Offset/","page":"-","title":"-","text":"See Offset for a worked-out example.","category":"page"},{"location":"examples/Hybrid_trees/#Hybrid-trees","page":"Hybrid trees","title":"Hybrid trees","text":"","category":"section"},{"location":"examples/Hybrid_trees/","page":"Hybrid trees","title":"Hybrid trees","text":"Hybrid trees: why smooth trees are not always good enough.","category":"page"},{"location":"examples/Hybrid_trees/","page":"Hybrid trees","title":"Hybrid trees","text":"There are two characteristics of HTB trees that make them hybrid:","category":"page"},{"location":"examples/Hybrid_trees/","page":"Hybrid trees","title":"Hybrid trees","text":"Splits are smooth by default, but can be forced to be sharp for some features.\nThe fitted value from each tree is the input for a univariate nonlinear transformation.","category":"page"},{"location":"examples/Hybrid_trees/","page":"Hybrid trees","title":"Hybrid trees","text":"Here we explore the first characteristic, while examples/Projection Pursuit Regreession.jl  explores the second.","category":"page"},{"location":"examples/Hybrid_trees/","page":"Hybrid trees","title":"Hybrid trees","text":"When the smoothness parameter τ is estimated for each split and allowed to take high values, including τ=Inf (which corresponds to a sharp split), it's perhaps intuitive to think that a smooth tree can capture both smooth and sharp functions. However, this is not necessarily true in a boosting context, as illustrated in this script.","category":"page"},{"location":"examples/Hybrid_trees/","page":"Hybrid trees","title":"Hybrid trees","text":"Consider the following example: If we are trying to approximate a step function with a single step (second column in the plot), a smooth tree which can take high values of τ performs asymptotically as well as a sharp split. However, if the step functions has multiple steps (third column in the plot), the fitting function is initially smooth, and it is then impossible for the subsequent trees to fully recover a sharp function. In this example, the greedy nature of boosting gets the algorithm stuck in a local minimu.  A hybrid tree attempts to recognize such cases from a preliminary run, and, if necessary, imposes sharp splits on some features to avoid the local minima. The cross-validated loss is then  used to decide in which combination to use the hybrid and the smooth tree.","category":"page"},{"location":"examples/Hybrid_trees/","page":"Hybrid trees","title":"Hybrid trees","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Plots \n\n# USER'S OPTIONS \n\nRandom.seed!(123)\n\n# Some options for HTBoost\nloss      = :L2            # :L2 or :logistic (or :Huber or :t). \nmodality  = :fastest       # :accurate, :compromise (default), :fast, :fastest \n\nntrees    = 2000          # maximum number of trees  \nnfold     = 1             # number of cv folds. 1 faster (single validation sets), default 4 is slower, but more accurate.\n\nverbose     = :Off\nwarnings    = :Off\n \n# options to generate data. y = sum of two additive nonlinear functions + Gaussian noise.\nn,p,n_test  = 10_000,6,100_000\nstde        = 0.2\n\nf_1(x,b)    = 1.0./(1.0 .+ (exp.(4.0*(x .- 0.5) ))) .- 0.1*b\nf_2(x,b)    = b*(x.>0.5) .- 0.1*b                      # step function with one step \nf_3(x,b)    = b*( (x .> -0.5) +  (x .> 0.5) )        # step function with several steps\nf_4(x,b)    =  (-0.25 .< x .< 0.25)                     # \"tower\" function \n#f_interact(x1,x2,b)  = b*(-0.25 .< x1 .< 0.25).*(0.25 .< x2 .< 0.5)  # sharp interaction\nf_interact(x1,x2,b)  = b*(-0.25 .< x1 .< 0.25).*(0.25 .< x2 .< 0.5) + (-1.0 .< x1 .< -0.75).*(1.25 .< x2 .< 1.5)  # sharp interaction\n\nb1,b2,b3,b4 = 1.0,1.0,1.0,1.0\nb_interact  = 3.0          # sharp interactions can be difficult to approximate well for a smooth tree\n\n# END USER'S OPTIONS\n\n# generate data\nx,x_test = randn(n,p), randn(n_test,p)\n\nf        = f_interact(x[:,5],x[:,6],b_interact) + f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4) \nf_test   = f_interact(x_test[:,5],x_test[:,6],b_interact) + f_1(x_test[:,1],b1) + f_2(x_test[:,2],b2) + f_3(x_test[:,3],b3) + f_4(x_test[:,4],b4) \n\ny = f + stde*randn(n)\n\n# set up HTBparam and HTBdata, then fit and predit\nntrees == 1 ? lambda = 1 : (modality == :fastest ? lambda = 0.2 : lambda = 0.1)\n\nparam  = HTBparam(loss=loss,nfold=nfold,verbose=verbose,warnings=warnings,\n           modality=modality,nofullsample=true,lambda=lambda,ntrees=ntrees)\n\ndata   = HTBdata(y,x,param)\n\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)  # predict the natural parameter\n\nprintln(\" \\n modality = $(param.modality), nfold = $nfold \")\nprintln(\" out-of-sample RMSE from truth \", sqrt(sum((yf - f_test).^2)/n_test) )\n\nprintln(\" \\n with smooth rather than hybrid trees \")\noutput_s = HTBfit(data,param,cv_hybrid=false)\nyf     = HTBpredict(x_test,output_s)  # predict the natural parameter\n\nprintln(\" out-of-sample RMSE from truth \", sqrt(sum((yf - f_test).^2)/n_test) )\n\n# After 1 tree, with lambda = 1\nparam.ntrees=1 \nparam.lambda=param.T(1)\n\noutput1 = HTBfit(data,param)\n\n# partial plots \nq_s,pdp_s  = HTBpartialplot(data,output_s,[1,2,3,4])\nq,pdp  = HTBpartialplot(data,output,[1,2,3,4])\nq,pdp_1  = HTBpartialplot(data,output1,[1,2,3,4])\n\npl   = Vector(undef,12)\nf,b  = [f_1,f_2,f_3,f_4],[b1,b2,b3,b4]\n\nfor i in 1:4\n    pl[i]   = plot( [q_s[:,i]],[pdp_1[:,i] f[i](q_s[:,i],b[i]) - f[i](q_s[:,i]*0,b[i])],\n           label = [\"1st tree λ=1\" \"dgp\"],\n           legend = :bottomright,\n           linecolor = [:blue :red],\n           linestyle = [:solid :dot],\n           linewidth = [5 2],\n           titlefont = font(15),\n           legendfont = font(12),\n           xlabel = \"x\",\n           ylabel = \"f(x)\",\n           )\nend\n\nfor i in 1:4\n    pl[i+4]   = plot( [q_s[:,i]],[pdp_s[:,i] f[i](q_s[:,i],b[i]) - f[i](q_s[:,i]*0,b[i])],\n           label = [\"smooth\" \"dgp\"],\n           legend = :bottomright,\n           linecolor = [:blue :red],\n           linestyle = [:solid :dot],\n           linewidth = [5 2],\n           titlefont = font(15),\n           legendfont = font(12),\n           xlabel = \"x\",\n           ylabel = \"f(x)\",\n           )\nend\n\nfor i in 1:4\n    pl[i+8]  = plot( [q[:,i]],[pdp[:,i] f[i](q[:,i],b[i]) - f[i](q[:,i]*0,b[i])],\n           label = [\"hybrid\" \"dgp\"],\n           legend = :bottomright,\n           linecolor = [:blue :red],\n           linestyle = [:solid :dot],\n           linewidth = [5 2],\n           titlefont = font(15),\n           legendfont = font(12),\n           xlabel = \"x\",\n           ylabel = \"f(x)\",\n           )\nend\n\ndisplay(plot(pl[1],pl[2],pl[3],pl[4],pl[5],pl[6],pl[7],pl[8],pl[9],pl[10],pl[11],pl[12],layout=(3,4), size=(1300,800)))  # display() will show it in Plots window.\n","category":"page"},{"location":"tutorials/t/#Really-Robust-Regression","page":"-","title":"Really Robust Regression","text":"","category":"section"},{"location":"tutorials/t/","page":"-","title":"-","text":"Problems with Huber loss.","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"LightGBM and XGBoost offer a Huber and pseudo-Huber loss respectively. The mae (mean absolute error) loss is also sometimes used for outlier-robust regression.","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"We assume that the user's objective is to recover the conditional mean E(y|x), and a Huber or mae loss is employed in the hope of improving inference when the errors ε = y - E(y|x), as proxied by the residuals, show signs of being leptokurtik (fat-tailed).","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"There are two main problems with these losses (or with their implementations), which are corrected in HTBoost:","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"Unless the errors ε are perfectly symmetric, the GBM will make biased forecasts (forecasts with the wrong conditional and unconditional mean), even if trained on large samples. This problem is bigger for mae (which recovers the conditional median) and if the SNR (signal-to-noise ratio) is low.\nThe implementation of the Huber loss scales the robustness parameter ψ once, on the unconditional distribution. As a result, inference is only robust if the SNR is low; in high SNR cases the results are nearly indistinguishable from those of a L2 loss. ","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"HTBoost fixes these two problems by correcting for the bias, and recalibrating the robustness parameter ψ after each tree. HTBoost also introduces (in the context of GBMs) a student-t loss (loss = :t), where the dispersion and degrees-of-freedom parameters are estimated internally by maximum likelihood after each tree, hence solving problem 2.","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"The student-t loss is recommended in HTBoost. The Huber loss is available but not recommended in general (a possible exception is the original motivation for the Huber loss: y is contaminated by measurement errors).","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"As a result, the :t loss in HTBoost is, unlike the Huber loss in XGBoost and LightGBM, typically more accurate than the :L2 loss when residuals are leptokurtik (fat-tailed) or strongly skewed, as long as errors are iid. If errors are heteroskedastic (i.e. if var(ε) depends on x) neither the Huber nor the t distributions will in general recover the true E(y|x) asymptotically.","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"See the examples Huber and t unbiased and student t for a slightly more detailed presentation. I intend to discuss these results more extensively in a paper in the near future.","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"The code below illustrates these points.","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"number_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Statistics,Plots,Distributions\nusing LightGBM","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"Generate data from y = f(x) + ε, where f(x) is the Friedman's function, and ε has a strongly right-skewed distribution (a mixture of two normals.)","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"# Options for data generation \nn         = 10_000\np         = 5       \nstde      = 5      # std(ε). 1 for high SNR, 5 for lowish, 10 for low (R2 around 4%) \n\nm2        = 3*stde  # mean of second component of the mixture. 0 for symmetric fat tails, 3*stde for skewed\nstde2     = 3*stde           # second component of the mixture has larger variance\nprob2     = 0.3              # probability of the second component\n\ndgp(x) = 10.0*sin.(π*x[:,1].*x[:,2]) + 20.0*(x[:,3].-0.5).^2 + 10.0*x[:,4] + 5.0*x[:,5]\n\n# Generate data. \nn_test     = 200_000\nx,x_test   = rand(n,p), rand(n_test,p)\nftrue      = dgp(x)\nftrue_test = dgp(x_test)\n\nu1    = randn(n)*stde\nu2    = m2 .+ randn(n)*stde2\nS1    = rand(n).>prob2 \nu     = @. u1*S1 + u2*(1 - S1) - prob2*m2     # skewed distribution with zero mean  \ny     = ftrue + u\n\nhistogram(u,title=\"errors\",label=\"\")","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"<img src=\"../assets/skewed errors.png\" width=\"400\" height=\"250\">","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"Specify parameters for HTBoost and LightGBM, for a L2 and Huber regression.    No cv is performed on either since it would not change the conclusions we are interestd in.","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"param  = HTBparam(loss=:Huber,modality=:fastest,warnings=:Off)\ndata   = HTBdata(y,x,param)\n\n# ligthGBM parameters \nestimator_l2 = LGBMRegression(\n    objective = \"regression\",\n    num_iterations = 1000,\n    early_stopping_round = 100)\n\nestimator_huber = LGBMRegression(\n    objective = \"huber\",\n    metric    = [\"huber\"],\n    num_iterations = 1000,\n    early_stopping_round = 100)\n","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"Fit LightGBM with both L2 and Huber loss.","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"\nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n    \nLightGBM.fit!(estimator_l2,x_train,y_train,(x_val,y_val),verbosity=-1)\n    \nyf     = LightGBM.predict(estimator,x_test)[:,1]\nmse_l2 = sum((yf - ftrue_test).^2)/n_test\n\nprintln(\"\\n bias = E(prediction) - E(y) \")\nprintln(\"\\n bias of lightGBM with L2 loss    \", mean(yf-ftrue_test))\n\nLightGBM.fit!(estimator_huber,x_train,y_train,(x_val,y_val),verbosity=-1)\nyf_gbm = LightGBM.predict(estimator_huber,x_test)[:,1]\nmse_huber   = sum((yf_gbm - ftrue_test).^2)/n_test\n\nprintln(\" bias of lightGBM with Huber loss \", mean(yf_gbm-ftrue_test))\nprintln(\"\\n oos RMSE from true f(x), lightGBM, Huber loss \", sqrt(mse_l2) )\nprintln(\" oos RMSE from true f(x), lightGBM, L2 loss      \", sqrt(mse_huber) )\n","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"Biased inference and poor forecasts in LightGBM with Huber loss","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"The Huber loss produces a strong bias which translates into very poor forecasts. The L2 loss has no bias, and shows much better accuracy in spite of the leptokurtik residuals. The bias is very strong here because the SNR is fairly small. ","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":" bias = E(prediction) - E(y)\n\n bias of lightGBM with L2 loss     0.0288\n bias of lightGBM with Huber loss -2.4534\n\n oos RMSE from true f(x), lightGBM, Huber loss  1.9069\n oos RMSE from true f(x), lightGBM, L2 loss     2.9161\n","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"We now repeat the experiment with HTBoost, using loss = :t, which is the  recommended option for robustness.  ","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"\nparam.loss = :L2\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)  \nbias_l2  = mean(yf - ftrue_test)\nmse_l2   = sum((yf - ftrue_test).^2)/n_test\n\nparam.loss = :t\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)  \nbias_t  = mean(yf - ftrue_test)\nmse_t   = sum((yf - ftrue_test).^2)/n_test\n\nprintln(\"\\n bias of HTBoost with L2 loss, and t loss  \", [bias_l2,bias_t] )\nprintln(\" oos RMSE from true f(x). HTBoost with L2 loss, and t loss  \", [sqrt(mse_l2),sqrt(mse_t)] )\n","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"Unbiased inference and good forecasts in HTBoost with Huber and t loss","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"HTBoost corrects for the bias. Since errors are iid, we expect robust losses to outperform L2, which is what we see. ","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"bias of HTBoost with L2 loss, Huber loss, t loss  [0.0218, 0.0263]\noos RMSE from true f(x). HTBoost with L2 loss, Huber loss, t loss  [0.9864, 0.7171]","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"To inspect the student-t coefficients, we call HTBcoeff(output)","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"HTBcoeff(output)","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"The degrees-of-freedom are estimated at 2. This is an extremely low number, due to the strong skew in the data. ","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"(loss = :t, scale = 34.8386, dof = 2.033, variance = \"scale*dof/(dof-2)\")","category":"page"},{"location":"tutorials/t/","page":"-","title":"-","text":"You can run the example student t to verify that, when the errors are drawn from a student-t distribution, HTBoost consistently estimates its scale and degrees of freedom. ","category":"page"},{"location":"examples/Huber_and_t_unbiased/#Huber-and-student-t-loss-modified-to-give-unbiased-forecasts","page":"Huber and student-t loss modified to give unbiased forecasts","title":"Huber and student-t loss modified to give unbiased forecasts","text":"","category":"section"},{"location":"examples/Huber_and_t_unbiased/","page":"Huber and student-t loss modified to give unbiased forecasts","title":"Huber and student-t loss modified to give unbiased forecasts","text":"Purpose and main results:","category":"page"},{"location":"examples/Huber_and_t_unbiased/","page":"Huber and student-t loss modified to give unbiased forecasts","title":"Huber and student-t loss modified to give unbiased forecasts","text":"Show how Huber loss functions leads to biased fitted and predicted values when the errors have a skewed distribution, and the resulting mse can be much higher than for L2 loss even if the errors are fat-tailed.\nIn contrast, if the errors are fat-tailed but symmetric, the lightGBM Huber loss tends to outperform L2 loss.\nHTBoost with loss = :t and loss=:Huber automatically corrects for biases due to skewed errors. (t recommended over Huber)\nIn HTBoost, the t loss (plus de-biasing) improves on the L2 loss in this settings (due to IID errors) \nCorrecting the bias improves the mse of lightGBM predictions compared to the original version, but the rmse is often inferior to L2 loss. \nThe impact of the bias is stronger if signal-to-noise is low. \nHTBoost re-estimates all parameters (dispersion and dof for a t) after each tree.  ","category":"page"},{"location":"examples/Huber_and_t_unbiased/","page":"Huber and student-t loss modified to give unbiased forecasts","title":"Huber and student-t loss modified to give unbiased forecasts","text":"Note:","category":"page"},{"location":"examples/Huber_and_t_unbiased/","page":"Huber and student-t loss modified to give unbiased forecasts","title":"Huber and student-t loss modified to give unbiased forecasts","text":"LightGBM is only fitted at default parameters, since the main interest here is not the comparison with HTBoost but  the performance of Huber loss and of bias correction. ","category":"page"},{"location":"examples/Huber_and_t_unbiased/","page":"Huber and student-t loss modified to give unbiased forecasts","title":"Huber and student-t loss modified to give unbiased forecasts","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Statistics,Plots\nusing LightGBM\n\n# USER'S OPTIONS \n\nRandom.seed!(1)\n\n# Options for data generation (from Friedman function plus errors drawn from a mixture of two Gaussian) \nn         = 10_000\np         = 5      # p>=5. Number of features. Only the first 4 variables are used in the function f(x) below \nstde      = 5      # e.g. 1 for high SNR, 5 for lowish, 10 for low (R2 around 4%) \n\nm2        = 3*stde  # mean of second component of mixture of normal. 0 for symmetric fat tails, 3*stde for skewed\n\n# Options for HTBoost: modality is the key parameter guiding hyperparameter tuning and learning rate.\n# :fast and :fastest only fit one model at default parameters, while :compromise and :accurate perform\n# automatic hyperparameter tuning. \n\nloss      = :t      # :t (recommended) or :Huber for HTBoost. :t automatically estimates degrees of freedom and can recover a Gaussian \nmodality  = :fastest   # :accurate, :compromise (default), :fast, :fastest\nntrees    = 2000     # maximum number of trees for HTBoost. \n\n# define the function dgp(x), here the Friedman's function for x~U  \ndgp(x) = 10.0*sin.(π*x[:,1].*x[:,2]) + 20.0*(x[:,3].-0.5).^2 + 10.0*x[:,4] + 5.0*x[:,5]\n\n# End user's options \n\n# generate data. x is standard uniform, and errors are a mixture of two normals, with right skew\nn_test     = 200_000\nx,x_test   = rand(n,p), rand(n_test,p)\nftrue      = dgp(x)\nftrue_test = dgp(x_test)\n\nstde2 = 3*stde\nu1    = randn(n)*stde\nu2    = m2 .+ randn(n)*stde2\nprob  = 0.3\nS1    = rand(n).>prob \nu     = @. u1*S1 + u2*(1 - S1) - prob*m2     # skewed distribution with zero mean  \ny      = ftrue + u\n\nhistogram(u,title=\"errors\",label=\"\")\n\n# HTBoost parameters\nparam  = HTBparam(loss=loss,nfold=1,ntrees=ntrees,nofullsample=true,modality=modality,verbose=:Off)\ndata   = HTBdata(y,x,param)\n\n# ligthGBM parameters \nestimator = LGBMRegression(\n    objective = \"regression\",\n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    num_threads = number_workers\n)\n\nestimator_huber = LGBMRegression(\n    objective = \"huber\",\n    metric    = [\"huber\"],\n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    num_threads = number_workers\n)\n\n\n# Fit lightGBM \n\nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n    \nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\n    \nyf_gbm = LightGBM.predict(estimator,x_test)\nyf_gbm2 = yf_gbm[:,1]    # drop the second dimension or a (n_test,1) matrix \nMSE2    = sum((yf_gbm2 - ftrue_test).^2)/n_test\n\nprintln(\"\\n bias = E(prediction) - E(y) \")\nprintln(\"\\n bias of lightGBM with L2 loss    \", mean(yf_gbm-ftrue_test))\n\nLightGBM.fit!(estimator_huber,x_train,y_train,(x_val,y_val),verbosity=-1)\nyf_gbm = LightGBM.predict(estimator_huber,x_test)\nyf_gbm = yf_gbm[:,1]    # drop the second dimension or a (n_test,1) matrix \nMSE3    = sum((yf_gbm - ftrue_test).^2)/n_test\n\nprintln(\" bias of lightGBM with Huber loss \", mean(yf_gbm-ftrue_test))\n\nprintln(\"\\n correlation of fitted values, lightGBM L2 and Huber \", cor(yf_gbm,yf_gbm2))\n# correct the bias of lightGBM \nyhat = LightGBM.predict(estimator_huber,x_train)\nbias = mean(yhat) - mean(y_train)\nyf_unbiased = yf_gbm .- bias \nMSE4    = sum((yf_unbiased - ftrue_test).^2)/n_test\n\nprintln(\"\\n oos RMSE from true f(x), lightGBM, Huber loss                    \", sqrt(MSE3) )\nprintln(\" oos RMSE from true f(x), lightGBM, Huber loss, de-biased         \", sqrt(MSE4) )\nprintln(\" oos RMSE from true f(x), lightGBM, L2 loss                       \", sqrt(MSE2) )\n\n# Fit HTBoost, :t (or :Huber) \noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)  \nMSE1    = sum((yf - ftrue_test).^2)/n_test\n\nprintln(\" oos RMSE from true f(x) parameter, HTBoost, loss=$loss            \", sqrt(MSE1) )\n\n# Fit HTBoost, :L2 \nparam.loss = :L2 \noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)  \nMSE0    = sum((yf - ftrue_test).^2)/n_test\n\nprintln(\" oos RMSE from true f(x) parameter, HTBoost, loss=L2           \", sqrt(MSE0) )\n","category":"page"},{"location":"tutorials/Time_series_and_panels/#Time-series-and-panels","page":"-","title":"Time series and panels","text":"","category":"section"},{"location":"tutorials/Time_series_and_panels/#Working-with-time-series-and-longitudinal-data-(panels).-Data-as-DataFrame.","page":"-","title":"Working with time series and longitudinal data (panels). Data as DataFrame.","text":"","category":"section"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"The user only needs to provide features and, optionally, a vector of dates in HTBdata( ) and, if there is overlapping, the overlap parameter in HTBparam().   Example:","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"param  = HTBparam(overlap=20)         \ndata   = HTBdata(y,x,param,dates,fnames = fnames)","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"where y,x and dates can be regular vectors and matrices or dataframes, e.g.","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"y = df[:,:excessret]\nfeatures = [:logCAPE, :momentum,:vol3m,:vol12m ]\nx = df[:,features]\ndates = df[:,:date]  ","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"There is no need to specify dates for an individual time series. For a panel, dates is required for correct default block-cv (but not if user-specified train and test samples are provided in indtraina and indtesta).","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"Overlap defaults to 0. Typically overlap = h-1, where y(t) = Y(t+h)-Y(t) (e.g. Y a log price and h=20 for monthly financial returns from daily data). This is used for purged-CV and to calibrate the priors.","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"By default, HTBoost uses block-cv, which is suitable for time series and longitudinal data.  Another good alternative for times series and panels is expanding window cross-validation, which requires the user to provide indtraina and indtesta in HTBparam( ). The function HTBindexesfromdates() can assist in building indtraina and indtesta. Example:","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"first_date = Date(\"2017-12-31\", Dates.DateFormat(\"y-m-d\"))\nindtrain_a,indtest_a = HTBindexes_from_dates(df,:date,first_date,12)  # 12 periods in each block, starting from first_date\n","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"See API for more details.    The code below provides an application to forecasting international stock market indexes. ","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"\n# On multiple cores\nnumber_workers  = 8  # desired number of workers\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random, Plots, CSV, DataFrames, Statistics\n\nRandom.seed!(1)\n","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"If we work with log returns, :L2 (or perhaps :t) is a natural loss.   If we work with raw returns P(t)/P(t-1), :L2loglink is an interesting alternative.  ","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"\n# data \nlog_ret        = false    # true to predict log returns, false (default) to predict returns\noverlap        = 0        \n","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"loss       = :L2loglink  # if log_ret=false, consider :L2loglink instead of :L2 \nmodality   = :accurate    # # :accurate, :compromise, :fast, :fastest\n\n# Specify cv \ncv_type     = \"block\"  # \"block\" or \"expanding\" or \"randomized\" (not recommended for time series and panels)\nnfold       = 4        # number of folds for cv (default 4). Irrelevant if cv_type = \"expanding\".\n\n# for cv_type = \"expanding\" \ncv_first_date     = 197001   # start date for expanding window cv       \ncv_block_periods  = 120      # number of periods (months in this dataset): if cv_type=\"block\", this is the block size\n\n# END USER'S OPTIONS\n\ndf = CSV.read(\"examples/data/GlobalEquityReturns.csv\", DataFrame, copycols = true) # import data as dataframe. Monthly LOG excess returns.\n#display(describe(df))        \n\n# prepare data \nlog_ret ? y = 100*df[:,:excessret] : y  = @. 100*(exp(df[:,:excessret]) )\n\nfeatures_vector = [:logCAPE, :momentum, :vol3m, :vol12m ]\nfnames = [\"logCAPE\", \"momentum\", \"vol3m\", \"vol12m\" ]\n\nx      = df[:,features_vector]\n\n# set up HTBparam and HTBdata, then fit, depending on cross-validation type\n\nif cv_type == \"randomized\"\n  param  = HTBparam(nfold=nfold,overlap=overlap,loss=loss,modality=modality,randomizecv=true) \nelseif cv_type == \"block\"   # default \n  param  = HTBparam(nfold=nfold,overlap=overlap,loss=loss,modality=modality) \nelseif cv_type == \"expanding\"\n  indtrain_a,indtest_a = HTBindexes_from_dates(df,:date,cv_first_date,cv_block_periods)\n  param  = HTBparam(nfold=nfold,overlap=overlap,loss=loss,modality=modality,\n                     indtrain_a=indtrain_a,indtest_a=indtest_a)\nend \n\ndata   = HTBdata(y,x,param,df[:,:date],fnames = fnames)\noutput = HTBfit(data,param)\n\nyhat   = HTBpredict(x,output)  # in-sample fitted value.\n\nprintln(\"\\n modality=$(param.modality), cv=$(cv_type), depth = $(output.bestvalue), number of trees = $(output.ntrees) \")\nprintln(\" in-sample R2 = \", round(1.0 - sum((y - yhat).^2)/sum((y .- mean(y)).^2),digits=3) )\n\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data);\n","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"The output is   ","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"\nmodality = :accurate, cv=\"block\", depth = 6, number of trees = 76\n in-sample R2 = 0.017\n\nFeature relevance, sorted from highest to lowest, adding up to 100\n\n   logCAPE    47.954\n  momentum    26.725\n    vol12m    25.320\n     vol3m     0.000\n","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"Partial dependence plots help visualize the model's fit. (However, they don't capture interactions). The marginal effect of higher valuations and volatility is to reduce expected returns, while positive momentum increases them. ","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"\n# partial dependence plots, best four features. q1st is the first quantile. e.g. 0.01 or 0.05\nq,pdp  = HTBpartialplot(data,output,sortedindx[[1,2,3,4]],q1st=0.01,npoints = 5000)\n\n# partial dependence plots\npl = Vector(undef,4)\n\nfor i in 1:4 \n  pl[i]   = plot(q[:,i],pdp[:,i], legend=false,title=fnames[sortedindx[i]],color=:green)\nend \n\ndisplay(plot(pl[1],pl[2],pl[3],pl[4], layout=(2,2), size=(1200,600))) \n","category":"page"},{"location":"tutorials/Time_series_and_panels/","page":"-","title":"-","text":"<img src=\"../assets/GlobalEquityPanel.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"examples/Categoricals/#Categoricals","page":"Categoricals","title":"Categoricals","text":"","category":"section"},{"location":"examples/Categoricals/#How-to-inform-HTBoost-about-categorical-features:","page":"Categoricals","title":"How to inform HTBoost about categorical features:","text":"","category":"section"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"If cat_features is not specified, non-numerical features (e.g. Strings) are treated as categorical\nIf cat_features is specified, it can be a vector of Integers (positions), a vector of Strings (corresponding to  data.fnames, which must be provided) or a vector of Symbols (the features' names in the dataframe).","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"Example of use: all categorical features are non-numerical.","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"    param = HTBparam()  ","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"Example of use: specify positions in data.x","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"    param = HTBparam(cat_features=[1])\n    param = HTBparam(cat_features=[1,9])","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"Example of use: specify names from data.fnames ","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"    data = HTBdata(y,x,param,fnames=[\"country\",\"industry\",\"earnings\",\"sales\"]) \n    param = HTBparam(cat_features=[\"country\",\"industry\"])","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"Example of use: specify names in dataframe ","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"    data = HTBdata(y,x) #  where x is DataFrame                           \n    param = HTBparam(cat_features=[:country,:industry])         ","category":"page"},{"location":"examples/Categoricals/#How-HTBoost-handles-categorical-features:","page":"Categoricals","title":"How HTBoost handles categorical features:","text":"","category":"section"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"Missing values are assigned to a new category.\nIf there are only 2 categories, a 0-1 dummy is created. For anything more than two categories, it uses a variation of target encoding.\nThe categories are encoded by their mean, frequency and variance. (For financial variables, the variance may be more informative than the mean.)\nSmooth splits are particularly promising with target encoding, particularly in high dimensions (lots of categories). \nOne-hot-encoding with more than 2 categories is not supported, but is easily implemented as data preprocessing.\nIf modality is :accurate or :compromise, a quick and rough cross-validation is performed over two parameters:\nThe prior sample size n0 for target encoding.\nThe penalization attached to categorical features, which mitigates the over-representation of categorical features in the final model due to data leakage. (This seems to be a new approach to the problem: it has the advantage of being fast and using the entire sample. Data leakage remains, and implies that train set loss is lower than validation loss). ","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"An example with simulated data","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"The code below simulate data from y = f(x1,x2) + u, where x1 is continuous and x2 is categorical of possibly very high dimensionality. Each category of x2 is assigned its own coefficient drawn from a distribution (\"uniform\", \"normal\", \"chi\", \"lognormal\"). The user can specify the form of f(x1,x2).","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"LightGBM does not use target encoding, and can completely break down (very poor in-sample and oos fit) when the number of categories is high in relation to n (e.g. n=10k,n_cat=1k). (The LightGBM manual https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html suggests treating high dimensional categorical features as numerical or embedding them in a lower-dimensional space.)","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"CatBoost (note: code in separate Python file), in contrast, adopts target encoding as default, can handle very high dimensionality and has a sophisticated approach to avoiding data leakage which HTBoost is missing. In spite of the absence of data leakage (which would presumably benefit HTBoost as well), in this simple simulation set-up HTBoost substantially outperforms CatBoost if n_cat is high and the categorical feature interacts with the continuous feature, presumably because target encoding generates smooth functions in this set-up.    It seems reasonable to assume that target encoding, by its very nature, will generate smooth functions in most settings, making  HTBoost a promising tool for high dimensional categorical features. The current treatment of categorical features is however quite crude compared to CatBoost, so some of these gains are not yet realized. ","category":"page"},{"location":"examples/Categoricals/","page":"Categoricals","title":"Categoricals","text":"number_workers  = 8  # desired number of workers\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random\nusing Statistics\nusing LightGBM\n\n# USER'S OPTIONS \n\n# Options to generate data \n\nRandom.seed!(1)\n\nn          =     10_000   # sample size   \nncat       =     100     # number of categories (actual number may be lower as they are drawn with reimmission)\n\nbcat       =     1.0      # coeff of categorical feature (if 0, categories are not predictive)\nb1         =     1.0      # coeff of continuous feature \nstde       =     1.0      # error std\n\ncat_distrib =   \"chi\"  # distribution for categorical effects: \"uniform\", \"normal\", \"chi\", \"lognormal\" for U(0,1), N(0,1), chi-square(1), lognormal(0,1)\ninteraction_type = \"step\" # \"none\", \"multiplicative\", \"step\", \"linear\"\n\n# specify the function f(x1,x2), with the type of interaction (if any) between x1 (continuous) and x2 (categorical)\nfunction yhat_x1xcat(b1,b2,x1,interaction_type)\n\n    if interaction_type==\"none\"\n        yhat = b2 + b1*x1\n    elseif interaction_type==\"multiplicative\"     \n        yhat = b2 + b1*b2*x1\n    elseif interaction_type==\"step\"     \n        yhat = b2 + b1*x1*(x1>0)    \n    elseif interaction_type==\"linear\"  \n        yhat = b2 + (b1-b2)*x1\n    end \n\n    return yhat\nend \n\n# HTBoost parameters \nloss         = :L2\nmodality     = :compromise # :accurate, :compromise, :fast, :fastest\ndepth        = 3           # fix depth to speed up estimation  \nnfold        = 1           # number of folds in cross-validation. 1 for fair comparison with LightGBM \nnofullsample = true        # true to speed up execution when nfold=1. true for fair comparison with LightGBM \nverbose      = :Off\n\ncat_features = [2]       # The second feature is categorical. Needs to be an input in param (see below) since it is numerical.  \n\n# LightGBM parameters \n\n# Accoding to the LightGBM manual (https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html):\n# \"For a categorical feature with high cardinality (#category is large), it often works best to treat the feature as numeric, either by simply ignoring the categorical interpretation of the integers or by embedding the categories in a low-dimensional numeric space.\"\n\nignore_cat_lightgbm = false  # true to ignore the categorical nature and treat as numerical in lightGBM \n\n# END USER'S OPTIONS   \n\n# create data \nn_test  = 100_000 \ncate            = collect(1:ncat)[randperm(ncat)]   # create numerical categories (integers) \nxcat,xcat_test  = rand(cate,n),rand(cate,n_test)    # draw element of categorical features from the list of categories\nx,x_test        = hcat(randn(n),xcat),hcat(randn(n_test),xcat_test)\nyhat,yhat_test  = zeros(n),zeros(n_test)\n\nif cat_distrib==\"uniform\"\n    b = bcat*rand(ncat)   # uniform fixed-effects\nelseif cat_distrib==\"normal\"\n    b = bcat*randn(ncat)  # Gaussian fixed effects\nelseif cat_distrib==\"chi\"                        \n    b = bcat*randn(ncat).^2 # chi(1) fixed effects (long tail)\nelseif cat_distrib==\"lognormal\"                        \n    b = bcat*exp.(randn(ncat)) # chi(1) fixed effects (very long tail)\nelse\n    @error \"cat_distribution is misspelled\"\nend \n\nfor r in 1:n\n    b2 = b[findfirst(cate .== xcat[r])]\n    yhat[r] = yhat_x1xcat(b1,b2,x[r,1],interaction_type) \nend\n\nfor r in 1:n_test\n    b2 = b[findfirst(cate .== xcat_test[r])] \n    yhat_test[r] = yhat_x1xcat(b1,b2,x_test[r,1],interaction_type) \nend\n\ny = yhat + stde*randn(n)\ny_test = yhat_test + stde*randn(n_test)\n\n# Fit HTBoost \nparam  = HTBparam(loss=loss,modality=modality,depth=depth,nfold=nfold,nofullsample=nofullsample,verbose=verbose,cat_features=cat_features)\ndata   = HTBdata(y,x,param)\noutput = HTBfit(data,param,cv_grid=[depth])\n\nntrain = Int(round(n*(1-param.sharevalidation)))\nyhat   = HTBpredict(x[1:ntrain,:],output) \nyf     = HTBpredict(x_test,output) \n\nprintln(\"\\n n and number of unique features \", [n length(unique(xcat))])\nprintln(\"\\n HTBoost \" )\nprintln(\"\\n in-sample R2           \", 1 - mean((yhat - y[1:ntrain]).^2)/var(y[1:ntrain])  )\nprintln(\" validation  R2         \", 1 - output.loss/var(y[ntrain+1:end]) )\nprintln(\" out-of-samplesample R2 \", 1 - mean((yf - y_test).^2)/var(y_test) )\n\nprintln(\" Average τ on categorical feature is low, suggesting gains from smoothness. \")\navgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data,verbose=true,best_model=false)\n\n# LightGBM\nif ignore_cat_lightgbm == true\n    estimator = LGBMRegression(objective = \"regression\",num_iterations = 1000,early_stopping_round = 100)\nelse \n    estimator = LGBMRegression(objective = \"regression\",num_iterations = 1000,early_stopping_round = 100,\n            categorical_feature = cat_features)\nend \n\nn_train         = Int(round((1-param.sharevalidation)*length(y)))\nx_train,y_train = x[1:n_train,:], Float64.(y[1:n_train])\nx_val,y_val     = x[n_train+1:end,:], Float64.(y[n_train+1:end])\n\nprintln(\" \\n Running LightGBM, which is usually lightning fast, but can be quite slow with high-dimensional categorical features.\")\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\nyhat_gbm_default = LightGBM.predict(estimator,x)[:,1]\nyf_gbm_default = LightGBM.predict(estimator,x_test)[:,1]\n\nprintln(\"\\n LightGBM default, ignore_cat_lightgbm = $ignore_cat_lightgbm  \" )\nprintln(\"\\n in-sample R2      \", 1 - mean((yhat_gbm_default - y).^2)/var(y)  )\nprintln(\" out-of-sample R2  \", 1 - mean((yf_gbm_default - y_test).^2)/var(y_test) )\n","category":"page"},{"location":"examples/gammaPoisson/#gammaPoisson","page":"gammaPoisson","title":"gammaPoisson","text":"","category":"section"},{"location":"examples/gammaPoisson/","page":"gammaPoisson","title":"gammaPoisson","text":"gammaPoisson for (potentially) overdisperesed count data.","category":"page"},{"location":"examples/gammaPoisson/","page":"gammaPoisson","title":"gammaPoisson","text":"HTBoost with gammaPoisson (aka negative binomial) distribution on simulated data: E(y)=μ(x), var(y)=μ(1+αμ)\nThe overdispersion parameter α is estimated internally. \nloss=:Poisson is also available (α=0)","category":"page"},{"location":"examples/gammaPoisson/","page":"gammaPoisson","title":"gammaPoisson","text":"Note: LightGMB does not have a gammaPoisson option. loss = poisson is used. ","category":"page"},{"location":"examples/gammaPoisson/","page":"gammaPoisson","title":"gammaPoisson","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Plots,Distributions \nusing LightGBM\n\n# USER'S OPTIONS \n\nRandom.seed!(1)\n\n# Some options for HTBoost\nloss      = :gammaPoisson      # :gammaPoisson or Poisson              \nmodality  = :fastest         # :accurate, :compromise (default), :fast, :fastest \n\npriortype = :hybrid       # :hybrid (default) or :smooth to force smoothness \nnfold     = 1             # number of cv folds. 1 faster (single validation sets), default 5 is slower, but more accurate.\nnofullsample = true       # if nfold=1 and nofullsample=true, the model is not re-fitted on the full sample after validation of the number of trees\n \nrandomizecv = false       # false (default) to use block-cv. \nverbose     = :On\nwarnings    = :On\n\n# options to generate data.\nα           = 0.5   # overdispersion parameter. Poisson for α -> 0 \nn,p,n_test  = 50_000,5,100_000\n\n# no interaction terms  \nf_1(x,b)    = b./(1.0 .+ (exp.(2.0*(x .- 1.0) ))) .- 0.1*b \nf_2(x,b)    = b./(1.0 .+ (exp.(4.0*(x .- 0.5) ))) .- 0.1*b \nf_3(x,b)    = b./(1.0 .+ (exp.(7.0*(x .+ 0.0) ))) .- 0.1*b\nf_4(x,b)    = b./(1.0 .+ (exp.(10.0*(x .+ 0.5) ))) .- 0.1*b\n\n#b1,b2,b3,b4 = 0.2,0.2,0.2,0.2\nb1,b2,b3,b4 = 0.6,0.6,0.6,0.6\n\n# generate data\nα>0 ? ddgp = :gammaPoisson : ddgp = :Poisson \nx,x_test = randn(n,p), randn(n_test,p)\n\nc        = 0    #  \nf        = c .+ f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4)\nf_test   = c .+ f_1(x_test[:,1],b1) + f_2(x_test[:,2],b2) + f_3(x_test[:,3],b3) + f_4(x_test[:,4],b4)\n\nμ        = exp.(f)        # conditional mean \nμ_test   = exp.(f_test)   # conditional mean \n\ny        = zeros(n)\ny_test   = zeros(n_test)\n\nif ddgp==:gammaPoisson\n    for i in eachindex(y)\n        r = 1/α                    # gammaPoisson is negative binomial with r=1\\α, and p = r./(μ .+ r)\n        pg = r./(μ .+ r)\n        y[i] = rand(NegativeBinomial(r,pg[i]))     \n    end \nelse \n    α = 0.0\n    for i in eachindex(y)\n        y[i]  = rand(Poisson.(μ[i]))\n    end \nend \n\n\nhistogram(y)\n@show [mean(y), std(y), std(μ), maximum(y)]\n\n# set up HTBparam and HTBdata, then fit and predit\n\n# coefficient estimated internally. \nparam  = HTBparam(loss=loss,priortype=priortype,randomizecv=randomizecv,nfold=nfold,\n                   verbose=verbose,warnings=warnings,modality=modality,nofullsample=nofullsample)\ndata   = HTBdata(y,x,param)\n\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output,predict=:Ey)\n\nprintln(\" \\n loss = $(param.loss), modality = $(param.modality), nfold = $nfold \")\nprintln(\" depth = $(output.bestvalue), number of trees = $(output.ntrees) \")\nprintln(\" out-of-sample RMSE from true μ     \", sqrt(sum((yf - μ_test).^2)/n_test) )\nprintln(\" out-of-sample MAD from true μ      \", mean(abs.(yf - μ_test)) )\n\n\nprintln(\"\\n true overdispersion = $α, estimated = $(exp(output.bestparam.coeff_updated[1][1])) \")\n\nprintln(\"\\n For more information about coefficients, use HTBcoeff(output) \")\nHTBcoeff(output)\n\n# lightGBM\n\nestimator = LGBMRegression(\n    objective = \"poisson\",\n    metric = [\"poisson\"],    # default seems (strangely) \"l2\" regardless of objective: LightGBM.jl bug?  \n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    num_threads = number_workers\n)\n\nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n\n# fit at parameters given by estimator, no cv\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\n    \nyf_gbm_default = LightGBM.predict(estimator,x_test)\n\n# parameter search over num_leaves and max_depth\nsplits = (collect(1:n_train),collect(1:min(n_train,100)))  # goes around the problem that at least two training sets are required by search_cv (we want the first)\n\nparams = [Dict(:num_leaves => num_leaves,\n               :max_depth => max_depth) for\n          num_leaves in (4,16,32,64,127,256),\n          max_depth in (2,3,5,6,8)]\n\nlightcv = LightGBM.search_cv(estimator,x,y,splits,params,verbosity=-1)\n\nloss_cv = [lightcv[i][2][\"validation\"][estimator.metric[1]][1] for i in eachindex(lightcv)]\nminind = argmin(loss_cv)\n\nestimator.num_leaves = lightcv[minind][1][:num_leaves]\nestimator.max_depth  = lightcv[minind][1][:max_depth]\n\n# re-fit at cv parameters\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\n    \nyf_gbm = LightGBM.predict(estimator,x_test)\nyf_gbm = yf_gbm[:,1]    # drop the second dimension or a (n_test,1) matrix \n\nprintln(\"\\n oss RMSE from truth, μ, LightGBM default \", sqrt(sum((yf_gbm_default - μ_test).^2)/n_test) )\nprintln(\" oss RMSE from true μ, LightGBM cv      \", sqrt(sum((yf_gbm - μ_test).^2)/n_test) )\nprintln(\" oos MAD from true μ, LightGBM cv       \", mean(abs.(yf_gbm - μ_test)) )\n\n# HTBoost partial plots\n\nq,pdp  = HTBpartialplot(data,output,[1,2,3,4],predict=:Egamma)\n\n# plot partial dependence in terms of the natural parameter \npl   = Vector(undef,4)\nf,b  = [f_1,f_2,f_3,f_4],[b1,b2,b3,b4]\n\nfor i in 1:length(pl)\n        pl[i]   = plot( [q[:,i]],[pdp[:,i] f[i](q[:,i],b[i]) - f[i](q[:,i]*0,b[i])],\n           label = [\"HTB\" \"dgp\"],\n           legend = :bottomright,\n           linecolor = [:blue :red],\n           linestyle = [:solid :dot],\n\n           linewidth = [5 5],\n           titlefont = font(15),\n           legendfont = font(12),\n           xlabel = \"x\",\n           ylabel = \"f(x)\",\n           )\nend\n\ndisplay(plot(pl[1], pl[2], pl[3], pl[4], layout=(2,2), size=(1300,800)))  # display() will show it in Plots window.\n","category":"page"},{"location":"tutorials/Basic_use/#Basic-use","page":"-","title":"Basic use","text":"","category":"section"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Summary","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Illustrates use of the main functions of HTBoost on a regression problem with simulated data. \nparam.modality as the most important user's choice, depending on time budget. \nIn default modality, HTBoost performs automatic hyperparameter tuning.","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Main points ","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"default loss is :L2. Other options for continuous y are :Huber, :t (recommended in place of :Huber), :gamma, :gammaPoisson, :L2loglink. For zero-inflated continuous y, options are :hurdleGamma, :hurdleL2loglink, :hurdleL2   \ndefault is block cross-validation with nfolds=4: use randomizecv = true to scramble the data. See Time series and panels and Global Equity Panel for further options on cross-validation (e.g. sequential cv, or generally controlling the training and validation sets).","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"fit, with automatic hyperparameter tuning if modality is :compromise or :accurate\nsave fitted model (upload fitted model)\naverage τ (smoothness parameter), which is also plotted. (Smoother functions ==> larger gains compared to other GBM)\nfeature importance\npartial effects plots","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Import HTBoost for distributed parallelization on the desired number of workers.","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"This step is not required by other GMBs, which rely on shared parallelization.   The time to first plot increases with the number of cores. HTBoost parallelizes well up to 8 cores, and quite well up to 16 if p/#cores is sufficiently high. ","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"number_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\n# import required packages for this script; JLD2 only required to save and import output\nusing Random,Plots\nusing JLD2            \n","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Options for HTBparam( ).  ","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"I prefer to specify parameter settings separately (here at the top of the script) rather than directly in HTBparam( ), which is of course also possible.   modality is the key parameter: automatic hyperparameter tuning if modality is :compromise or :accurate, no tuning (except of #trees) if :fast or :fastest.     In HTBoost, it is not recommended that the user performs  hyperparameter tuning by cross-validation, because this process is done automatically if modality is :compromise or :accurate. The recommended process is to first run in modality=:fast or :fastest, for exploratory analysis and to gauge computing time, and then switch to :compromise (default) or :accurate. For a tutorial on user-controlled cross-validation, see User's controlled cross-validation.","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"\nRandom.seed!(1)\n\n# Options for HTBparam()\nloss      = :L2            # :L2 is default. Other options for regression are :L2loglink (if y≥0), :t, :Huber\nmodality  = :fastest        # :accurate, :compromise (default), :fast, :fastest \npriortype = :sharp       # :hybrid (default) or :smooth to force smoothness (typically not recommended)\nnfold     = 1             # number of cv folds (default 4) \nnofullsample = true       # if true and nfold=1, no re-fitting on the full sample after validation\n\nverbose     = :Off\nwarnings    = :On","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Options for cross-validation:","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"While the default in other GBM is to randomize the allocation to train and validation sets, the default in HTBoost is block cv, which is suitable for time series and panels. Set randomizecv=true to bypass this default.  See Global Equity Panel for further options on cross-validation (e.g. sequential cv, or generally controlling the training and validation sets).","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"randomizecv = false       # false (default) to use block-cv. \n","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Options to generate data.","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"y is the sum of six additive nonlinear functions, plus Gaussian noise.","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"n,p,n_test  = 10_000,6,100_000\nstde        = 1.0\n\nf1(x,b)    = @. b*x + 1 \nf2(x,b)    = @. 2*sin(2.5*b*x)  \nf3(x,b)    = @. b*x^3\nf4(x,b)    = @. b*(x < 0.5) \nf5(x,b)    = @. b/(1.0 + (exp(4.0*x )))\nf6(x,b)    = @. b*(-0.25 < x < 0.25) \n\nb1,b2,b3,b4,b5,b6 = 1.5,2.0,0.5,4.0,5.0,5.0\n","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"End of user's options.  ","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"# Generate data.\n\nx,x_test = randn(n,p), randn(n_test,p)\n\nf      = f1(x[:,1],b1)+f2(x[:,2],b2)+f3(x[:,3],b3)+f4(x[:,4],b4)+f5(x[:,5],b5)+f6(x[:,6],b6)\nf_test = f1(x_test[:,1],b1)+f2(x_test[:,2],b2)+f3(x_test[:,3],b3)+f4(x_test[:,4],b4)+f5(x_test[:,5],b5)+f6(x_test[:,6],b6)\ny      = f + stde*randn(n)\n\n# set up HTBparam and HTBdata, then fit and predit\nparam  = HTBparam(loss=loss,priortype=priortype,randomizecv=randomizecv,nfold=nfold,verbose=verbose,\n                warnings=warnings,modality=modality,nofullsample=nofullsample)\n\ndata   = HTBdata(y,x,param)\n\n@time output = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)  \n\n# save (load) fitted model with JLD2 \n\n@save \"output.jld2\" output\n@load \"output.jld2\" output    ","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Feature importance and average smoothing parameter for each feature.  ","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"tau is the smoothness parameter; lower values give smoother functions, while tau=Inf is a sharp split (tau is truncated at 40 for this function).   avgtau is a summary of the smoothness of f(x), with features weighted by their importance. avgtau_a is a vector array with the importance weighted tau for each feature.  ","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"avgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data,verbose=true);\n\nplot(x_plot,g_plot,title=\"avg smoothness of splits\",xlabel=\"standardized x\",label=:none)","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"The plot gives an idea of the average (importance weighted) smoothness across all splits.... ","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"<img src=\"../assets/avgtau.png\" width=\"400\" height=\"250\">","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"... which in this case is a mix of very different values across features: approximate linearity for x1, smooth functions for x3 and x5, and essentially sharp splits for x2, x4, and x6:","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":" Row │ feature  importance  avgtau     sorted_feature  sorted_importance  sorted_avgtau \n     │ String   Float32     Float64    String          Float32            Float64\n─────┼──────────────────────────────────────────────────────────────────────────────────\n   1 │ x1          14.5666   0.458996  x3                        19.0633       3.30638\n   2 │ x2          12.9643  19.6719    x5                        18.6942       3.72146\n   3 │ x3          19.0633   3.30638   x6                        17.9862      35.1852\n   4 │ x4          16.7254  36.0846    x4                        16.7254      36.0846\n   5 │ x5          18.6942   3.72146   x1                        14.5666       0.458996\n   6 │ x6          17.9862  35.1852    x2                        12.9643      19.6719\n\n Average smoothing parameter τ is 7.3.\n\n In sufficiently large samples, and if modality=:compromise or :accurate\n\n - Values above 20-25 suggest very little smoothness in important features. HTBoost's performance may slightly outperform or slightly underperform other gradient boosting machines.\n - At 10-15 or lower, HTBoost should outperform other gradient boosting machines, or at least be worth including in an ensemble.\n - At 5-7 or lower, HTBoost should strongly outperform other gradient boosting machines.","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Some examples of smoothness corresponding to a few values of tau (for a single split) help to interpret values of avgtau","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"<img src=\"../assets/Sigmoids.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"On simulated data, we can evaluate the RMSE from the true f(x), exluding noise:","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"\nprintln(\" \\n modality = $(param.modality), nfold = $nfold , priortype = $(param.priortype)\")\nprintln(\" depth = $(output.bestvalue), number of trees = $(output.ntrees), gavgtau $gavgtau \")\nprintln(\" out-of-sample RMSE from truth \", sqrt(sum((yf - f_test).^2)/n_test) )","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Hybrid trees outperform both smooth and standard trees","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Here is the output for n=10k (nfold=1, nofullsample=true).  Hybrid trees strongly outperform both smooth trees and standard symmetric (aka oblivious) trees. (Note: modality = :sharp is a very inefficient way to run a symmetric tree; use CatBoost or EvoTrees instead!)","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"  modality = fastest, nfold = 1, priortype = hybrid\n depth = 5, number of trees = 141, gavgtau 7.3\n out-of-sample RMSE from truth 0.3136\n\nmodality = fastest, nfold = 1, priortype = smooth \n depth = 5, number of trees = 121, gavgtau 4.5\n out-of-sample RMSE from truth 0.5751\n\n modality = fastest, priortype = sharp\ndepth = 5, number of trees = 183, avgtau 40.0\n out-of-sample RMSE from truth 0.5320","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Feature importance and partial dependence plots","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Variable (feature) importance is computed as in Hastie et al., \"The Elements of Statistical Learning\", second edition, except that the normalization is for sum=100.   Partial dependence assumes (in default) that other features are kept at their mean.","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data,verbose=false);\nq,pdp  = HTBpartialplot(data,output,[1,2,3,4,5,6]) # partial effects for the first 6 variables \n\n# plot partial dependence\npl   = Vector(undef,6)\nf,b  = [f1,f2,f3,f4,f5,f6],[b1,b2,b3,b4,b5,b6]\n\nfor i in 1:length(pl)\n    pl[i]   = plot( [q[:,i]],[pdp[:,i] f[i](q[:,i],b[i]) - f[i](q[:,i]*0,b[i])],\n           label = [\"HTB\" \"true\"],\n           legend = :bottomright,\n           linecolor = [:blue :red],\n           linestyle = [:solid :dot],\n           linewidth = [6 3],\n           titlefont = font(15),\n           legendfont = font(12),\n           xlabel = \"x\",\n           ylabel = \"f(x)\"\n           )\nend\n\ndisplay(plot(pl[1],pl[2],pl[3],pl[4],pl[5],pl[6],layout=(3,2), size=(1300,800)))","category":"page"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"Partial plots for n = 1k,10k,100k, with modality = :fastest and nfold = 1.    Notice how plots are smooth only for some features. ","category":"page"},{"location":"tutorials/Basic_use/#n-1_000","page":"-","title":"n = 1_000","text":"","category":"section"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"<img src=\"../assets/Minimal1k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"tutorials/Basic_use/#n-10_000","page":"-","title":"n = 10_000","text":"","category":"section"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"<img src=\"../assets/Minimal10k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"tutorials/Basic_use/#n-100_000","page":"-","title":"n = 100_000","text":"","category":"section"},{"location":"tutorials/Basic_use/","page":"-","title":"-","text":"<img src=\"../assets/Minimal100k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"#HTBoost","page":"Introduction","title":"HTBoost","text":"","category":"section"},{"location":"#Data-efficient-boosting-with-hybrid-trees","page":"Introduction","title":"Data efficient boosting with hybrid trees","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A Julia implementation of Hybrid Trees Boosting as described in [HTBoost][! LINK TO HTBoost paper!] ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"HTBoost is slower than other boosting packages, but the use of hybrid trees (an evolution of the smooth trees in SMARTboost) delivers superior accuracy in many situations, making HTBoost a promising tool, particularly when data is limited or very noisy. ","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Latest:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"pkg> add \"https://github.com/PaoloGiordani/HTBoost.jl\"    ","category":"page"},{"location":"#For-R-and-Python","page":"Introduction","title":"For R and Python","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"I am working on solutions for R and Python users. Coming soon ... ","category":"page"},{"location":"#Documentation","page":"Introduction","title":"Documentation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Parameters\nAPI\nTutorials (including comparison with LightGBM) \nExamples (including comparison with LightGBM) ","category":"page"},{"location":"#Minimal-example","page":"Introduction","title":"Minimal example","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"\n# set desired number of workers for parallelization and load HTBoost on all workers\nnumber_workers  = 8      \nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random \n\n# define data-generating-process and simulate data  \nn,p    = 1_000,6\nstde   = 1.0\n\nf_1(x,b)    = @. b*x + 1 \nf_2(x,b)    = @. 2*sin(2.5*b*x)  \nf_3(x,b)    = @. b*x^3\nf_4(x,b)    = @. b*(x < 0.5) \nf_5(x,b)    = @. b/(1.0 + (exp(4.0*x )))\nf_6(x,b)    = @. b*(-0.25 < x < 0.25) \n\nb1,b2,b3,b4,b5,b6 = 1.5,2.0,0.5,4.0,5.0,5.0\n\nx = randn(n,p)\nf = f_1(x[:,1],b1)+f_2(x[:,2],b2)+f_3(x[:,3],b3)+f_4(x[:,4],b4)+f_5(x[:,5],b5)+f_6(x[:,6],b6)\ny = f + stde*randn(n)\n\n# set up HTBparam and HTBdata, then fit and predit\nparam  = HTBparam(loss=:L2)       \ndata   = HTBdata(y,x,param)\n\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)  \n","category":"page"},{"location":"#Minimal-example-with-n-1_000","page":"Introduction","title":"Minimal example with n = 1_000","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"<img src=\"./assets/Minimal1k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"#Minimal-example-with-n-10_000","page":"Introduction","title":"Minimal example with n = 10_000","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"<img src=\"./assets/Minimal10k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"#Minimal-example-with-n-100_000","page":"Introduction","title":"Minimal example with n = 100_000","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"<img src=\"./assets/Minimal100k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"#Main-features-and-advantages-of-HTBoost","page":"Introduction","title":"Main features and advantages of HTBoost","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Hybrid trees build on smooth trees, which are more accurate than standard trees if f(x) is smooth wrt at least some of the features, but can escape local minima that occasionally trap boosted smooth trees. See Hybrid Tree.\nHybrid trees also refine each tree with a modified single-index model, which allows them to more efficiently capture some types of data on which standard trees struggle. See Projection Pursuit Regression. For more on when HTBoost can be expected to outperform other GBMs, see Outperforming other GBM.\nEase of use: a parsimonious cross-validation of the most important parameters is performed automatically if modality = :compromise or :accurate, while modality = :fast and :fastest fit just one model at default parameters.\nAdapts to both dense and sparse settings. Unless n/p is large, one of the parameters being cross-validated is a sparsity-inducing penalization, which can result in more aggressive variable selection compared to standard boosting.\nAdditional coefficients (e.g. overdispersion for gammaPoisson, shape for Gamma, dof for t) are estimated internally by maximum likelihood; no user's input or cv required.\nThe exact loss function is typically evaluated, instead of using a quadratic approximation as in other GBMs. This contributes to improved accuracy with small n or low SNR.\nVery efficient at dealing with missing values internally.\nIdeal for time series and longitudinal data (aka panel data).\nImproved inference for Huber and t loss. \nAvailable loss functions cover most cases for regression, classification (binary and multi), count data, zero-inflated data. Ranking can be performed with a regression approach, where appropriate (see L2loglink and ranking)","category":"page"},{"location":"#Main-limitations-of-HTBoost","page":"Introduction","title":"Main limitations of HTBoost","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Slower training than other packages for GBMs.\nDeep trees are particularly slow.\nMemory intensive in the current implementation. ","category":"page"},{"location":"#Recommended-workflow","page":"Introduction","title":"Recommended workflow","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Start exploratory analysis with modality = :fast (or even :fastest unless the sample is very small), then switch to modality = :compromise (default) or :accurate for best accuracy. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"param  = HTBparam(modality=:fastest)       \ndata   = HTBdata(y,x,param)\noutput = HTBfit(data,param)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"See speeding up HTBoost for suggestions on how to handle large n if computing time becomes a constraint.","category":"page"},{"location":"#Help-improve-HTBoost","page":"Introduction","title":"Help improve HTBoost","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"If you have a dataset in which HTBoost does not outperform other GBMs (particularly if HTBweightedtau( ) suggests it should, see Basic use), and you have read Outperforming other GBM, please get in touch with me at paolo.giordani@bi.no\nSuggestions are welcome.","category":"page"},{"location":"#References","page":"Introduction","title":"References","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Paolo Giordani, 2025, \"HTBoost: Data Efficient Learning via Hybrid Tree Boosting\"","category":"page"},{"location":"tutorials_R/Time_series_and_panels/#Time-series-and-panels","page":"-","title":"Time series and panels","text":"","category":"section"},{"location":"tutorials_R/Time_series_and_panels/#Working-with-time-series-and-longitudinal-data-(panels).-Data-as-DataFrame.","page":"-","title":"Working with time series and longitudinal data (panels). Data as DataFrame.","text":"","category":"section"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"The user only needs to provide features and, optionally, a vector of dates in HTBdata( ) and, if there is overlapping, the overlap parameter in HTBparam().   Example:","category":"page"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"param  = HybridTreeBoosting$HTBparam(overlap=20)         \ndata   = HybridTreeBoosting$HTBdata(y,x,param,dates,fnames = fnames)","category":"page"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"There is no need to specify dates for an individual time series. For a panel, dates is required for correct default block-cv (but not if user-specified train and test samples are provided in indtraina and indtesta).","category":"page"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"Overlap defaults to 0. Typically overlap = h-1, where y(t) = Y(t+h)-Y(t) (e.g. Y a log price and h=20 for monthly financial returns from daily data). This is used for purged-CV and to calibrate the priors.","category":"page"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"By default, HTBoost uses block-cv, which is suitable for time series and longitudinal data.  Another good alternative for times series and panels is expanding window cross-validation, which requires the user to provide indtraina and indtesta in HTBparam( ). The function HTBindexesfromdates() can assist in building indtraina and indtesta.","category":"page"},{"location":"tutorials_R/Time_series_and_panels/#Preliminary-steps-required-in-all-scripts","page":"-","title":"Preliminary steps required in all scripts","text":"","category":"section"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"See Basic_use","category":"page"},{"location":"tutorials_R/Time_series_and_panels/#Some-user's-inputs-for-this-dataset","page":"-","title":"Some user's inputs for this dataset","text":"","category":"section"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"\n# data\nlog_ret = FALSE    # TRUE to predict log returns, FALSE (default) to predict returns\noverlap = 0        # 0 for non-overlapping (default), h-1 for overlapping data, where h is the forecast horizon. \n\n# HTBoost\n\nloss = \"L2\"  # if log_ret=FALSE, consider \"L2loglink\" instead of \"L2\"\nmodality = \"accurate\"    # \"accurate\", \"compromise\", \"fast\", \"fastest\"\npriortype = \"hybrid\"     # \"hybrid\" (accurate), \"smooth\" (forces smooth split)\n\ncv_type = \"expanding\"  # \"block\" (default) or \"expanding\" or \"randomized\" (not recommended for time series and panels)\nnfold = 4          # number of folds for cv (default 4). Irrelevant if cv_type = \"expanding\".\n\n# for cv_type = \"expanding\" \ncv_first_date    = 197001   # start date for expanding window cv. Another example (Julia code): first_date = Date(\"2017-12-31\", Dates.DateForma(\"y-m-d\"))       \ncv_block_periods = 120      # number of periods (months in this dataset): if cv_type=\"block\", this is the block size\n","category":"page"},{"location":"tutorials_R/Time_series_and_panels/#Import-and-prepare-data","page":"-","title":"Import and prepare data","text":"","category":"section"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"\nlibrary(data.table)\n\ndf = fread(\"HTBoost/examples/data/GlobalEquityReturns.csv\") # import data as dataframe. Monthly LOG excess returns.\n\ny = if (log_ret) 100 * df$excessret else 100 * (exp(df$excessret))\nfnames = c(\"logCAPE\", \"momentum\", \"vol3m\", \"vol12m\")    # will be an input (optional) to HTBdata().\n\n# select features, and form a Julia DataFrame \nfeatures_vector = c(\"logCAPE\", \"momentum\", \"vol3m\", \"vol12m\")\nx = DataFrames$DataFrame(df[, ..features_vector])     \n\n# Translate the R dataframe to a Julia DataFrame\nx = DataFrames$DataFrame(x)\n","category":"page"},{"location":"tutorials_R/Time_series_and_panels/#Set-up-models-depending-on-selected-option-for-cross-validation.-Fit,-print-some-output.","page":"-","title":"Set up models depending on selected option for cross-validation. Fit, print some output.","text":"","category":"section"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"The panel does not need to be chronologically sorted.","category":"page"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"\nif (cv_type == \"randomized\") {\n  param = HybridTreeBoosting$HTBparam(nfold=nfold, overlap=overlap, loss=loss, modality=modality, priortype=priortype, randomizecv=TRUE)\n} else if (cv_type == \"block\") {   # default \n  param = HybridTreeBoosting$HTBparam(nfold=nfold, overlap=overlap, loss=loss, modality=modality, priortype=priortype)\n} else if (cv_type == \"expanding\") {\n  df_julia = DataFrames$DataFrame(df)   # dataframe including \"dates\" \n  indtrain_a = HybridTreeBoosting$HTBindexes_from_dates(df_julia,\"dates\", cv_first_date, cv_block_periods)$indtrain_a\n  indtest_a = HybridTreeBoosting$HTBindexes_from_dates(df_julia,\"dates\", cv_first_date, cv_block_periods)$indtest_a\n  param = HTBparam(nfold=nfold, overlap=overlap, loss=loss, modality=modality, priortype=priortype, indtrain_a=indtrain_a, indtest_a=indtest_a)\n}\n\ndata = HybridTreeBoosting$HTBdata(y,x,param,dates,fnames=fnames)\noutput = HybridTreeBoosting$HTBfit(data, param)\n\nyhat = HybridTreeBoosting$HTBpredict(x, output)  # in-sample fitted value.\n\ncat(\"\\n depth =\", output$bestvalue, \", number of trees =\", output$ntrees, \"\\n\")\ncat(\" in-sample R2 =\", round(1.0 - sum((y - yhat)^2) / sum((y - mean(y))^2), digits=3), \"\\n\")\n","category":"page"},{"location":"tutorials_R/Time_series_and_panels/#Feature-importance-and-smoothness;-partial-dependence-plot","page":"-","title":"Feature importance and smoothness; partial dependence plot","text":"","category":"section"},{"location":"tutorials_R/Time_series_and_panels/","page":"-","title":"-","text":"See Basic_use","category":"page"},{"location":"Tutorials/#HTBoost-Tutorials","page":"Tutorials (Julia)","title":"HTBoost Tutorials","text":"","category":"section"},{"location":"Tutorials/","page":"Tutorials (Julia)","title":"Tutorials (Julia)","text":"The following tutorials cover hands-on use of HTBoost. The examples provide more cases of different loss functions as well as illustrations to understand what HTBoost does and how it differs from other GBMs.","category":"page"},{"location":"Tutorials/#Most-important-user-cases","page":"Tutorials (Julia)","title":"Most important user cases","text":"","category":"section"},{"location":"Tutorials/","page":"Tutorials (Julia)","title":"Tutorials (Julia)","text":"Basic use (main options, cv, savings and loading results, variable importance and more post-estimation analysis)\nLogistic regression (binary classification; comparison with LightGBM)\nTime series and panels (Data Frames, time series and panels/longitudinal data, with various options for cv)\nCategorical features (how HTBoost handles categorical features; comparison with LightGBM and CatBoost)\nMissing data (HTBoost excels at handling missing data)\nSpeeding up HTBoost with large n (strategies to reduce computing time for large n)\nUser's controlled cross-validation( when to go beyond HTBfit for cross-validation)","category":"page"},{"location":"Tutorials/#Other-distributions-(loss-functions)","page":"Tutorials (Julia)","title":"Other distributions (loss functions)","text":"","category":"section"},{"location":"Tutorials/","page":"Tutorials (Julia)","title":"Tutorials (Julia)","text":"Multiclass (multiclass classification; comparison with LightGBM)\nZero inflated y (loss functions for zero-inflated y; comparison with LightGBM)\nPoisson and GammaPoisson (aka negative binomial for count data; comparison with LightGBM)  \nL2loglink and rank (a new option if min(y)≥0, whether continuous, count, or rank)\nRobust regression (student-t and Huber; comparison with LightGBM)","category":"page"},{"location":"Tutorials/#Others","page":"Tutorials (Julia)","title":"Others","text":"","category":"section"},{"location":"Tutorials/","page":"Tutorials (Julia)","title":"Tutorials (Julia)","text":"Average tau (interpreting average tau values and break-down by feature)\nOffset (exposure) (how to add an offset, common in e.g. insurance, biology ...)\nBeyond accurate (some suggestions when chasing maximum accuracy)","category":"page"},{"location":"examples/Projection_pursuit_regression/#Hybrid-trees-incorporate-elements-of-projection-pursuit-regression","page":"Hybrid trees incorporate elements of projection pursuit regression","title":"Hybrid trees incorporate elements of projection pursuit regression","text":"","category":"section"},{"location":"examples/Projection_pursuit_regression/","page":"Hybrid trees incorporate elements of projection pursuit regression","title":"Hybrid trees incorporate elements of projection pursuit regression","text":"A simulated example in which adding a single index model to each tree improves accuracy","category":"page"},{"location":"examples/Projection_pursuit_regression/","page":"Hybrid trees incorporate elements of projection pursuit regression","title":"Hybrid trees incorporate elements of projection pursuit regression","text":"The boosted sequence of single index models builds a projection pursuit regressio of sort. \nData is simulated from f(z), where z is a linear combination of the features and f() is ReLu.\nAdding a projection pursuit regression to each tree improves accuracy considerably in this admittedly  artificial example.\nLightGBM and XGB struggle with this type of data, typically requiring many trees and delivering poor accuracy.","category":"page"},{"location":"examples/Projection_pursuit_regression/","page":"Hybrid trees incorporate elements of projection pursuit regression","title":"Hybrid trees incorporate elements of projection pursuit regression","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n#@everywhere using HybridTreeBoosting\n\nusing Random,Plots \nusing LightGBM\n\n# USER'S OPTIONS \n\n# Some options for HTBoost\nloss      = :L2            # :L2 or :sigmoid (or :Huber or :t). \nmodality  = :compromise     # :accurate, :compromise (default), :fast, :fastest \n\npriortype = :hybrid       # :hybrid (default) or :smooth to force smoothness \nnfold     = 1             # number of cv folds. 1 faster (single validation sets), default 4 is slower, but more accurate.\nnofullsample = true       # if nfold=1 and nofullsample=true, the model is not re-fitted on the full sample after validation of the number of trees\n\nrandomizecv = false       # false (default) to use block-cv. \nverbose     = :Off\nwarnings    = :Off\n \n# options to generate data. y = sum of six additive nonlinear functions + Gaussian noise.\nn,n_test  = 10_000,100_000\nstde      = 1.0\nb         = 5.0\nrndseed   = 1234\n\n# END USER'S OPTIONS\n\nfunction simulatedata(n,b,stde;rndseed=1)\n    \n    Random.seed!(rndseed)\n\n    x     = randn(n,5)\n    z     = 2.5*x[:,1] + 2.0*x[:,2] + 1.5*x[:,3] + 1.0*x[:,4] + 0.5*x[:,5]\n\n    f = b*z.*(z.>0)\n    y = f + stde*randn(n)\n\n    return y,x,f \n\nend \n\ny,x,f             = simulatedata(n,b,stde,rndseed=rndseed)\ny_test,x_test,f_test = simulatedata(100_000,b,stde,rndseed=rndseed)\n\n# set up HTBparam and HTBdata, then fit and predit\nparam  = HTBparam(loss=loss,priortype=priortype,randomizecv=randomizecv,nfold=nfold,verbose=verbose,warnings=warnings,\n           modality=modality,nofullsample=nofullsample,lambda=lambda)\n\ndata   = HTBdata(y,x,param)\n\nfor depthppr in [0,2]\n\n    param.depthppr = depthppr\n\n    @time local output = HTBfit(data,param)\n    local yf  = HTBpredict(x_test,output) \n\n    println(\" \\n modality = $(param.modality), nfold = $nfold, depthppr=$(param.depthppr) \")\n    println(\" depth = $(output.bestvalue), number of trees = $(output.ntrees) \")\n    println(\" out-of-sample RMSE from truth \", sqrt(sum((yf - f_test).^2)/n_test) )\n\n   if param.depthppr>0    # visualize impact of projection pursuit transformation on first tree\n      yf1,yf0,tau = HTBppr_plot(output,which_tree=1)\n      plot(yf0,yf1,title=\"depthppr=$(param.depthppr)\")\n   end\n\n\nend \n\n# LightGBM \nestimator = LGBMRegression(objective = \"regression\",num_iterations = 1000,early_stopping_round = 100)\n\ny       = Float64.(data.y)                 \nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n\n# parameter search over num_leaves and max_depth\nsplits = (collect(1:n_train),collect(1:min(n_train,100)))  # goes around the problem that at least two training sets are required by search_cv (we want the first)\n\nparams = [Dict(:num_leaves => num_leaves,\n           :max_depth => max_depth) for\n          num_leaves in (4,16,64,256),\n         max_depth in (2,4,6,8)]\n\nprintln(\"\\n running lightGBM\")\n@time lightcv = LightGBM.search_cv(estimator,x,y,splits,params,verbosity=-1)\n\nloss_cv = [lightcv[i][2][\"validation\"][estimator.metric[1]][1] for i in eachindex(lightcv)]\nminind = argmin(loss_cv)\n\nestimator.num_leaves = lightcv[minind][1][:num_leaves]\nestimator.max_depth  = lightcv[minind][1][:max_depth]\n\n# fit at cv parameters\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\nyf_gbm = LightGBM.predict(estimator,x_test)[:,1]\n\nprintln(\" lightgbm depth = $(estimator.max_depth), nleaves = $(estimator.num_leaves) \")\nprintln(\" out-of-sample RMSE lightgbm  \", sqrt(sum((yf_gbm - f_test).^2)/n_test) )\n","category":"page"},{"location":"tutorials/Logistic/#Logistic-regression-(binary-classification-with-logloss)","page":"-","title":"Logistic regression (binary classification with logloss)","text":"","category":"section"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Summary","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Comparison with lightGBM on a logistic regression problem with simulated data. \nparam.modality as the most important user's choice.\nIn default modality, HTBoost performs automatic hyperparameter tuning.","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Main points ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Sketch of a comparison of HTBoost and lightGBM on a logistic regression problem. The comparison with LightGBM will favor HTBoost if the function generating the data is  smooth in some features (this is easily changed by the user). lightGBM is cross-validated over maxdepth and numleaves, with the number of trees set to 1000 and found by early stopping.","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Import HTBoost for distributed parallelization on the desired number of workers.","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"This step is not required by other GMBs, which rely on shared parallelization.   The time to first plot increases with the number of cores. HTBoost parallelizes well up to 8 cores, and quite well up to 16 if p/#cores is sufficiently high. ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\n# import required packages for this script\nusing Random,Statistics\nusing Plots\nusing LightGBM\n","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Options for HTBparam( ).  ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Options for HTBoost: modality is the key parameter guiding hyperparameter tuning and learning rate. :fast and :fastest only fit one model at default parameters, while :compromise and :accurate perform automatic hyperparameter tuning. In HTBoost, it is not recommended that the user performs  hyperparameter tuning by cross-validation, because this process is done automatically if modality is :compromise or :accurate. The recommended process is to first run in modality=:fast or :fastest, for exploratory analysis and to gauge computing time, and then switch to :compromise (default) or :accurate.","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"\nRandom.seed!(1)\n\n# Options for HTBparam()\nloss      = :logistic\nmodality  = :compromise   # :accurate, :compromise (default), :fast, :fastest\nverbose   = :Off\nwarnings  = :On\n","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Options for cross-validation:","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"While the default in other GBM is to randomize the allocation to train and validation sets, the default in HTBoost is block cv, which is suitable for time series and panels. Set randomizecv=true to bypass this default.  See Global Equity Panel for further options on cross-validation (e.g. sequential cv).","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"randomizecv = false       # false (default) to use block-cv. \n","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Options for data generation ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Define the function f(x), where x are indendent N~(0,1), and f(x) is for the natural parameter, so f(x) = log(prob/(1-prob))","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"\nn         = 10_000\np         = 10           # mumber of features. p>=4.  \nn_test    = 100_000   \n\nf_1(x,b)    = b*x .+ 1 \nf_2(x,b)    = sin.(b*x)  \nf_3(x,b)    = b*x.^2\nf_4(x,b)    = b./(1.0 .+ (exp.(10*(x .- 0.5) )))    \n\nb1,b2,b3,b4 = 1.5,2.0,0.5,2.0\ndgp(x)       = f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4)\n","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"End of user's options.","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Build model for HTBoost and LightGBM. In HTBparam( ), nfold=1 and nofullsample=true for a fair comparison with LightGBM (both models have the sample training and validation sets.) The validation set is the last 30% of the data. (Change param.sharevalidation to change this percentage, and randomizecv=true for a random subsample.)","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"\nparam  = HTBparam(loss=loss,nfold=1,nofullsample=true,modality=modality,warnings=warnings)\n\n estimator = LGBMClassification(   # LGBMRegression(...)\n    objective = \"binary\",\n    num_class = 1,          \n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    metric = [\"binary_logloss\"],\n    num_threads = number_workers\n )\n","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Generate data and fit both models","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"In HTBpredict, predict=:Ey (default) predicts E(y|x), i.e. prob(y=1|x), while predict=:Egamma predicts the natural parameter. Here we choose the latter for a less noisy comparison, since we are working with simulated data.  ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"\nx,x_test    = randn(n,p), randn(n_test,p)\nftrue       = dgp(x)\nftrue_test  = dgp(x_test)\n\ny = (exp.(ftrue)./(1.0 .+ exp.(ftrue))).>rand(n) \n\n# HTBoost\ndata   = HTBdata(y,x,param)\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output,predict=:Egamma)  # NOTE: on real data we'll want to predict = :Ey (the default)   \nMSE_HTB = sum((yf - ftrue_test).^2)/n_test\n","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"For LightGBM, we specify a parameter search over numleaves and maxdepth.","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"\n# lightGBM. Specify \ny       = Float64.(y)                 \nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n    \nsplits = (collect(1:n_train),collect(1:min(n_train,100)))  # goes around the problem that at least two training sets are required by search_cv (we want the first)\n\nparams = [Dict(:num_leaves => num_leaves,\n               :max_depth => max_depth) for\n          num_leaves in (4,16,32,64,127,256),\n          max_depth in (2,3,5,6,8)]\n\nlightcv = LightGBM.search_cv(estimator,x,y,splits,params,verbosity=-1)\n\nloss_cv = [lightcv[i][2][\"validation\"][estimator.metric[1]][1] for i in eachindex(lightcv)]\nminind = argmin(loss_cv)\n\nestimator.num_leaves = lightcv[minind][1][:num_leaves]\nestimator.max_depth  = lightcv[minind][1][:max_depth]\n\n# fit at best parameters\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\n\nyf_gbm = LightGBM.predict(estimator,x_test)[:,1]\nyf_gbm = log.(yf_gbm./(1.0 .- yf_gbm))   #  prediction for the natural parameter\n\nMSE_Light    = sum((yf_gbm - ftrue_test).^2)/n_test\n","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Print the results for a comparison.","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"println(\"\\n n = $n, p = $p\")\nprintln(\"RMSE from true natural parameter, HTBoost, modality = $modality \", sqrt(MSE_HTB) )\nprintln(\"RMSE from true natural parameter, lightGBM                      \", sqrt(MSE_Light) )\n","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Comparing accuracy  ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"modality=:fast uses depth=5, while :compromise selects depth=1 (dgp(x) is additive), resulting in better performance. \nHTBboost outperforms LightGBM in terms of accuracy (but with much longer training time).\nThe extent of this outperformance will vary depending on the smoothness of dgp(x)","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"\nn = 10000, p = 10\nRMSE from true natural parameter, HTBoost, modality = fast        0.5346\nRMSE from true natural parameter, HTBoost, modality = compromise  0.4665\nRMSE from true natural parameter, lightGBM                        0.6375\n","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Feature importance and average smoothing parameter for each feature.  ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"tau is the smoothness parameter; lower values give smoother functions, while tau=Inf is a sharp split (tau is trancated at 40 for this function).   avgtau is a summary of the smoothness of f(x), with features weighted by their importance. avgtau_a is a vector array with the importance weighted tau for each feature.  ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"avgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data,verbose=true);\n\nplot(x_plot,g_plot,title=\"smoothness of splits\",xlabel=\"standardized x\",label=:none)","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"The plot gives an idea of the average (importance weighted) smoothness across all splits. In this case, the average across features is 3.6, which is substantial smoothness. ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"<img src=\"../assets/avgtau_logistic.png\" width=\"400\" height=\"250\">","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"The function HTBweightedtau( ) with verbose=true produces the following table, from which we can see that f(x) is quite smooth with respect to all features. This explains why HTBoost outperforms LightGBM so strongly.  ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"\n Row │ feature  importance  avgtau   sorted_feature  sorted_importance  sorted_avgtau \n     │ String   Float32     Float64  String          Float32            Float64       \n─────┼────────────────────────────────────────────────────────────────────────────────\n   1 │ x1        37.1133    1.07181  x1                      37.1133          1.07181\n   2 │ x2        18.5243    4.6509   x4                      20.5592          7.44056\n   3 │ x3        16.9142    2.72758  x2                      18.5243          4.6509\n   4 │ x4        20.5592    7.44056  x3                      16.9142          2.72758\n   5 │ x5         1.21922   1.5      x9                       2.38085         1.5\n   6 │ x6         1.37907   1.5      x6                       1.37907         1.5\n   7 │ x7         0.0       0.0      x5                       1.21922         1.5\n   8 │ x8         0.879137  1.5      x10                      1.03073         1.5\n   9 │ x9         2.38085   1.5      x8                       0.879137        1.5\n  10 │ x10        1.03073   1.5      x7                       0.0             0.0\n\n Average smoothing parameter τ is 2.5.\n\n In sufficiently large samples, and if modality=:compromise or :accurate\n\n - Values above 20-25 suggest little smoothness in important features. HTBoost's performance may slightly outperform or slightly underperform other gradient boosting machines.\n - At 10-15 or lower, HTBoost should outperform other gradient boosting machines, or at least be worth including in an ensemble.\n - At 5-7 or lower, HTBoost should strongly outperform other gradient boosting machines.","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"Larger sample size","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"As the sample size gets larger, HTBoost can match the accuracy of LightGBM with only a fraction of the data. For example, HTBoost is approximately as accurate with n = 100k as LightGBM with n = 500k.   (The key to this result lies in the smoothness of f(x) with respect to at least a subset of important features, not in f(x) being additive).     ","category":"page"},{"location":"tutorials/Logistic/","page":"-","title":"-","text":"\nn = 100000, p = 10\nRMSE from true natural parameter, HTBoost, modality = compromise  0.2201\nRMSE from true natural parameter, lightGBM                        0.3346\n\nn = 500000, p = 10\nRMSE from true natural parameter, lightGBM                        0.2115\n","category":"page"},{"location":"tutorials/Beyond_accurate/#Beyond-accurate","page":"Beyond accurate","title":"Beyond accurate","text":"","category":"section"},{"location":"tutorials/Beyond_accurate/","page":"Beyond accurate","title":"Beyond accurate","text":"Modality = :accurate should cover the needs of most users. ","category":"page"},{"location":"tutorials/Beyond_accurate/","page":"Beyond accurate","title":"Beyond accurate","text":"In situations where computing time is not a factor and even the smallest increment in performance matter, the following may be tried:","category":"page"},{"location":"tutorials/Beyond_accurate/","page":"Beyond accurate","title":"Beyond accurate","text":"Lower the learning rate parameter lambda to 0.05, particularly if the function is highly nonlinear (some features have high average τ values). In some cases this may require increasing the maximum number of trees, e.g HTBparam(lambda=0.05,ntrees=4000).\nHTBboost cross-validates depth up to 6 (7) in modality = :compromise (:accurate). This is sufficient in most circustances. If the best depth is 6, try 7 and then perhaps 8. (Note that computing time can easily double with each increment in depth.) This can be achieved by running, for example: ","category":"page"},{"location":"tutorials/Beyond_accurate/","page":"Beyond accurate","title":"Beyond accurate","text":"  output = HTBfit(data,param,cv_grid=[5,6,7,8])","category":"page"},{"location":"tutorials/Beyond_accurate/","page":"Beyond accurate","title":"Beyond accurate","text":"Consider alternative loss functions, such as :L2loglink or (particularly for small n) :t.    ","category":"page"},{"location":"tutorials/Beyond_accurate/","page":"Beyond accurate","title":"Beyond accurate","text":"In datasets where the accuracy of HTBoost is roughly comparable to that of XGBoost and LightGBM, a 50-50 combination (or more sophisticated stacking) of HTBoost with one of the other two is likely  to yield the best results. (If HTBoost is mostly selecting sharp splits –-  as illustrated in Basic use – it will however be much faster to combine XGBoost with CatBoost rather than with HTBoost).  ","category":"page"},{"location":"Outperforming_other_GBM/#When-is-HTBoost-likely-to-outperform-(underperform)-other-GBMs-like-XGB-and-LightGBM?","page":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","title":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","text":"","category":"section"},{"location":"Outperforming_other_GBM/#When-is-HTBoost-more-likely-to-outperform?","page":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","title":"When is HTBoost more likely to outperform?","text":"","category":"section"},{"location":"Outperforming_other_GBM/","page":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","title":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","text":"When one or more of the following conditions are met:","category":"page"},{"location":"Outperforming_other_GBM/","page":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","title":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","text":"The underlying function is smooth with respect to at least a subset of the features. This can be assessed and visualized using HTBweightedtau() (see Basic use)\nSmall, highly unbalanced, or noisy datasets.","category":"page"},{"location":"Outperforming_other_GBM/#What-to-do-when-HTBoost-performs-approximately-as-well-as-other-GBM.","page":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","title":"What to do when HTBoost performs approximately as well as other GBM.","text":"","category":"section"},{"location":"Outperforming_other_GBM/","page":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","title":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","text":"If time is of the essence and/or accuracy and data efficiency are not priorities, drop HTBoost.\nIf maximizing accuracy is important, then combinations (stacking) of HTBoost with XGB and/or LightGMB typically improve on XGB/LightGBM (the different tree construction results in less than perfect correlation in predictions). However, if the function shows little smoothness (which can be assessed and visualized using HTBweightedtau(), see Basic use), CatBoost is a more computationally efficient option unless other positive features of HTBoost are relevant (see index) ","category":"page"},{"location":"Outperforming_other_GBM/#When-is-HTBoost-outperformed-by-other-GBM?","page":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","title":"When is HTBoost outperformed by other GBM?","text":"","category":"section"},{"location":"Outperforming_other_GBM/","page":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","title":"When is HTBoost likely to outperform (underperform) other GBMs like XGB and LightGBM?","text":"Different packages handle categorical features differently in their default mode, which can lead to different performance if categorical features are prominent. Other than that, if HTBoost is fitted in modality=:accurate or :compromise, it may slightly underperform in situations where symmetric trees are inferior to non-symmetric trees or when the underlying function is so irregular that the preliminary optimization (based on a rough grid) finds a local mode and does not split on the best feature. It will also slightly underperform is very deep trees (depth >>7) give the best fit. These conditions are probably more likely with near-perfect fit. The first condition can be evaluated by running CatBoost with grow_policy = SymmetricTree and grow_policy = Depthwise, and comparing the results. The second problem would be solved by setting param.mugridpoints >> 10, but computing times would increase proportionally. Finally, HTBoost becomes too slow to run with depth larger than 6 or 7; if the setting requires deeper trees, performance may suffer.  ","category":"page"},{"location":"examples/Zero_inflated_y/#Strategies-for-zero-inflated-y","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"","category":"section"},{"location":"examples/Zero_inflated_y/","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"Options available in HTBoost for zero-inflated data.","category":"page"},{"location":"examples/Zero_inflated_y/","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"HTBoost has three loss functions for zero inflated data:     :hurdleGamma, :hurdleL2, :hurdleL2loglink The :hurdleGamma is closest to the Tweedie distribution in LightGBM, XGB, and CatBoost. Hurdle models in HTBoost build two separate models, one with logistic loss to predict the occurence of a zero, and a second model with loss gamma or L2 or L2loglink to predict y|y≠0. Compared to a Tweedie regression, hurdle models have richer parametrization but far weaker constraints on the process, implying higher variance and smaller bias. My reading of the literature is that hurdle model typically outperform Tweedy in terms of forecasting. ","category":"page"},{"location":"examples/Zero_inflated_y/","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"While :hurdleGamma and :hurdleL2loglink require y≥0, a :hurdleL2 loss can be used if some y are negative. A hurdleL2 loss could therefore also be used if an otherwise continuous y has positive mass at some value v other than zero, by working with y-v. ","category":"page"},{"location":"examples/Zero_inflated_y/","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"A hurdleL2loglink loss can be a strong alternative to a hurdleGamma loss, if the gamma assumption is incorrect.","category":"page"},{"location":"examples/Zero_inflated_y/","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"A hurdleL2loglink is also an option for zero-inflated count data, as an alternative to a Poisson or GammaPoisson. ","category":"page"},{"location":"examples/Zero_inflated_y/","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"What this script does. ","category":"page"},{"location":"examples/Zero_inflated_y/","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"Generates data from a gamma distribution, which is then transformed to produce excess zeros. \nFit HTBoost, with loss = :hurdleGamma or :hurdleL2loglink\nA comparison with LightGBM using the Tweedie loss is promising.\nHTBpredict takes the form:   yf,prob0,yfnot0     = HTBpredict(xtest,output) ","category":"page"},{"location":"examples/Zero_inflated_y/","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"where yf = E(y|x) = (1-prob0)*yf_not0","category":"page"},{"location":"examples/Zero_inflated_y/","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Plots,Distributions \nusing LightGBM\n\n# USER'S OPTIONS \n\nRandom.seed!(1)\n\n# Some options for HTBoost\nloss      = :hurdleGamma    # options for y>=0 data are :L2loglink, :L2, :gamma, :hurdleGamma, :hurdleL2loglink, :hurdleL2     \nmodality  = :compromise     # :accurate, :compromise (default), :fast, :fastest \npriortype = :hybrid       # :hybrid (default) or :smooth to force smoothness \nnfold     = 1             # number of cv folds. 1 faster (single validation sets), default 5 is slower, but more accurate.\nnofullsample = true       # if nfold=1 and nofullsample=true, the model is not re-fitted on the full sample after validation of the number of trees\n\nrandomizecv = false       # false (default) to use block-cv. \nverbose     = :Off\nwarnings    = :On\n\n# options to generate data. \ntrue_k      = 10     # dispersion parameter of gamma distribution\nα           = 0.5    # Generates y=0 data. Set to 0 for all y strictly positive, larger numbers for more mass at 0 \n\nn,p,n_test  = 10_000,4,100_000\n\nf_1(x,b)    = b./(1.0 .+ (exp.(1.0*(x .- 1.0) ))) .- 0.1*b \nf_2(x,b)    = b./(1.0 .+ (exp.(4.0*(x .- 0.5) ))) .- 0.1*b \nf_3(x,b)    = b./(1.0 .+ (exp.(8.0*(x .+ 0.0) ))) .- 0.1*b\nf_4(x,b)    = b./(1.0 .+ (exp.(16.0*(x .+ 0.5) ))) .- 0.1*b\n\nb1,b2,b3,b4 = 0.2,0.2,0.2,0.2\n\n# generate data\nx,x_test = randn(n,p), randn(n_test,p)\n\nc        = -2  \nf        = c .+ f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4)\nf_test   = c .+ f_1(x_test[:,1],b1) + f_2(x_test[:,2],b2) + f_3(x_test[:,3],b3) + f_4(x_test[:,4],b4)\n\nμ        = exp.(f)        # conditional mean \nμ_test   = exp.(f_test)   # conditional mean \n\n# k can depend on features for a ≠ 1\na        = -0   \nlogk     = log(true_k)\nlogk     = logk .+ a*(b1*x[:,1] + b2*x[:,2] + b4*x[:,4])\nk        = exp.(logk)\nlogk_test = log(true_k) .+ a*(b1*x_test[:,1] + b2*x_test[:,2] + b4*x_test[:,4] )\nk_test    = exp.(logk_test) \n# end k dependent on features\nscale    = μ./k\nscale_test = μ_test./k_test\ny       = zeros(n)\ny_test  = zeros(n_test)\n\nfor i in eachindex(y)\n    y[i]  = rand(Gamma.(k[i],scale[i]))\n    μ[i] < α*rand(1)[1] ? y[i] = 0.0 : nothing    # zero-inflated data\nend \n\nfor i in eachindex(y_test)\n    y_test[i]  = rand(Gamma.(k_test[i],scale_test[i]))\n    μ_test[i] < α*rand(1)[1] ? y_test[i] = 0.0 : nothing    \nend \n\nprintln(\"\\n share of y=0 is $(mean(y.==0)) \\n\")\nhistogram(y,title=\"y, unconditional distribution\") \n\n# set up HTBparam and HTBdata, then fit and predit\n\nparam  = HTBparam(loss=loss,priortype=priortype,randomizecv=randomizecv,nfold=nfold,\n                   verbose=verbose,warnings=warnings,modality=modality,nofullsample=nofullsample)\ndata   = HTBdata(y,x,param)\noutput = HTBfit(data,param)\n\nprintln(\" \\n loss = $loss, modality = $(param.modality), nfold = $nfold \")\n\nif loss in [:hurdleL2,:hurdleL2loglink,:hurdleGamma]\n    yf,prob0,yf_not0     = HTBpredict(x_test,output)\n    println(\" depth logistic = $(output[1].bestvalue), number of trees logistic = $(output[1].ntrees) \")\n    println(\" depth = $(output[2].bestvalue), number of trees = $(output[2].ntrees) \")\nelse     \n    yf     = HTBpredict(x_test,output,predict=:Ey)\n    println(\" depth = $(output.bestvalue), number of trees = $(output.ntrees) \")\nend \n\nprintln(\" out-of-sample RMSE (y-yf), HTBoost       \", sqrt(sum((yf - y_test).^2)/n_test) )\n\n# ligthGBM \nestimator = LGBMRegression(\n    objective = \"tweedie\",\n    metric = [\"tweedie\"],\n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    num_threads = number_workers\n)\n\nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n    \n# parameter search over num_leaves and max_depth\nsplits = (collect(1:n_train),collect(1:min(n_train,100)))  # goes around the problem that at least two training sets are required by search_cv (we want the first)\n\nparams = [Dict(:num_leaves => num_leaves,\n               :max_depth => max_depth) for\n          num_leaves in (4,16,32,64,127,256),\n          max_depth in (2,3,5,6,8)]\n\nlightcv = LightGBM.search_cv(estimator,x,y,splits,params,verbosity=-1)\n\nloss_cv = [lightcv[i][2][\"validation\"][estimator.metric[1]][1] for i in eachindex(lightcv)]\nminind = argmin(loss_cv)\n\nestimator.num_leaves = lightcv[minind][1][:num_leaves]\nestimator.max_depth  = lightcv[minind][1][:max_depth]\n\n# re-fit at cv parameters\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\n    \nyf_gbm = LightGBM.predict(estimator,x_test)\nyf_gbm = yf_gbm[:,1]    # drop the second dimension or a (n_test,1) matrix \n\nprintln(\"\\n out-of-sample RMSE (y-yf), LightGBM cv      \", sqrt(sum((yf_gbm - y_test).^2)/n_test) )\n","category":"page"},{"location":"examples/Zero_inflated_y/","page":"Strategies for zero inflated y","title":"Strategies for zero inflated y","text":"","category":"page"},{"location":"tutorials/L2loglink_and_rank/#L2loglink-as-a-general-loss-if-*y*0,-with-applications-to-ranking","page":"-","title":"L2loglink as a general loss if y≥0, with applications to ranking","text":"","category":"section"},{"location":"tutorials/L2loglink_and_rank/#L2loglink-as-a-general-loss-if-*y*0","page":"-","title":"L2loglink as a general loss if y≥0","text":"","category":"section"},{"location":"tutorials/L2loglink_and_rank/","page":"-","title":"-","text":"param = HTBoost(loss=:L2loglink)","category":"page"},{"location":"tutorials/L2loglink_and_rank/","page":"-","title":"-","text":"The L2loglink loss is designed as a robust and general alternative to :gamma and other distributions defined on y>0. When y is continuous, y = 0 is allowed in the :L2loglink function, but not in :gamma. The L2loglink can also work well for count data, rank data, and any situations where y≥0.","category":"page"},{"location":"tutorials/L2loglink_and_rank/","page":"-","title":"-","text":"The :L2loglink is a L2 loss, but with a log-link function, meaning that the tree ensemble approximates log(E(y|x)), while loss = [y - exp(γ)]², and therefore γ = log(E(y|x)). Compared to specialized distributions like the gamma, it sacrifices some efficiency if the assumed distribution is indeed the true distribution, but can be more efficient if it is not.","category":"page"},{"location":"tutorials/L2loglink_and_rank/","page":"-","title":"-","text":"I don't believe that this option is available in other GBM packages. XGBoost offers reg:squaredlogerror, which is  a L2 loss on log(y+1). A similar option is also available in HTBoost, as loss = :lognormal, which is equivalent to fitting a L2 (Gaussian) loss to log(y). The :L2loglink is different, as it fits a L2 loss to y, but with a log-link function. The reason to specify these two separate options is that a lognormal is consistent for E(log(y)|x), but not necessarily for E(y|x); the possible adjustment E(y|x) = exp(E(log(y)|x) + 0.5*Var(log(y)|x)) requires fitting a separate model for Var(log(y)|x), or hoping that the variance is constant. On the other hand, :L2loglink is consistent for E(y|x).","category":"page"},{"location":"tutorials/L2loglink_and_rank/","page":"-","title":"-","text":"See Gamma for a case where :L2loglink approaches the performance of :gamma even when the data is in fact generated from a gamma distribution.","category":"page"},{"location":"tutorials/L2loglink_and_rank/#L2loglink-loss-as-an-interesting-option-for-ranking-problems.","page":"-","title":"L2loglink loss as an interesting option for ranking problems.","text":"","category":"section"},{"location":"tutorials/L2loglink_and_rank/","page":"-","title":"-","text":"HTBoost does not have specialized ranking losses. ","category":"page"},{"location":"tutorials/L2loglink_and_rank/","page":"-","title":"-","text":"According to the benchmarks posted in CatBoost ranking benchmarks the :L2 loss can be surprisingly competitive in ranking tasks. The :L2loglink loss available in HTBoost may be a better choice than :L2 for ranking tasks, since it enforces E(y|x)>0. ","category":"page"},{"location":"tutorials/L2loglink_and_rank/","page":"-","title":"-","text":"The suggested practice is to make sure that ranking is expressed numerically, with y ∈ {1,2,...}, (NOTE: y=0 is allowed but best avoided in this case) and loss=:L2loglink. ","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/#Installation-and-Introduction-to-the-Python-bindings-of-HTBoost.jl","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"","category":"section"},{"location":"tutorials_py/Installation_and_use_in_Python/#Install-Julia","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Install Julia","text":"","category":"section"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"Install Julia, if required. This may not be necessary, as juliacall should automatically download a suitable version of Julia if required.  Julia can be downloaded from (https://julialang.org/downloads/)","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/#Each-script-should-start-with-the-following-3-steps","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Each script should start with the following 3 steps","text":"","category":"section"},{"location":"tutorials_py/Installation_and_use_in_Python/#1.-Install-juliacall-package-(*pip-install-juliacall*),-then-load-it-as","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"1. Install juliacall package (pip install juliacall), then load it as","text":"","category":"section"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"from juliacall import Main as jl, convert as jlconvert","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/#2.-Install-the-Julia-packages-Distributed,-DataFrames,-HTBoost,-and-load-them.","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"2. Install the Julia packages Distributed, DataFrames, HTBoost, and load them.","text":"","category":"section"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"Installation is needed only once, and can be done from Julia or in Python as follows:","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"jl.seval(\"using Pkg\")\njl.seval(\"Pkg.add('Distributed')\")\njl.seval(\"Pkg.add('HybridTreeBoosting')\")","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"To install HTBoost from the registry, use the following command.","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"jl.seval('using Pkg; Pkg.add(\"HybridTreeBoosting\")')","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"Alternatively, this will work even if the package is not in the registry.","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"jl.seval('using Pkg; Pkg.add(\"https://github.com/PaoloGiordani/HybridTreeBoosting.jl\")')","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"Load the packages in Julia","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"jl.seval(\"using DataFrames\")  \njl.seval(\"using HybridTreeBoosting\")   # HTBoost must be installed in Julia ","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/#3.-Set-the-desired-number-of-workers-(cores)-to-be-used-in-parallel.","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"3. Set the desired number of workers (cores) to be used in parallel.","text":"","category":"section"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"Note: Python incurs this compile time cost every time the program is run (unlike Julia and R). The compile cost increases in the number of workes, and is around 80'' for 8 workers.","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"jl.seval(\"using Distributed\")\njl.seval(\"number_workers = 8; nprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\")\njl.seval(\"@everywhere using HybridTreeBoosting\")","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/#Running-HTBoost","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Running HTBoost","text":"","category":"section"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"The most important functions in HTBoost are","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"param  = jl.HTBparam()           # defines the model     \ndata   = jl.HTBparam(y,x,param)  # defines the data, including optional features such as names, weights, time\noutput = jl.HTBfit(data,param)   # fits the model \nyf     = jl.HTBpredict(x_predict,output)\nt      = jl.HTBweightedtau(output,data)  # variable importance and smoothness\nt      = jl.HTBpartialplot(data,output,[1,2])  # partial dependence plots, here for the first two features  ","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/#Further-notes-on-using-HybridTreeBoosting-in-Python","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Further notes on using HybridTreeBoosting in Python","text":"","category":"section"},{"location":"tutorials_py/Installation_and_use_in_Python/#To-translate-the-Julia-tutorials-in-Python","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"To translate the Julia tutorials in Python","text":"","category":"section"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"Change Julia symbols to Python strings. e.g. :modality to 'modality' or \"modality\", :On to 'On' on \"On\"\nChange Julia true/false to True/False \nThe output of each function in Julia is a named tuple. Its elements can be accessed in the usual way, e.g. ","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"  ntrees = output.ntrees  ","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/#Getting-your-data-from-Python-to-HTBoost","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Getting your data from Python to HTBoost","text":"","category":"section"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"If y and x are numerical matrices with no missing data, they can be taken into HTBdata() directly, e.g. ","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"x  = np.random.normal(0,1,n)\nu  = np.random.normal(0,1,n)\ny  = x + np.random.normal(0,1,n)\n\nparam   = jl.HTBparam(modality='accurate')  \ndata   = jl.HTBdata(y,x,param)\noutput = jl.HTBfit(data,param)","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"This will not work properly if there are missing data (NaN in Python, NaN or missing in Julia). See Missing data) for how HTBoost deals with missing data internally, delivering superior accuracy if the underlying function is at least partially smooth.  ","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"When there are missing data, or when y and/or x contain strings (categorical features), we must work translate our Python dataframe into a Julia DataFrame, which is then fed to HTBdata(), e.g. (continuing from the previous example)","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"fnames = ['x1']\ndf = pd.DataFrame(x, columns=fnames)  # convert to pandas dataframe\njl.seval(\"using DataFrames\")  \nx = jl.DataFrame(df)\n\ndata     = jl.HTBdata(y,x,param,fnames=colnames(df))    # pass the column names \noutput   = jl.HTBfit(data,param)                        \n","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"Columns of string values are automatically interpreted by HTBoost as a categorical. If some categorical features are represented by numerical values, it is necessary to list them in param (in which case all categorical features, even strings, must be listed). This can be done with a vector of their column positions.","category":"page"},{"location":"tutorials_py/Installation_and_use_in_Python/","page":"Installation and Introduction to the Python bindings of HTBoost.jl","title":"Installation and Introduction to the Python bindings of HTBoost.jl","text":"param = jl.HTBparam(cat_features=[3])\ndata  = jl.HTBdata(y,x,param)    # passing the column names is optional\n","category":"page"},{"location":"tutorials/Missing/#Missing-data","page":"-","title":"Missing data","text":"","category":"section"},{"location":"tutorials/Missing/","page":"-","title":"-","text":"HTBoost should outperform other GBMs in the automatic treatment of missing values in x.  ","category":"page"},{"location":"tutorials/Missing/","page":"-","title":"-","text":"HTBoost handles missing values automatically. No user intervention required. ","category":"page"},{"location":"tutorials/Missing/","page":"-","title":"-","text":"data.x may contain NaN or missing. Missing values of y are discarded.\nForecasting with missing values is also supported.\nHTBoost handles missing values (NaN or missing) internally, like other GBMs. The approach is Block Propagation (see Josse et al. 2020, \"On the consistency of supervised learning with missing values\"), as LightGBM, but with a key difference ... \nTaking advantage of soft splits (where splits are indeed soft), missing can be optimally allocated to either branch in fractional proportions, where the fraction is optimized at each split. The result is more efficient inference with missing values in finite samples.\nUnlike imputation, the internal assignments in all these GMBs recovers f(x) asymptotically whether data are missing at random, or missing not at random as a function of x only, or missing not at random as a function of E(y). (Josse et al. 2020)\nWhen feasible, a high-quality imputation of missing values + mask may perform better, particularly in small samples, high predictability of missing values from non-missing values, linear or quasi-linear f(x), and missing at random (in line with the results of Josse et al.)  ","category":"page"},{"location":"tutorials/Missing/","page":"-","title":"-","text":"There is an option HTBparam(delete_missing=true) to delete all rows with missing values, but, in light of the discussion above, this is not recommended. ","category":"page"},{"location":"tutorials/Missing/","page":"-","title":"-","text":"See Missing for a more detailed discussion and code to reproduce the simulations in Josse et al. 2020, including a comparison with LightGBM. HTBoost strongly outperforms LightGBM in the settings of Josse et al. 2020.","category":"page"},{"location":"examples/t/#Student-t-loss-with-estimated-degrees-of-freedom","page":"Student t loss with estimated degrees of freedom","title":"Student t loss with estimated degrees of freedom","text":"","category":"section"},{"location":"examples/t/","page":"Student t loss with estimated degrees of freedom","title":"Student t loss with estimated degrees of freedom","text":"Purpose and main results:","category":"page"},{"location":"examples/t/","page":"Student t loss with estimated degrees of freedom","title":"Student t loss with estimated degrees of freedom","text":"Generates synthetic data with t-distributed errors.\nShow how to use the option loss = :t for leptokurtic (fat tailed) data, which is recommended over  the Huber loss since the degrees of freedom of the t distribution are estimated internally, thus providing the \"right\" amount of robustness (in the sense of maximizing the likelihood.)\nIF the errors are IID, the :t loss will generally outperform the :L2 loss. However, errors are often not IID, and then the :L2 loss can outperform the :t or :Huber loss even with leptokurtic errors. ","category":"page"},{"location":"examples/t/","page":"Student t loss with estimated degrees of freedom","title":"Student t loss with estimated degrees of freedom","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Statistics,Plots,Distributions\nusing LightGBM\n\n# USER'S OPTIONS \n\nRandom.seed!(123)\n\n# Options for data generation \nn         = 5_000\np         = 5      # p>=5. Only the first 4 variables are used in the function f(x) below \nstde      = 1     # e.g. 1 for high SNR, 5 for lowish, 10 for low (R2 around 4%). Not really the stde unless dof is high. \ndof       = 3     # degrees of freedom for the t distribution to generate data. e.g. 3 or 5 or 10.\n\n# Options for HTBoost: modality is the key parameter guiding hyperparameter tuning and learning rate.\n# :fast and :fastest only fit one model at default parameters, while :compromise and :accurate perform\n# automatic hyperparameter tuning. \n\nloss      = :t      # :t  \nmodality  = :fastest   # :accurate, :compromise (default), :fast, :fastest\n\n# define the function dgp(x), here the Friedman's function for x~U  \ndgp(x) = 10.0*sin.(π*x[:,1].*x[:,2]) + 20.0*(x[:,3].-0.5).^2 + 10.0*x[:,4] + 5.0*x[:,5]\n\n# End user's options \n\n# generate data. x is standard uniform, and errors are a mixture of two normals, with right skew\nx,x_test   = rand(n,p),rand(n_test,p)\nftrue      = dgp(x)\nftrue_test = dgp(x_test)\n\nt_object = TDist(dof)\n\nu = rand(t_object,n)\n\ny      = ftrue + u*stde\n\nhistogram(u,title=\"errors\",label=\"\")\n\n# HTBoost parameters\nparam  = HTBparam(loss=loss,nfold=1,nofullsample=true,modality=modality,verbose=:Off)\ndata   = HTBdata(y,x,param)\n\n# ligthGBM parameters \nestimator = LGBMRegression(\n    objective = \"regression\",\n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    num_threads = number_workers\n)\n\nestimator_huber = LGBMRegression(\n    objective = \"huber\",\n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    num_threads = number_workers\n)\n\n\n# Fit lightGBM \n\nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n    \nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\n    \nyf_gbm = LightGBM.predict(estimator,x_test)\nyf_gbm2 = yf_gbm[:,1]    # drop the second dimension or a (n_test,1) matrix \nMSE2    = sum((yf_gbm2 - ftrue_test).^2)/n_test\n\nLightGBM.fit!(estimator_huber,x_train,y_train,(x_val,y_val),verbosity=-1)\nyf_gbm = LightGBM.predict(estimator_huber,x_test)\nyf_gbm = yf_gbm[:,1]    # drop the second dimension or a (n_test,1) matrix \nMSE3    = sum((yf_gbm - ftrue_test).^2)/n_test\n\nprintln(\"\\n oos RMSE from true f(x), lightGBM, Huber loss                    \", sqrt(MSE3) )\nprintln(\" oos RMSE from true f(x), lightGBM, L2 loss                       \", sqrt(MSE2) )\n\n# Fit HTBoost, :t (or :Huber) \noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)  \nMSE1   = sum((yf - ftrue_test).^2)/n_test\nθ      = HTBcoeff(output,verbose=false)         # info on estimated coeff\n\nprintln(\" oos RMSE from true f(x) parameter, HTBoost, loss = $loss          \", sqrt(MSE1) )\n\n# Fit HTBoost, :L2 \nparam.loss = :L2 \noutput_L2 = HTBfit(data,param)\nyf     = HTBpredict(x_test,output_L2)  \nMSE0    = sum((yf - ftrue_test).^2)/n_test\n\nprintln(\" oos RMSE from true f(x) parameter, HTBoost, loss = L2         \", sqrt(MSE0) )\n\nprintln(\"\\n true dof = $dof and estimated dof = $(θ.dof) \")\nprintln(\"\\n For more information about coefficients, use HTBcoeff(output) \")\nHTBcoeff(output)\n","category":"page"},{"location":"examples/Global_Equity_Panel/#Panel-data-(longitudinal-data)-example-with-global-equity","page":"Panel data (longitudinal data) example with global equity","title":"Panel data (longitudinal data) example with global equity","text":"","category":"section"},{"location":"examples/Global_Equity_Panel/","page":"Panel data (longitudinal data) example with global equity","title":"Panel data (longitudinal data) example with global equity","text":"Working with time series and longitudinal data (panels).","category":"page"},{"location":"examples/Global_Equity_Panel/","page":"Panel data (longitudinal data) example with global equity","title":"Panel data (longitudinal data) example with global equity","text":"The user only needs to provide features and a vector of dates in HTBdata() and, if there is overlapping, the overlap parameter in HTBparam(). Example:  param  = HTBparam(overlap=20)         data   = HTBdata(y,x,param,dates,fnames = fnames) where y,x and dates can be dataframes, e.g. y = df[:,:excessret], x = df[:,features_vector], dates = df[:,:date]\nOverlap defaults to 0. Typically overlap = h-1, where y(t) = Y(t+h)-Y(t). Used for purged-CV and to calibrate loglikdivide.\nBy default, HTBoost uses block-cv, which is suitable for time series and longitudinal data.  To use expanding window cross-validation instead, provide indtraina and indtesta in HTBparam(): the function HTBindexesfromdates() assists in building these indexes. Example:  firstdate = Date(\"2017-12-31\", Dates.DateFormat(\"y-m-d\")) indtraina,indtesta = HTBindexesfromdates(df,:date,firstdate,12)  # 12 periods in each block, starting from first_datae\nThe code below provides an application to forecasting international stock market indexes. ","category":"page"},{"location":"examples/Global_Equity_Panel/","page":"Panel data (longitudinal data) example with global equity","title":"Panel data (longitudinal data) example with global equity","text":"See HTBindexesfromdates() for more details.   ","category":"page"},{"location":"examples/Global_Equity_Panel/","page":"Panel data (longitudinal data) example with global equity","title":"Panel data (longitudinal data) example with global equity","text":"\n# On multiple cores\nnumber_workers  = 8  # desired number of workers\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random, Plots, CSV, DataFrames, Statistics\n\nRandom.seed!(1)\n\n# USER'S INPUTS \n\n# data\nlog_ret        = false    # true to predict log returns, false (default) to predict returns\noverlap        = 0        # 0 for non-overlapping (default), h-1 for overlapping\n\n# HTBoost\n\nloss       = :L2loglink  # if log_ret=false, consider :L2loglink instead of :L2 \nmodality   = :accurate    # :accurate, :compromise, :fast, :fastest\npriortype  = :hybrid     #:hybrid (accurate), :smooth (forces smooth split)\n\ncv_type     = \"block\"  # \"block\" or \"expanding\" or \"randomized\" (not recommended for time series and panels)\nnfold       = 4        # number of folds for cv (default 4). Irrelevant if cv_type = \"expanding\".\n\n# for cv_type = \"expanding\" \ncv_first_date     = 197001   # start date for expanding window cv       \ncv_block_periods  = 120      # number of periods (months in this dataset): if cv_type=\"block\", this is the block size\n\n# END USER'S OPTIONS\n\ndf = CSV.read(\"examples/data/GlobalEquityReturns.csv\", DataFrame, copycols = true) # import data as dataframe. Monthly LOG excess returns.\ndisplay(describe(df))\n\n# prepare data \nlog_ret ? y = 100*df[:,:excessret] : y  = @. 100*(exp(df[:,:excessret]) )\n\nfeatures_vector = [:logCAPE, :momentum, :vol3m, :vol12m ]\nfnames = [\"logCAPE\", \"momentum\", \"vol3m\", \"vol12m\" ]\n\nx      = df[:,features_vector]\n\n# set up HTBparam and HTBdata, then fit, depending on cross-validation type\n\nif cv_type == \"randomized\"\n  param  = HTBparam(nfold=nfold,overlap=overlap,loss=loss,modality=modality,priortype=priortype,randomizecv=true) \nelseif cv_type == \"block\"   # default \n  param  = HTBparam(nfold=nfold,overlap=overlap,loss=loss,modality=modality,priortype=priortype) \nelseif cv_type == \"expanding\"\n  indtrain_a,indtest_a = HTBindexes_from_dates(df,:date,cv_first_date,cv_block_periods)\n  param  = HTBparam(nfold=nfold,overlap=overlap,loss=loss,modality=modality,priortype=priortype,\n                     indtrain_a=indtrain_a,indtest_a=indtest_a)\nend \n\ndata   = HTBdata(y,x,param,df[:,:date],fnames = fnames)\n@time output = HTBfit(data,param)\n\nyhat   = HTBpredict(x,output)  # in-sample fitted value.\n\nprintln(\"\\n depth = $(output.bestvalue), number of trees = $(output.ntrees) \")\nprintln(\" in-sample R2 = \", round(1.0 - sum((y - yhat).^2)/sum((y .- mean(y)).^2),digits=3) )\n\n# feature importance\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data);\n\n# partial dependence plots, best four features. q1st is the first quantile. e.g. 0.01 or 0.05\nq,pdp  = HTBpartialplot(data,output,sortedindx[[1,2,3,4]],q1st=0.01,npoints = 5000)\n\n# partial dependence plots\npl = Vector(undef,4)\n\nfor i in 1:4 \n  pl[i]   = plot(q[:,i],pdp[:,i], legend=false,title=fnames[sortedindx[i]],color=:green)\nend \n\ndisplay(plot(pl[1],pl[2],pl[3],pl[4], layout=(2,2), size=(1200,600)))  # display() will show it in Plots window.\n#savefig(\"examples/GlobalEquityPanel.png\")  # save it as png file\n","category":"page"},{"location":"tutorials/tau_values/#Plotting-and-printing-τ-values","page":"-","title":"Plotting and printing τ values","text":"","category":"section"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"Summary","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"Simulates a dataset from four different functions.\nFor each dataset, plots the average (across features) value of τ (weighted by importance).\nFor a specific function, prints a detailed breakdown to screen   ","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"Main points","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"The average tau across all features provides a rough estimate of the overall smoothness of f(x), and therefore of the expected gains from HTBoost.\nThe function may be considerably more nonlinear than what plots of a single value of tau may imply! Average values of tau mostly provide a guide to the gains that can be obtained by HTBoost: to gauge the nonlinearity of the function, these numbers should be supplemented by partial effect plots. ","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"Import HTBoost and other required packages","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"number_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Plots \n\n# USER'S OPTIONS \n","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"Specify some user's options","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"\n# dgp: four increasingly irregular functions\ndgp_a = [\"Linear\",\"Friedman\",\"Threshold Friedman\",\"AND\"] \nwhich_function = 3                                     # for which function to print detailed break-down \n\n# Some options for HTBoost\nloss      = :L2         \nmodality  = :fast     # :accurate, :compromise (default), :fast, :fastest \n\npriortype = :hybrid       # :hybrid (default) or :smooth to force smoothness \nnfold     = 1             # number of cv folds. 1 faster (single validation sets), default 4 is slower, but more accurate.\nnofullsample = true       # if nfold=1 and nofullsample=true, the model is not re-fitted on the full sample after validation of the number of trees\n\nrandomizecv = false       # false (default) to use block-cv. \nverbose     = :Off\nwarnings    = :Off\n \n# options to generate data. y = sum of six additive nonlinear functions + Gaussian noise.\nn,n_test  = 10_000,100_000\nstde      = 1.0\nrndseed   = 1234\n","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"Four functions, increasingly irregular (from linearity to sharp splits)","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"function simulatedata(n,stde;rndseed=1,dgp=\"Friedman\")\n    \n    Random.seed!(rndseed)\n\n    if dgp==\"Linear\"\n        p = 5\n        x = randn(n,p)\n        f = x[:,1] + x[:,2] + x[:,3] + x[:,4] + x[:,5]\n        y = f + stde*randn(n)   \n    elseif dgp==\"Friedman\"\n        p = 5\n        x     = rand(n,p)  # for Friedman function x is uniform\n        f = 10.0*sin.(π*x[:,1].*x[:,2]) + 20.0*(x[:,3].-0.5).^2 + 10.0*x[:,4] + 5.0*x[:,5]\n        y = f + stde*randn(n)\n    elseif dgp==\"Threshold Friedman\"\n        p = 6\n        x = rand(n,p)  # for Friedman function x is uniform\n        f1 = 10.0*sin.(π*x[:,1].*x[:,2]) + 20.0*(x[:,3].-0.5).^2 + 10.0*x[:,4] + 5.0*x[:,5]\n        f2 = 0.5*f1\n        f  = f1.*(x[:,6].<0.5) + f2.*(x[:,6].>0.5)\n        y = f + stde*randn(n)\n    elseif dgp==\"AND\"\n        p = 3\n        x = randn(n,p)\n        f = 10*( (x[:,1].>0.3).*(x[:,2].>0.3).*(x[:,3].>0.3) )   # same threshold: symmetric trees outperform     \n        y = f + stde*randn(n) \n    end \n\n    return y,x,f \n\nend \n","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"For each function, generate a dataset, fit HTBoost, store output","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"\noutput_a = Vector(undef,length(dgp_a))\n\nfor (i,dgp) in enumerate(dgp_a)\n\n    y,x,f             = simulatedata(n,stde,rndseed=rndseed,dgp=dgp)\n    y_test,x_test,f_test = simulatedata(100_000,stde,rndseed=rndseed,dgp=dgp)\n\n    param  = HTBparam(loss=loss,priortype=priortype,randomizecv=randomizecv,nfold=nfold,verbose=verbose,warnings=warnings,\n                modality=modality,nofullsample=nofullsample)\n\n    data   = HTBdata(y,x,param)\n    output = HTBfit(data,param)\n    output_a[i] = output\n\nend\n","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"Plotting average values for each function","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"\npl   = Vector(undef,length(dgp_a))\n\nfor i in eachindex(dgp_a)\n\n    avgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output_a[i],data,verbose=false,best_model=false);\n\n    pl[i]   = plot( x_plot,g_plot,\n           title =  dgp_a[i],\n           legend = :bottomright,\n           linecolor = [:blue],\n           linestyle = [:solid],\n\n           linewidth = [5],\n           titlefont = font(15),\n           legendfont = font(12),\n           xlabel=\"standardized x\",\n           ylabel=\"avg tau\",\n           label=:none,\n           )           \nend\n\ndisplay(plot(pl[1],pl[2],pl[3],pl[4],layout=(2,2), size=(1200,800)))  \n","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"While the actual function will typically be more nonlinear than implied by these plots, we do get a useful estimate of whether HTBoost is using smooth or quasi-hard (or hard) splits, and therefore of the efficiency gains that can be expected. ","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"<img src=\"../assets/tau values.png\" width=\"600\" height=\"350\">","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"Printing a more detailed break-down for a specific function (here Threshold Friedman)","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"avgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output_a[which_function],data_a[which_function],verbose=true,best_model=false);","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"Notice how x6, which operates a hard split, has a high avg tau, x1,x2,x3, which enter nonlinearly, have average values of tau, and x4 and x5, which enter linearly, have small values.","category":"page"},{"location":"tutorials/tau_values/","page":"-","title":"-","text":"\nRow │ feature  importance  avgtau     sorted_feature  sorted_importance  sorted_avgtau \n     │ String   Float32     Float64    String          Float32            Float64       \n─────┼──────────────────────────────────────────────────────────────────────────────────\n─────┼──────────────────────────────────────────────────────────────────────────────────\n─────┼──────────────────────────────────────────────────────────────────────────────────\n   1 │ x1         15.4995    3.17203   x6                       30.5483       38.58\n   2 │ x2         15.655     3.20808   x4                       19.1541        0.669109\n   3 │ x3          9.6912    7.06228   x2                       15.655         3.20808\n   4 │ x4         19.1541    0.669109  x1                       15.4995        3.17203\n   5 │ x5          9.45188   0.994279  x3                        9.6912        7.06228\n   6 │ x6         30.5483   38.58      x5                        9.45188       0.994279\n\n Average smoothing parameter τ is 4.9.\n\n In sufficiently large samples, and if modality=:compromise or :accurate\n\n - Values above 20-25 suggest little smoothness in important features. HTBoost's performance may slightly outperform or slightly underperform other gradient boosting machines.\n - At 10-15 or lower, HTBoost should outperform other gradient boosting machines, or at least be worth including in an ensemble.\n - At 5-7 or lower, HTBoost should strongly outperform other gradient boosting machines.\n","category":"page"},{"location":"examples/Logistic/#Logistic-regression-for-binary-classification","page":"Logistic regression for binary classification","title":"Logistic regression for binary classification","text":"","category":"section"},{"location":"examples/Logistic/","page":"Logistic regression for binary classification","title":"Logistic regression for binary classification","text":"Short description:","category":"page"},{"location":"examples/Logistic/","page":"Logistic regression for binary classification","title":"Logistic regression for binary classification","text":"Comparison with lightGBM on a logistic regression problem with simulated data.\nparam.modality as the most important user's choice.\nIn default modality, HTBoost performs automatic hyperparameter tuning.","category":"page"},{"location":"examples/Logistic/","page":"Logistic regression for binary classification","title":"Logistic regression for binary classification","text":"Extensive description: ","category":"page"},{"location":"examples/Logistic/","page":"Logistic regression for binary classification","title":"Logistic regression for binary classification","text":"Sketch of a comparison of HTBoost and lightGBM on a logistic regression problem. The comparison with LightGBM is biased toward HTBoost if the function generating the data is  smooth in some features (this is easily changed by the user). lightGBM is cross-validated over maxdepth and numleaves, with the number of trees set to 1000 and found by early stopping.","category":"page"},{"location":"examples/Logistic/","page":"Logistic regression for binary classification","title":"Logistic regression for binary classification","text":"Options for HTBoost: modality is the key parameter guiding hyperparameter tuning and learning rate. :fast and :fastest only fit one model at default parameters, while :compromise and :accurate perform automatic hyperparameter tuning. In HTBoost, it is not recommended that the user performs  hyperparameter tuning by cross-validation, because this process is done automatically if modality is :compromise or :accurate. The recommended process is to first run in modality=:fast or :fastest, for exploratory analysis and to gauge computing time, and then switch to :compromise (default) or :accurate.","category":"page"},{"location":"examples/Logistic/","page":"Logistic regression for binary classification","title":"Logistic regression for binary classification","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Statistics\nusing LightGBM\n\n# USER'S OPTIONS \n\nRandom.seed!(1)\n\n# Options for data generation \nn         = 10_000\np         = 10      # mumber of features. p>=4. Only the first 4 variables are used in the function f(x) below \nnsimul    = 1       # number of simulated datasets. \n\n# Options for HTBoost: modality is the key parameter guiding hyperparameter tuning and learning rate.\n# :fast and :fastest only fit one model at default parameters, while :compromise and :accurate perform\n# automatic hyperparameter tuning. \n\nmodality  = :fast   # :accurate, :compromise (default), :fast, :fastest\n\n# define the function f(x), where x are indendent N~(0,1), and f(x) is for the natural parameter,\n# so f(x) = log(prob/(1-prob))\n\nf_1(x,b)    = b*x .+ 1 \nf_2(x,b)    = sin.(b*x)  \nf_3(x,b)    = b*x.^2\nf_4(x,b)    = b./(1.0 .+ (exp.(5*(x .- 0.5) )))   \n\nb1,b2,b3,b4 = 1.5,2.0,0.5,2.0\ndgp(x)        = f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4)\n \n# END USER'S OPTIONS  \n\nfunction simul_logistic(n,p,nsimul,modality,dgp)\n\n n_test = 100_000\n loss = :logistic\n\n # initialize containers and parameters for HTBoost and lightGBM\n MSE1 = zeros(nsimul)\n MSE2 = zeros(nsimul)\n\n param  = HTBparam(loss=loss,nfold=1,nofullsample=true,modality=modality,warnings=:Off,newton_gauss_approx =true)\n\n # Create an estimator with the desired parameters—leave other parameters at the default values.\n estimator = LGBMClassification(   # LGBMRegression(...)\n    objective = \"binary\",\n    num_class = 1,\n    categorical_feature = Int[],\n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    metric = [\"binary_logloss\"],\n    num_threads = number_workers,\n    device_type=\"cpu\"\n )\n\n for simul in 1:nsimul\n\n    # generate data\n    x,x_test = randn(n,p), randn(n_test,p)\n    ftrue       = dgp(x)\n    ftrue_test  = dgp(x_test)\n\n    y = (exp.(ftrue)./(1.0 .+ exp.(ftrue))).>rand(n) \n    data   = HTBdata(y,x,param)\n\n    output = HTBfit(data,param)\n   yf     = HTBpredict(x_test,output,predict=:Egamma)  # predict the natural parameter (only with simulated data; typically we'll want to predict=:Ey (default))\n   MSE1[simul]    = sum((yf - ftrue_test).^2)/n_test\n\n    # lightGBM\n    y       = Float64.(y)                 \n    n_train = Int(round((1-param.sharevalidation)*length(y)))\n    x_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\n    x_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n    \n   # parameter search over num_leaves and max_depth\n   splits = (collect(1:n_train),collect(1:min(n_train,100)))  # goes around the problem that at least two training sets are required by search_cv (we want the first)\n\n   params = [Dict(:num_leaves => num_leaves,\n               :max_depth => max_depth) for\n          num_leaves in (4,16,32,64,127,256),\n          max_depth in (2,3,5,6,8)]\n\n   lightcv = LightGBM.search_cv(estimator,x,y,splits,params,verbosity=-1)\n\n   loss_cv = [lightcv[i][2][\"validation\"][estimator.metric[1]][1] for i in eachindex(lightcv)]\n   minind = argmin(loss_cv)\n\n   estimator.num_leaves = lightcv[minind][1][:num_leaves]\n   estimator.max_depth  = lightcv[minind][1][:max_depth]\n\n   # fit at cv parameters\n   LightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\n\n    yf_gbm = LightGBM.predict(estimator,x_test)[:,1]\n    yf_gbm = log.(yf_gbm./(1.0 .- yf_gbm))\n\n    MSE2[simul]    = sum((yf_gbm - ftrue_test).^2)/n_test\n\n\n end     \n\n return MSE1,MSE2\n\nend \n\n\nMSE1,MSE2 = simul_logistic(n,p,nsimul,modality,dgp)\n\nprintln(\"\\n n = $n, p = $p, number of simulations = $nsimul, modality = $modality\")\nprintln(\" avg out-of-sample RMSE from true natural parameter, HTBoost    \", sqrt(mean(MSE1)) )\nprintln(\" avg out-of-sample RMSE from true natural parameter, lightGBM      \", sqrt(mean(MSE2)) )\n","category":"page"},{"location":"JuliaAPI/#API","page":"API","title":"API","text":"","category":"section"},{"location":"JuliaAPI/#Overview","page":"API","title":"Overview","text":"","category":"section"},{"location":"JuliaAPI/","page":"API","title":"API","text":"HTBinfo","category":"page"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBinfo","page":"API","title":"HybridTreeBoosting.HTBinfo","text":"HTBinfo()\n\nBasic information about the main functions in HTBoost (see help on each function for more details)\n\nSetting up the model\n\nHTBindexes_from_dates builds train and test sets indexes for expanding window cross-validation (if the user wants to over-ride the default block-cv)\nHTBparam           parameters, defaults or user-provided.\nHTBdata            y,x and, optionally, dates, weights, and names of features\n\nFitting and forecasting\n\nHTBfit             fits HTBoost with cv (or validation/early stopping) of number of trees and, optionally, other parameters;                      pre-determined options for more or less extensive cross-validation are controlled by :modality in HTBparam\nHTBpredict         predictions for y or natural parameter\nHTBcv              user-controlled cross-validation. (Note that the recommended process is to use :modality in HTBfit rather than HTBcv.)            \n\nPOST-ESTIMATION ANALYSIS\n\nHTBcoeff           provides information on constant coefficients, e.g. dispersion and dof for loss=:t\nHTBrelevance       computes feature importance (Breiman et al 1984 relevance)\nHTBoutput          collects fitted parameters in matrices\nHTBweightedtau     computes weighted smoothing parameter to help assess function smoothness\nHTBpartialplot     partial dependence plots (keeping all other features fixed, not integrating out)\nHTBmarginaleffect  Numerical computation of marginal effects.\nHTBplot_tau        plotting sigmoid for a given tau \nHTBplot_ppr        provides tau and plot for projection pursuit for a single tree\n\nExample of use of info function \n\nhelp?> HTBinfo\nor ...\njulia> HTBinfo()\n\nTo find more information about a specific function, e.g. HTBfit\n\nhelp?> HTBfit\n\nExample of basic use of HTBoost functions with iid data and default settings\n\nparam  = HTBparam()                                 \ndata   = HTBdata(y,x,param)\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)\n\nSee the examples and tutorials for illustrations of use. \n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#Setting-up-the-model","page":"API","title":"Setting up the model","text":"","category":"section"},{"location":"JuliaAPI/","page":"API","title":"API","text":"HTBparam\nHTBdata\nHTBindexes_from_dates","category":"page"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBparam","page":"API","title":"HybridTreeBoosting.HTBparam","text":"HTBparam(;)\n\nParameters for HTBoost\n\nNote: all Julia symbols can be replaced by strings. e.g. :L2 can be replaced by \"L2\".\n\nParameters that are most likely to be modified by user (all inputs are keywords with default values)\n\nloss             [:L2] Supported distributions:\n:L2 (Gaussian), aliases :l2,:mse,:Gaussian,:normal\n:logistic, aliases :binary (binary classification)\n:multiclass (multiclass classification)\n:t, aliases :student (student-t, robust alternative to :L2)\n:Huber, aliases :huber \n:gamma\n:lognormal, aliases :logL2, :logl2 (positive continuous data) \n:Poisson (count data)\n:gammaPoisson, aliases :gammaPoisson,:gammapoisson,:negbin,:negative_binomial (aka negative binomial, count data)\n:L2loglink, aliases :l2loglink (alternative to :L2 if y≥0)\n:hurdleGamma (zero-inflated y)\n:hurdleL2loglink (zero-inflated y)\n:hurdleL2 (zero-inflated y)\nSee the examples for uses of each loss function. Fixed coefficients (such as shape for :gamma, dispersion and dof for :t, and overdispersion for :gammaPoisson) are computed internally by maximum likelihood. Inspect them using HTBcoeff(). In HTBpredict(), predictions are for E(y) if predict=:Ey (default), while predict=:Egamma forecasts the fitted parameter ( E(logit(prob) for :logistic, log(E(y)) for :gamma etc ... )\nmodality         [:compromise] Options are: :accurate, :compromise (default), :fast, :fastest.                    These options are meant to replace the need for the user to cross-validate parameters. Advanced users with a big computational budget can still do so.                    :fast and :fastest run only one model, while :compromise and :accurate cross-validate the most important parameters.                    :fast runs only one model (only cv number of trees) at values defined in param = HTBparam().                     :fastest runs only one model, setting lambda=0.2, nfold=1 and nofullsample=true (does not re-estimate on the full sample after cv).                     Recommended for faster preliminary analysis only.                     In most cases, :fast and :fastest also use the quadratic approximation to the loss for large samples.                     :compromise and :accurate cross-validates several models at the most important parameters (see HTBfit() for details),                     then stack all the cv models.\npriortype               [:hybrid] :hybrid encourages smoothness, but allows both smooth and sharp splits, :smooth forces smooth splits,                           :disperse is :hybrid but with no penalization encouraging smooth functions (not recommended).                           Set to :smooth if you want to force derivatives to be defined everywhere. \nrandomizecv       [false] default is block-cv (aka purged cv); a time series or panel structure is automatically detected (see HTBdata)                           if a date column is provided. Set to true for standard cv.\nnfold              [4] n in n-fold cv. Set nfold = 1 for a single validation set (by default the last param.sharevalidation share of the sample).                           nfold, sharevalidation, and randomizecv are disregarded if train and test observations are provided by the user.\nsharevalidation:        [0.30] Can be: a) Integer, size of the validation set, or b) Float, share of validation set.                           Relevant only if nfold = 1.\nindtrain_a:Vector{Vector{I}} [] for user's provided array of indices of train sets. e.g. vector of 5 vectors, each with indices of train set observations\nindtest_a:Vector{Vector{I}}  [] for user's provided array of indices of test sets. e.g. vector of 5 vectors, each with indices of train set observations\nnofullsample      [false] if true and nfold=1, HTBoost is not re-estimated on the full sample after validation.                           Reduces computing time by roughly 60%, at the cost of a modest loss of accuracy.                           Useful for very large datasets, in preliminary analysis, in simulations, and when instructions specify a train/validation                           split with no re-estimation on full sample. Activated by default when modality=:fastest.     \ncat_features            [] vector of indices of categorical features, e.g. [2] or [2,5], or vector of names in DataFrame,                           e.g. [:wage,:age] or [\"wage\",\"age\"]. If empty, categoricals are automatically detected as non-numerical features.\ncv_categoricals     [:default] whether to run preliminary cross-validation on parameters related to categorical features.                       :none uses default parameters                        :penalty runs a rough cv the penalty associated to the number of categories; recommended if n/n_cat if low for any feature, particularly if SNR is low                                                    :n0 runs a rough of cv the strength of the prior shrinking categorical values to the overall mean; recommended with highly unequal number of observations in different categories                       :both runs a rough cv or penalty and n0                        :default uses :none for modality in [:fastest,:fast], :penalty for :compromise, and :both for :accurate        \noverlap:            [0] number of overlaps in time series and panels. Typically overlap = h-1, where y(t) = Y(t+h)-Y(t). Used for purged-CV.\nverbose         [:Off] verbosity :On or :Off\nwarnings        [:On] or :Off\n\nParameters that may sometimes be be modified by user\n\nlambda           [0.1 or 0.2] Learning rate. 0.1 for (nearly) best performance. 0.2 can be almost as accurate, particularly if the function is smooth and p is small.                    The default is 0.1, except in modality = :fastest, where it's 0.2. Modality = :compromise carries out the cv at lambda=0.2 and then fits the best model at 0.1.                    Consider 0.05 if tiny improvements in accuracy are important and computing time is not a concern.\ndepth              [5] tree depth. Unless modality = :fast or :fastest, this is over-written as depth is cross-validated. See HTBfit() for more options.\nweights                 NOTE: weights for weighted likelihood are set in HTBdata, not in HTBparam.\noffset                  NOTE: offsets (aka exposures) are set in HTBdata, not in HTBparam. See examples/Offset or exposure.jl     \nsparsity_penalization   [0.3] positive numbers encourage sparsity. The range [0.0-1.5] should cover most scenarios.                            Automatically cv in modality=:compromise and :accurate. Increase to obtain a more parsimonious model, set to 0 for standard boosting.\nntrees             [2000] Maximum number of trees. HTBfit will automatically stop when cv loss stops decreasing.\nsharevs                 [1.0] row subsampling in variable selection phase (only to choose feature on which to split.) Default is no subsampling.                           sharevs = :Auto sets the subsample size to min(n,50k*sqrt(n/50k)).                           At high n, sharevs<1 speeds up computations, but can reduce accuracy, particularly in sparse setting with low SNR.         \nsubsampleshare_columns  [1.0] column subsampling (aka feature subsampling) by level.\nmin_unique              [:default] sharp splits are imposed on features with less than min_unique values (default is 5 for modality=:compromise or :accurate, else 10)\nmixed_dc_sharp          [false] true to force sharp splits on discrete and mixed discrete-continuous features (defined as having over 20% obs on a single value)\ndelete_missing          [false] true to delete rows with missing values in any feature, false to handle missing internally (recommended).\ntheta                   [1]  numbers larger than 1 imply tighter penalization on β (final leaf values) compared to default.\nmeanlntau               [1.0] prior mean of log(τ). Set to higher numbers to suggest less smooth functions.        \nmugridpoints       [11] number of points at which to evaluate μ during variable selection. 5 is sufficient on simulated data with normal or uniform distributions, but actual data may benefit from more (due to with highly non-Gaussian features).                           For extremely complex and nonlinear features, more than 10 may be needed.        \nforce_sharp_splits      [] optionally, a p vector of Bool, with j-th value set to true if the j-th feature is forced to enter with a sharp split.\nforce_smooth_splits     [] optionally, a p vector of Bool, with j-th value set to true if the j-th feature is forced to enter with a smooth split (high values of τ not allowed).\ncat_representation_dimension  [4 (2 for classification)] 1 for mean encoding, 2 adds frequency, 3 adds variance, 4 adds robust skewness, 5 adds robust kurtosis\nlosscv                  [:default] loss function for cross-validation (:mse,:mae,:logistic,:sign). \nn_refineOptim      [10^6] MAXIMUM number of observations to use fit μ and τ (split point and smoothness).                           Lower numbers can provide speed-ups with very large n at some cost in terms of fit.\nloglikdivide         [1.0] Higher numbers increase the strength or all priors. The defaults sets it internally using HTBloglikdivide(),                           when it detects a dates series in HTBdata().\ntau_threshold         [10.0] lowest threshold for imposing sharp splits. Lower numbers give more sharp splits.\nmultiplier_stdtau    [5.0] The default priors suggest smoother splits on features whose unconditional distribution (appropriately transformed according to the link function) is closer to the unconditional distribution of y or, when not applicable, to a Gaussian. To disengage this feature, set multiplier_stdtau = 0\n\nAdditional parameters to control the cross-validation process can be set in HTBfit(), but keeping the defaults is generally encouraged.\n\nExample\n\nparam = HTBparam()\n\nExample\n\nparam = HTBparam(nfold=1,nofullsample=true)\n\n\n\n\n\n","category":"type"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBdata","page":"API","title":"HybridTreeBoosting.HTBdata","text":"    HTBdata(y,x,param,[dates];weights=[],fnames=[],offset=[])\n\nCollects and pre-processes data in preparation for fitting HTBoost\n\nInputs\n\ny::Vector              Vector of responses. Can be a vector of lables, or a dataframe with one column. \nx                      Matrix of features. Can be a vector or matrix of floats, or a dataframe. Converted internally to a Matrix{T}, T as defined in HTBparam\nparam::HTBparam\n\nOptional Inputs\n\ndates::AbstractVector  [1:n] Typically Vector{Date} or Vector{Int}. Used in cross-validation to determine splits.                           If not supplied, the default 1:n assumes a cross-section of independent realizations (conditional on x) or a single time series.\n'weights'                [ones(T,n)] vector of floats or Floats, weights for weighted likelhood\nfnames::Vector{String} [x1, x2, ... ] feature names\n'offset'                 vector of offsets (exposure), in logs if the loss adopts a loss link (:gamma,:gammaPoisson,:L2loglink,....) \n\nExamples of use\n\ndata = HTBdata(y,x,param)\ndata = HTBdata(y,x,param,dates,fnames=names)\ndata = HTBdata(y,df[:,[:CAPE, :momentum ]],param,df.dates,fnames=df.names)\ndata = HTBdata(y,df[:,3:end],param)\n\nNotes\n\nWhen dates are provided, the data will be ordered chronologically (for cross-validation functions) unless the user has provided explicit training and validation sets.\n\n\n\n\n\n","category":"type"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBindexes_from_dates","page":"API","title":"HybridTreeBoosting.HTBindexes_from_dates","text":"HTBindexesfromdates(df::DataFrame,datesymbol::Symbol,firstdate::Date,nreestimate::Int)\n\nComputes indexes of training set and test set for cumulative CV and pseudo-real-time forecasting exercises in time series and panel data. The function is inefficient for large datasets, but does not require the data to be sorted.\n\nINPUTS\ndf                    DataFrame with dates and other variables\ndatesymbol            symbol (or string) name of the date\nfirst_date            when the first training set ENDS (end date of the first training set)\nn_reestimate          every how many periods to re-estimate (update the training set)\nOUTPUT indtraina,indtesta are arrays of arrays of indexes of train and test samples\nExample of use\nfirstdate = Date(\"2017-12-31\", Dates.DateFormat(\"y-m-d\"))   indtraina,indtesta = HTBindexesfromdates(df,:date,firstdate,12)\nor as a named tuple with elements indtraina,indtesta)\nt = HTBindexesfromdates(df,:date,first_date,12)\nNOTES\nInefficient for large datasets\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#Fit-and-predict","page":"API","title":"Fit and predict","text":"","category":"section"},{"location":"JuliaAPI/","page":"API","title":"API","text":"HTBfit\nHTBpredict\nHTBcv","category":"page"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBfit","page":"API","title":"HybridTreeBoosting.HTBfit","text":"HTBfit(data,param;cv_grid=[],cv_sparsity=:Auto,cv_depthppr=:Auto)\n\nFits HTBoost with with k-fold cross-validation of number of trees and depth, and possibly a few more models.\n\nIf param.modality is :fast or :fastest, fits one model, at param, and if needed a second where sharp splits are  forced on features with high average values of τ. For param.modality=:accurate or :compromise,   may then cross-validate the following hyperparamters:\n\nParameters for categorical features, if any.\ndepth in the range 1-6 (1-7 for :accurate) (starts with 3 and 5, then moves up or down based on results, breaking the loop as soon as the cv loss increases)\nA penalization to encourage sparsity (fewer relevant features). Whether this is performed or not depends on the effective sample size (n and signal-to-noise ratio).\nA model without projection pursuit nonlinear expansion of trees (only if modality=:accurate). \nOne or two models with column subsampling and slightly lower learning rate. One in :compromise, two in :accurate if the first reduced the cv loss. \n\nA maximum of 10 models are fitted in modality = :accurate, and of 7 in :compromise.\n\nThe default range for depth can be replaced by providing a vector cv_grid, e.g. \n\nHTBfit(data,param,cv_grid = [2,4,6])\n\nThe sparsity penalization can be de-activated by setting cv_sparsity=false, e.g. \n\n    HTBfit(data,param,cv_sparsity=false)\n\nIf param.modality=:accurate, the learning rate lambda for all models is left at param.lambda (0.1 in default). If modality=:compromise, lambda=0.2 is used in cv, and the best model is then refitted with lambda = param.lambda. \n\nFinally, all the estimated models considered are stacked, with weights chosen to minimize the cross-validated (original) loss.    Unless modality=:accurate, this stacking will typically be equivalent to the best model.\n\nInputs\n\ndata::HTBdata\nparam::HTBparam\n\nOptional inputs\n\ncv_grid::Vector         Defaul [2,3,5,6]. The code performs a search in the space depth in [2,3,5,6], trying to fit few models if possible. Provide a                           vector to over-ride (e.g. [2,4])\ncv_sparsity             Default :Auto. Set cvsparsity = true to guarantee search over sparsity penalization or cvsparsity=false to disactivate (and save computing time.)                           In :Auto, whether the cv is performed or not depends on :modality and on the n/p ratio. (Not implemented yet: it should ideally also depend on the signal-to-noise ratio). \ncv_depthppr             true to cv whether to remove projection pursuit regression. Default is true for modality in [:compromise,:accurate], else false.\ncv_col_subsample        Default:Auto. If true, fits one or two models with column subsampling. \n\nOutput (named tuple, or vector of named tuple for hurdle models)\n\nindtest::Vector{Vector{Int}}  indexes of validation samples\nbestvalue::Float              best value of depth in cv_grid\nbestparam::SAMRTparam         param for best model  \nntrees::Int                   number of trees (best value of param.ntrees) for best model\nloss::Float                   best cv loss\nlossw::Float                  loss of stacked models\nmeanloss:Vector{Float}        mean cv loss at bestvalue of param for param.ntrees = 1,2,....\nstdeloss:Vector{Float}        standard errror of cv loss at bestvalue of param for param.ntrees = 1,2,....\nlossgrid::Vector{Float}       cv loss for best tree size for each grid value \nloglikdivide:Float            loglikdivide. effective sample size = n/loglikvide. Roughly accounts for cross-correlation and serial correlation \nHTBtrees::HTBoostTrees        for the best cv value of param and ntrees\nHTBtrees_a                    length(cv_grid) vector of HTBtrees\ni                             (ntrees,depth) matrix of threshold features for best model\nmu                            (ntrees,depth) matrix of threshold points  for best model\ntau                           (ntrees,depth) matrix of sigmoid parameters for best model\nfi2                           (ntrees,depth) matrix of feature importance, increase in R2 at each split, for best model\nw                             length(cv_grid) vector of stacked weights\nratio_actual_max              ratio of actual number of candidate features over potential maximum. Relevant if sparsevs=:On: indicates sparsevs should be switched off if too high (e.g. higher than 0.5).\nproblems                      true if there were computational problems in any of the models: NaN loss or loss jumping up\n\nNotes\n\nThe following options for cross-validation are specified in param: randomizecv, nfold, sharevalidation, stderulestop\n\nExamples of use:\n\nparam = HTBparam()\ndata   = HTBdata(y,x,param)\noutput = HTBfit(data,param)\nntrees = output.ntrees \nbest_depth = output.bestvalue \n\nExample for hudle models (loss in [:hurdleGamma,:hurdleL2loglink,:hurdleL2])\nntrees_0    = output[1].ntrees   # number of trees for logistic regression, 0-not0\nntrees_not0 = output[2].ntrees   # number of trees for gamma or L2 loss\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBpredict","page":"API","title":"HybridTreeBoosting.HTBpredict","text":"HTBpredict(x,output)\n\nForecasts from HTBoost, for y or the natural parameter.\n\nInputs\n\nx                           (n,p) DataFrame or Float matrix of forecast origins (type<:real) or p vector of forecast origin                               In the same format as the x given as input is HTBdata(y,x,...). May contain missing or NaN.\noutput                      output from HTBfit\n\nOptional inputs\n\npredict                    [:Ey], :Ey or :Egamma. :Ey returns the forecast of y, :Egamma returns the forecast of the natural parameter                              (e.g. logit(prob) for :logistic and E(log(y)|x) for :lognormal).\nbest_model                 [false] true to use only the single best model, false to use stacked weighted average\noffset                     (n) vector, offset (or exposure), in terms of gamma (log exposure if the loss has a log-link)     \ncutoff_paralellel          [20_000] if x has more than these rows, a parallellized algorithm is called (which is slower for few forecasts)\n\nOutput for standard models\n\nyf                         (n) vector of forecasts of y (or, outside regression, of the natural parameter), or scalar forecast if n = 1\n\nOutput for hurdle models\n\nyf                         (n) vector of forecasts of E(y|x) for the combined model\nprob0                      (n) vector of forecasts of prob(y=0|x)\nyf_not0                    (n) vector of forecasts of E(y|x,y /=0)\n\nOutput for loss = :multiclass\n\nyf                        (n,num_class) matrix, yf[i,j] if the probability that observation i belongs to class j\nclass_values              (numclas) vector, classvalue[j] is the value of y associated with class j\nymax                      (n) vector, ymax[i] is the class value with highest probability at observation i. \n\nExamples of use\n\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_oos,output)\nyf     = HTBpredict(x_oos,output,best_model=true)\nyf     = HTBpredict(x_oos,output,offset = log.(exposure) )\n\nyf,prob0,yf_not0 = HTBpredict(x_oos,output)  # for hurdle models. Or as a tuple t = HTBpredict(x_oos,output) \nyf,class_value,ymax = HTBpredict(x_oos,output)  # for multiclass. Or as a tuple t = HTBpredict(x_oos,output)\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBcv","page":"API","title":"HybridTreeBoosting.HTBcv","text":"HTBcv(data,param,params_cv;internal_cv=true)\n\nUser's controlled cross-validation for HTBoost.\n\nThe recommended process for HTBoost is to use modality (:fastest, :fast, :compromise, :accurate) in HTBfit() rather than HTBcv(). The various modalities in HTBfit() internally control the most important hyperparameters, being as parsimonious as possible due  to the high computational costs of HTB. HTBfit() also stacks the models, which is not done here. All modalities use early stopping to determine the number of trees, so ntrees should not be cv. \n\nThe function HTBcv() is provided for advanced users who want to fully control the cross-validation process and can incur the costs. HTBcv() has the option to set internal_cv=true, in which case cv is also performed internally (at increase cost).\n\nA good use of HTBcv() is to cv a parameter that is not included in the modality (e.g. the strength of the smoothness prior), in conjunction with internal_cv = true. (At increase computational cost.)\n\nInputs\n\ndata                    HTBdata type \nparam                   HTBparam type. Includes number of folds, randomization (or block-cv) and, optionally, indexes of train and validation folds\nparams_cv               Array of Dictionaries. Each dictionary contains the hyperparameters to be cv'ed (keys) and the values to be cv'ed (values). \n\nOptional Inputs\n\ninternal_cv             [true] If true, it will perform internal cv (as dictated by param.modality) for each element of the dictionary.                           This could be very slow. Set to false if you want to fully control the cv process.                           NOTE: if false, param.modality is irrelevant (set to :fast)\n\nOutput (named tuple)\n\nbestindex               Index of the best model in params_cv \nbestparam               param (type HTBparam) of the best model\noutput                  output of the best model\nloss                    loss of the best model\noutput_a                Array of outputs for all models\nloss_a                  Array of losses for all models\n\nExample of use: cv over varlntau (strength of smoothness prior), and also cv internally.\n\n# Set up model and data \nparam  = HTBparam(loss=:L2,modality=:accurate,nfold=4)  # number of folds, randomization (or block-cv) and, optionally, indexes of train and validation folds\ndata   = HTBdata(x,y,param)\n\n# Specify hyperparameters and values for cv as a Dictionary. Here we cv only varlntau (strength of smoothness prior).\n# The resuls is a one-dimensional array. More dimensions can be added in the same way. \nparams_cv = [Dict(\n    :varlntau => varlntau)    # strength of prior on log(tau) (smoothness). Default is 0.5^2. Smaller numbers are stronger priors.\n    for\n    varlntau in (0.25^2,0.5^1,1.0^2)\n    ]\n\nhtbcv = HTBcv(data,param,params_cv)      # cv over dictionary and internally\n\n# Some info about the best model \nbestindex = htbcv.bestindex   # params_cv[bestindex] is for best set of cv hyperparameters       \nbestparam = htbcv.bestparam   \n\n# predict using best model\nyf    = HTBpredict(x_oos,htbcv.output)\n\nExample of use: replace HTBfit() (not recommended) by setting internal_cv=false\n\n# Set up model and data \nparam  = HTBparam(loss=:L2,randomizecv=false,nfold=4)  # number of folds, randomization (or block-cv) and, optionally, indexes of train and validation folds\ndata   = HTBdata(x,y,param)\n\n# Specify hyperparameters and values for cv as a Dictionary. Here we cv depth and varlntau (strength of smoothness prior).\n# The resuls is a two-dimensional array. More dimensions can be added in the same way. \nparams_cv = [Dict(\n    :depth => depth,\n    :varlntau => varlntau)    # strength of prior on log(tau) (smoothness). Default is 0.5^2. Smaller numbers are stronger priors.\n    for\n    depth in (2,4,6),\n    varlntau in (0.25^2,0.5^1,1.0^2)\n    ]\n\nhtbcv = HTBcv(data,param,params_cv,internal_cv=false)       \n\n# Some info about the best model \nbestindex = htbcv.bestindex   # params_cv[bestindex] is for best set of cv hyperparameters       \nbestparam = htbcv.bestparam   \noutput    = htbcv.output\n\n# predict using best model\nyf    = HTBpredict(x_oos,htbcv.output)\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#Post-estimation-analysis","page":"API","title":"Post-estimation analysis","text":"","category":"section"},{"location":"JuliaAPI/","page":"API","title":"API","text":"HTBcoeff\nHTBrelevance\nHTBpartialplot\nHTBmarginaleffect\nHTBoutput\nHTBweightedtau\nHTBplot_tau\nHTBplot_ppr","category":"page"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBcoeff","page":"API","title":"HybridTreeBoosting.HTBcoeff","text":"HTBcoeff(output;verbose=true)\n\nProvides some information on constant coefficients for best model (in the form of a tuple.) For example, error variance for :L2, dispersion and dof for :t.\n\nInputs\n\noutput                      output from HTBfit\n\nOutput\n\ncoeff                      named tuple with information on fixed coefficients (e.g. variance for :L2, dispersion and dof for :t)\n\nExample of use\n\noutput = HTBfit(data,param)\ncoeff  = HTBcoeff(output,verbose=false)\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBrelevance","page":"API","title":"HybridTreeBoosting.HTBrelevance","text":"HTBrelevance(output,data::HTBdata;verbose=true,best_model=false)\n\nComputes feature importance (summing to 100), defined by the relevance measure of Breiman et al. (1984), equation 10.42 in Hastie et al., \"The Elements of Statistical Learning\", second edition, except that the normalization is for sum = 100, not for largest = 100. Relevance is defined on the fit of the trees on pseudo-residuals. bestmodel=true for single model with lowest CV loss, bestmodel= false for weighted average (weights optimized by stacking)\n\nOutput\n\nfnames::Vector{String}         feature names, same order as in data\nfi::Vector{Float}              feature importance, same order as in data\nfnames_sorted::Vector{String}  feature names, sorted from highest to lowest importance\nfi_sorted::Vector{Float}       feature importance, sorted from highest to lowest\nsortedindx::Vector{Int}        feature indices, sorted from highest to lowest importance\n\nExample of use\n\noutput = HTBfit(data,param)\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data,verbose = false)\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data,best_model=true)\n\nor as a named tuple \n\nt = HTBrelevance(output,data,verbose = false)\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBpartialplot","page":"API","title":"HybridTreeBoosting.HTBpartialplot","text":"HTBpartialplot(data::HTBdata,output,features::Vector{Int64};,predict=:Egamma,best_model=false,other_xs::Vector=[],q1st=0.01,npoints=1000))\n\nPartial dependence plot for selected features. Notice: Default is for natural parameter (gamma) rather than y. For feature i, computes gamma(xi) - gamma(xi=mean(xi)) for xi between q1st and 1-q1st quantile, with all other features at their mean.\n\nInputs\n\ndata::HTBdata\noutput\nfeatures::Vector{Int}        position index (in data.x) of features to compute partial dependence plot for\nother_xs::Vector{Float}      (keyword), a size(data.x)[2] vector of values at which to evaluate the responses. []                                Note: otherxs should be expressed in standardized units, i.e. for (xi-mean(xi))/std(xi)   \nq1st::Float                  (keyword) first quantile to compute, e.g. 0.001. Last quantile is 1-q1st. [0.01]\n`npoints::Int'                 (keyword) number of points at which to evalute f(x). [1000]\n\nOptional inputs\n\npredict                    [:Egamma], :Ey or :Egamma. :Ey returns the impact on the forecast of y, :Egamma on the natural parameter.  \nbest_model                 [false]  true for single model with lowest CV loss, =false for weighted average (by stacking)\n\nOutput\n\nq::Matrix                   (npoints,length(features)), values of xi at which f(xi) is evaluated\npdp::Matrix                 (npoints,length(features)), values of f(x_i)\n\nExample of use\n\noutput = HTBfit(data,param)\nq,pdp  = HTBpartialplot(data,output.HTBtrees,sortedindx[1,2],q1st=0.001)\n\nor as a named tuple \n\nt      = HTBpartialplot(data,output.HTBtrees,sortedindx[1,2],q1st=0.001)\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBmarginaleffect","page":"API","title":"HybridTreeBoosting.HTBmarginaleffect","text":"HTBmarginaleffect(data::HTBdata,output,features::Vector{Int64};predict=:Egamma,best_model=false,other_xs::Vector =[],q1st=0.01,npoints=50,epsilon=0.02)\n\nAPPROXIMATE Computation of marginal effects using NUMERICAL derivatives (default ϵ=0.01)\n\nInputs\n\ndata::HTBdata\nHTBtrees::HTBoostTrees\nfeatures::Vector{Int}        position index (in data.x) of features to compute partial dependence plot for\nother_xs::Vector{Float}      (keyword), a size(data.x)[1] vector of values at which to evaluate the marginal effect. []                                Note: otherxs should be expressed in standardized units, i.e. for (xi-mean(xi))/std(xi)   \nq1st::Float                  (keyword) first quantile to compute, e.g. 0.001. Last quantile is 1-q1st. [0.01]\n`npoints::Int'                 (keyword) number of points at which to evalute df(xi)/dxi. [50]\n`epsilon::Float'               (keyword) epsilon for numerical derivative, [0.01]\n\nOptional inputs\n\npredict                     [:Egamma], :Ey or :Egamma. :Ey returns the impact on the forecast of y, :Egamma on the natural parameter.  \nbest_model                  [false]  true for single model with lowest CV loss, =false for weighted average (by stacking)\n\nOutput\n\nq::Matrix                   (npoints,length(features)), values of xi at which f(xi) is evaluated, or vector if npoints = 1\nd::Matrix                   (npoints,length(features)), values of marginal effects, or vector if npoints = 1\n\nNOTE: Provisional! APPROXIMATE Computation of marginal effects using NUMERICAL derivatives. (Analytical derivatives are available)\n\nNOTE: To compute marginal effect at one point x0 rather than over a grid, set npoints = 1 and other_xs = x0 (a p vector, p the number of features)\n\nExample\n\noutput = HTBfit(data,param)\nq,m    = HTBmarginaleffect(data,output.HTBtrees,[1,3])\nt      = HTBmarginaleffect(data,output.HTBtrees,[1,3])   # as named tuple\n\nExample\n\nq,m  = HTBmarginaleffect(data,output.HTBtrees,[1,2,3,4],other_xs = zeros(p),npoints = 1)\n\nExample\n\noutput = HTBfit(data,param)\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output.HTBtrees,data,verbose=false)\nq,m  = HTBmarginaleffect(data,output,sortedindx[1,2],q1st=0.001)\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBoutput","page":"API","title":"HybridTreeBoosting.HTBoutput","text":"HTBoutput(HTBtrees::HTBoostTrees;exclude_pp = true)\n\nOutput fitted parameters estimated from each tree, collected in matrices. Excluded projection pursuit regression parameters.\n\nInputs\n\nThe default exclude_pp does not give μ and τ for projection pursuit regression. \n\nOutput\n\ni         (ntrees,depth) matrix of threshold features\nμ         (ntrees,depth) matrix of threshold points\nτ         (ntrees,depth) matrix of sigmoid parameters\nfi2       (ntrees,depth) matrix of feature importance, increase in R2 at each split\nm         (ntrees,depth) matrix of threshold points for missing values \nβ         (ntrees,2^depth) matrix of leaf coefficients (1st phase, excluding ppr)\n\nExample of use\n\noutput = HTBfit(data,param)\ni,μ,τ,fi2,m,β = HTBoutput(output.HTBtrees)\nt = HTBoutput(output.HTBtrees)  # named tuple\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBweightedtau","page":"API","title":"HybridTreeBoosting.HTBweightedtau","text":"HTBweightedtau(output,data;verbose=true,best_model=false)\n\nComputes weighted (by variance importance gain at each split) smoothing parameter τ for each feature, and for the entire model (features are averaged by variance importance) statistics for each feature, averaged over all trees. Sharp thresholds (τ=Inf) are bounded at 40. bestmodel=true for single model with lowest CV loss, bestmodel= false for weighted average (weights optimized by stacking)\n\nInput\n\noutput   output from HTBfit\ndata     data input to HTBfit\n\nOptional inputs\n\nverbose   [true]  prints out the results to screen as DataFrame\nmax_tau   [40]    values of τ at or above this are set to Inf (hard splits) for plotting purposes\n\nOutput. Named tuple with the following fields\n\navgtau         scalar, average importance weighted τ over all features (also weighted by variance importance) \ngavgtau        scalar, geometric average of importance weighted log τ over all features (also weighted by variance importance).                  NB: Default in printed output and plot. (Arguably the more informative measure.) \navgtau_a       p-vector of avg importance weighted τ for each feature \ndf             dataframe collecting avgtau_a information (only if verbose=true)\nx_plot         x-axis to plot sigmoid for gavgtau, in range [-2 2] for standardized feature \ng_plot         y-axis to plot sigmoid for gavgtau \nfnames         feature names\nfi             feature importance, summing to 100\nfnames_sorted  sorted feature names, from highest to lowest importance\nfi_sorted      sorted feature importance\nsortedindx     sorted feature indices\n\nExample of use\n\noutput = HTBfit(data,param)\navgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data)\navgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data,verbose=false,plot_tau=false,best_model=true)\n\nusing Plots\nplot(x_plot,g_plot,title=\"avg smoothness of splits\",xlabel=\"standardized x\",label=:none,legend=:bottomright)    \n\nOr as a named tuple \n\nt = HTBweightedtau(output,data)\nplot(t.x_plot,t.g_plot,title=\"avg smoothness of splits\",xlabel=\"standardized x\",label=:none,legend=:bottomright)\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBplot_tau","page":"API","title":"HybridTreeBoosting.HTBplot_tau","text":"HTBplot_tau(tau;mu=0,sigmoid=:sigmoidsqrt,range=2)\n\nProduces x and y to plot sigmoid function for a given τ (typically the average τ for a feature).  \n\nInput\n\ntau                       smoothing parameter τ\n\nOptional inputs\n\nmu        [0]             location parameter for sigmoid\nsigmoid   [sigmoidsqrt]   :sigmoidsqrt or :sigmoidlogistic\nrange     [2]             x range for plot, [-range range]. x is standardized\nmax_tau   [40]            tau at which a hard split is assumed\n\nOutput\n\nx_plot         x-axis to plot sigmoid for tau, in range [-2 2] for standardized feature \ng_plot         y-axis to plot sigmoid for tau \n\nNote\n\ntau ≥ 40 is interpreted as a hard split. \n\nExample of use\n\noutput = HTBfit(data,param)\navgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data)\nx_plot,g_plot = HTBplot_tau(avgtau[1])     # tau of first feature\nusing Plots\nplot(x_plot,g_plot,title=\"avg tau of feature 1\",xlabel=\"standardized x\",label=:none,legend=:bottomright)\n\nor as a named tuple\n\nt = HTBplot_tau(avgtau[1])     # tau of first feature\nplot(t.x_plot,t.g_plot,title=\"avg tau of feature 1\",xlabel=\"standardized x\",label=:none,legend=:bottomright)\n\n\n\n\n\n","category":"function"},{"location":"JuliaAPI/#HybridTreeBoosting.HTBplot_ppr","page":"API","title":"HybridTreeBoosting.HTBplot_ppr","text":"HTBplot_ppr(output;which_tree=1)\n\nVisualize impact of projection pursuit for an individual tree.\n\nExample of use\n\nyf1,yf0,tau = HTBplot_ppr(output,which_tree=1)\nplot(yf0,yf1)\n\nwhere yf0 is the standardized prediction from the tree, and yf1 is the (non-standardized) prediction after ppr\n\n\n\n\n\n","category":"function"},{"location":"Tutorials_py/#HTBoost-Tutorials","page":"Tutorials (Python)","title":"HTBoost Tutorials","text":"","category":"section"},{"location":"Tutorials_py/","page":"Tutorials (Python)","title":"Tutorials (Python)","text":"I am working on a solution for Python that will not require julia and julicall and will not incur compilation costs each time the program is run. Meanwhile, Python users can run HTBoost via juliacall, as explained here:","category":"page"},{"location":"Tutorials_py/","page":"Tutorials (Python)","title":"Tutorials (Python)","text":"Installation and use in Python","category":"page"},{"location":"Tutorials_py/","page":"Tutorials (Python)","title":"Tutorials (Python)","text":"The following tutorial provides an introduction to the main functions of HTBoost:","category":"page"},{"location":"Tutorials_py/","page":"Tutorials (Python)","title":"Tutorials (Python)","text":"Basic use.  ","category":"page"},{"location":"Tutorials_py/","page":"Tutorials (Python)","title":"Tutorials (Python)","text":"For many more tutorials and examples, see Julia tutorials and Julia examples, using the guidelines in Installation and use in Python to adapt the code to Python. ","category":"page"},{"location":"tutorials_R/Categoricals/#Categoricals-features","page":"-","title":"Categoricals features","text":"","category":"section"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"HTBoost is promising for categorical features, particularly if high dimensionals.   This tutorials shows:","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"How to inform HTBoost about categorical features.\nParameters related to categorical features, and their default values.\nSee Categorical features for a comparison with LightGBM and CatBoost, with discussion.  ","category":"page"},{"location":"tutorials_R/Categoricals/#How-to-inform-HTBoost-about-categorical-features-in-R","page":"-","title":"How to inform HTBoost about categorical features in R","text":"","category":"section"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"When y and/or x contain strings (categorical features), we must translate our R dataframe into a Julia DataFrame, which is then fed to HTBdata(), e.g. (continuing from the previous example)","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"x_string =  sample(c(\"v1\", \"v2\", \"v3\"), n, replace = TRUE)   # create a categorical with 3 values\ndf       = data.frame(x,x_string)                          # R dataframe \nx        = DataFrames$DataFrame(df)                          # x is a Julia dataframe\ndata     = HybridTreeBoosting$HTBdata(y,x,param,fnames=colnames(df))    # pass the column names \noutput   = HybridTreeBoosting$HTBfit(data,param)                        \n","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"Columns of string values are automatically interpreted by HTBoost as a categorical. If some categorical features are represented by numerical values, it is necessary to list them in param (in which case all categorical features, even strings, must be listed). This can be done either with a vector of their column positions, or with their names, if fnames (an optional argument) is provided to HTBdata()","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"# either \nparam = HybridTreeBoosting$HTBparam(cat_features=c(3))\ndata  = HybridTreeBoosting$HTBdata(y,x,param)    # passing the column names is optional\n\n# or\nparam = HybridTreeBoosting$HTBparam(cat_features=c(\"x_string\"))\ndata  = HybridTreeBoosting$HTBdata(y,x,param,fnames=colnames(df))    # passing the column names is required\n","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"See examples/Categoricals for a discussion of how HTBoost treats categoricals under the hood. Key points:","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"Missing values are assigned to a new category.\nIf there are only 2 categories, a 0-1 dummy is created. For anything more than two categories, it uses a variation of target encoding.\nThe categories are encoded by 4 values in default mode: mean, frequency, variance (robust) and skew(robust). (For financial variables, the variance and skew may be more informative than the mean.) Set catrepresentationdimension = 1 to encode by mean only.\nOne-hot-encoding with more than 2 categories is not supported, but can of course be implemented as data preprocessing.","category":"page"},{"location":"tutorials_R/Categoricals/#Cross-validation-of-categorical-parameters","page":"-","title":"Cross-validation of categorical parameters","text":"","category":"section"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"param$cv_categoricals can be used to perform a rough cross-validation of n0_cat and/or mean_encoding_penalization, as follows:","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"cv_categoricals = \"none\" uses default parameters \ncv_categoricals = \"penalty\" runs a rough cv the penalty associated to the number of categories; recommended if n/n_cat if high for any feature, particularly if SNR is low                             \ncv_categoricals = \"n0\" runs a rough of cv the strength of the prior shrinking categorical values to the overall mean; recommended with highly unequal number of observations in different categories.\ncv_categoricals = \"both\" runs a rough cv of penalty and n0 ","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"The default is \"none\" if :modality in (\"fastest\",\"fast\"), \"penalty\" if \"compromise\", and \"both\" if \"accurate\". ","category":"page"},{"location":"tutorials_R/Categoricals/#Comparison-to-LightGBM-and-CatBoost","page":"-","title":"Comparison to LightGBM and CatBoost","text":"","category":"section"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"Different packages differ substantially in their treatment of categorical features.   LightGBM does not use target encoding, and can completely break down (very poor in-sample and oos fit) when the number of categories is high in relation to n (e.g. n=10k, #cat=1k). The LightGBM manual suggests treating high dimensional categorical features as numerical or embedding them in a lower-dimensional space. LightGBM can, however, perform very well in lower-dimensional cases.","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"CatBoost, in contrast, adopts mean target encoding as default, can handle very high dimensionality and has a sophisticated approach to avoiding data leakage which HTBoost is missing. (HTBoost resorts to a penalization on categorical features instead.) CatBoost also interacts categorical features, while HTBoost does not. In spite of the less sophisticated treatment of categoricals, in this simple simulation set-up HTBoost substantially outperforms CatBoost if n_cat is high and the categorical feature interacts with the continuous feature, presumably because target encoding generates smooth functions  by construction in this setting.","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"It seems reasonable to assume that high dimensional target encoding, by its very nature, will generate smooth functions in many settings, making  HTBoost a promising tool for high dimensional categorical features. The current treatment of categorical features is however quite crude compared to CatBoost, so some of these gains are not yet realized. ","category":"page"},{"location":"tutorials_R/Categoricals/","page":"-","title":"-","text":"See Categorical features for a comparison with LightGBM and CatBoost on simulated data, with discussion.","category":"page"},{"location":"tutorials/Categoricals/#Categoricals-features","page":"-","title":"Categoricals features","text":"","category":"section"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"HTBoost is promising for categorical features, particularly if high dimensionals.   This tutorials shows:","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"How to inform HTBoost about categorical features\nParameters related to categorical features, and their default values.\nComparison with LightGBM and CatBoost, with discussion.  ","category":"page"},{"location":"tutorials/Categoricals/#How-to-inform-HTBoost-about-categorical-features","page":"-","title":"How to inform HTBoost about categorical features","text":"","category":"section"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"If cat_features is not specified, non-numerical features (e.g. String or CategoricalValue) are treated as categorical\nIf catfeatures is specified, it can be a vector of Integers (positions), a vector of Strings (corresponding to  data.fnames, which must be provided) or a vector of Symbols (the features' names in the dataframe). Notice that if catfeatures is specified, it must include all features to be treated as categorical (no automatic detection).","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"Example of use: all categorical features are non-numerical","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"param = HTBparam()  ","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"***Examples of use: specify positions in data.x***","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"param = HTBparam(cat_features=[1])    # cat_features must be a vector\nparam = HTBparam(cat_features=[1,9])","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"Example of use: specify names from data.fnames ","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"data = HTBdata(y,x,param,fnames=[\"country\",\"industry\",\"earnings\",\"sales\"]) \nparam = HTBparam(cat_features=[\"country\",\"industry\"])","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"Example of use: specify names in dataframe ","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"data = HTBdata(y,x) #  where x is DataFrame                           \nparam = HTBparam(cat_features=[:country,:industry])         ","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"See examples/Categoricals for a discussion of how HTBoost treats categoricals under the hood. Key points:","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"Missing values are assigned to a new category.\nIf there are only 2 categories, a 0-1 dummy is created. For anything more than two categories, it uses a variation of target encoding.\nThe categories are encoded by 4 values in default mode: mean, frequency, variance (robust) and skew(robust). (For financial variables, the variance and skew may be more informative than the mean.) Set catrepresentationdimension = 1 to encode by mean only.\nOne-hot-encoding with more than 2 categories is not supported, but can of course be implemented as data preprocessing.","category":"page"},{"location":"tutorials/Categoricals/#Cross-validation-of-categorical-parameters","page":"-","title":"Cross-validation of categorical parameters","text":"","category":"section"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"param.cv_categoricals can be used to perform a rough cross-validation of n0_cat and/or mean_encoding_penalization, as follows:","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"cv_categoricals = :none uses default parameters \ncv_categoricals = :penalty runs a rough cv the penalty associated to the number of categories; recommended if n/n_cat if high for any feature, particularly if SNR is low                             \ncv_categoricals = :n0 runs a rough of cv the strength of the prior shrinking categorical values to the overall mean; recommended with highly unequal number of observations in different categories.\ncv_categoricals = :both runs a rough cv of penalty and n0 ","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"The default is :none if :modality in [:fastest,:fast], :penalty if :compromise, and :both if :accurate. ","category":"page"},{"location":"tutorials/Categoricals/#Comparison-to-LightGBM-and-CatBoost","page":"-","title":"Comparison to LightGBM and CatBoost","text":"","category":"section"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"Different packages differ substantially in their treatment of categorical features.   LightGBM does not use target encoding, and can completely break down (very poor in-sample and oos fit) when the number of categories is high in relation to n (e.g. n=10k, #cat=1k). The LightGBM manual suggests treating high dimensional categorical features as numerical or embedding them in a lower-dimensional space. LightGBM can, however, perform very well in lower-dimensional cases.","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"CatBoost, in contrast, adopts mean target encoding as default, can handle very high dimensionality and has a sophisticated approach to avoiding data leakage which HTBoost is missing. (HTBoost resorts to a penalization on categorical features instead.) CatBoost also interacts categorical features, while HTBoost does not. In spite of the less sophisticated treatment of categoricals, in this simple simulation set-up HTBoost substantially outperforms CatBoost if n_cat is high and the categorical feature interacts with the continuous feature, presumably because target encoding generates smooth functions  by construction in this setting.","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"It seems reasonable to assume that high dimensional target encoding, by its very nature, will generate smooth functions in many settings, making  HTBoost a promising tool for high dimensional categorical features. The current treatment of categorical features is however quite crude compared to CatBoost, so some of these gains are not yet realized. ","category":"page"},{"location":"tutorials/Categoricals/#An-example-with-simulated-data","page":"-","title":"An example with simulated data","text":"","category":"section"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"The code below simulate data from y = f(x1,x2) + u, where x1 is continuous and x2 is categorical of possibly very high dimensionality. Each category of x2 is assigned its own coefficient drawn from a distribution (\"uniform\", \"normal\", \"chi\", \"lognormal\"). The user can specify the form of f(x1,x2).","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"number_workers  = 8  # desired number of workers\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random\nusing Statistics\nusing LightGBM\n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"In this example there is only one categorical features. We can control:","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"the number of categories\nthe distribution of their average effect: \"uniform\" or \"normal\" for symmetric, thin-tailed effects, and \"chi\" or \"lognormal\" for right-skewed distributions.\nthe type of interaction with the continuous feature x1","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"\nRandom.seed!(1)\n\nn          =     10_000   # sample size   \nncat       =     100     # number of categories (actual number may be lower as they are drawn with reimmission)\n\nbcat       =     1.0      # coeff of categorical feature (if 0, categories are not predictive)\nb1         =     1.0      # coeff of continuous feature \nstde       =     1.0      # error std\n\ncat_distrib =   \"normal\"  # distribution for categorical effects: \"uniform\", \"normal\", \"chi\", \"lognormal\" for U(0,1), N(0,1), chi-square(1), lognormal(0,1)\ninteraction_type = \"multiplicative\" # \"none\", \"multiplicative\", \"step\", \"linear\"\n\n# specify the function f(x1,x2), with the type of interaction (if any) between x1 (continuous) and x2 (categorical)\nfunction yhat_x1xcat(b1,b2,x1,interaction_type)\n\n    if interaction_type==\"none\"\n        yhat = b2 + b1*x1\n    elseif interaction_type==\"multiplicative\"     \n        yhat = b2 + b1*b2*x1\n    elseif interaction_type==\"step\"     \n        yhat = b2 + b1*x1*(x1>0)    \n    elseif interaction_type==\"linear\"  \n        yhat = b2 + (b1-b2)*x1\n    end    \n\nreturn yhat\n\nend \n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"cv_categoricals = :none uses default parameters for categorical features. To speed up estimation, we set modality=:fast, depth=3, nfold=1 and nofullsample= true.","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"\n# HTBoost parameters \nloss         = :L2\nmodality     = :fast       # :accurate, :compromise, :fast, :fastest\ncv_categoricals = :none    # :none (default), :penalty, :n0, :both\n\ndepth        = 3           # fix depth to further speed up estimation  \nnfold        = 1           # number of folds in cross-validation. 1 for fair comparison with LightGBM \nnofullsample = true        # true to speed up execution when nfold=1. true for fair comparison with LightGBM \nverbose      = :Off\n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"The second feature is categorical. Needs to be an input in param (see below) since it is numerical, and therefore will not be automatically detected as categorical by HTBoost.  ","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"\ncat_features = [2]      \n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"LightGBM parameters ","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"Accoding to the LightGBM manual (https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html): \"For a categorical feature with high cardinality (#category is large), it often works best to treat the feature as numeric, either by simply ignoring the categorical interpretation of the integers or by embedding the categories in a low-dimensional numeric space.\"","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"\nignore_cat_lightgbm = false  # true to ignore the categorical nature and treat as numerical in lightGBM \n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"Having specified all our options, we simulate the data","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"\n# create data \nn_test  = 100_000 \ncate            = collect(1:ncat)[randperm(ncat)]   # create numerical categories (integers) \nxcat,xcat_test  = rand(cate,n),rand(cate,n_test)    # draw element of categorical features from the list of categories\nx,x_test        = hcat(randn(n),xcat),hcat(randn(n_test),xcat_test)\nyhat,yhat_test  = zeros(n),zeros(n_test)\n\nif cat_distrib==\"uniform\"\n    b = bcat*rand(ncat)   # uniform fixed-effects\nelseif cat_distrib==\"normal\"\n    b = bcat*randn(ncat)  # Gaussian fixed effects\nelseif cat_distrib==\"chi\"                        \n    b = bcat*randn(ncat).^2 # chi(1) fixed effects (long tail)\nelseif cat_distrib==\"lognormal\"                        \n    b = bcat*exp.(randn(ncat)) # chi(1) fixed effects (very long tail)\nelse\n    @error \"cat_distribution is misspelled\"\nend \n\nfor r in 1:n\n    b2 = b[findfirst(cate .== xcat[r])]\n    yhat[r] = yhat_x1xcat(b1,b2,x[r,1],interaction_type) \nend\n\nfor r in 1:n_test\n    b2 = b[findfirst(cate .== xcat_test[r])] \n    yhat_test[r] = yhat_x1xcat(b1,b2,x_test[r,1],interaction_type) \nend\n\ny = yhat + stde*randn(n)\ny_test = yhat_test + stde*randn(n_test)\n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"and now fit HTBoost and LightGBM. (We don't show CatBoost here.)","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"\n# Fit HTBoost \nparam  = HTBparam(loss=loss,modality=modality,depth=depth,nfold=nfold,\n                 nofullsample=nofullsample,verbose=verbose,cat_features=cat_features,cv_categoricals=cv_categoricals)\ndata   = HTBdata(y,x,param)\noutput = HTBfit(data,param,cv_grid=[depth])  # cv_grid=[depth] needed to combine modality=:compromise with depth not cv    \n\nntrain = Int(round(n*(1-param.sharevalidation)))\nyhat   = HTBpredict(x[1:ntrain,:],output)    # in-sample fitted value\nyf     = HTBpredict(x_test,output)           # out-of-sample forecasts\n\nprintln(\"\\n n and number of categories \", [n length(unique(xcat))])\nprintln(\"\\n HTBoost \" )\nprintln(\"in-sample R2           \", 1 - mean((yhat - y[1:ntrain]).^2)/var(y[1:ntrain])  )\nprintln(\" validation  R2         \", 1 - output.loss/var(y[ntrain+1:end]) )\nprintln(\" out-of-samples R2      \", 1 - mean((yf - y_test).^2)/var(y_test) )\n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"Test-set fit is higher than training-sample, but in line with validation-set. ","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":" n and number of categories [10000 100]\n\n HTBoost\n in-sample R2           0.657\n validation  R2         0.645\n out-of-samples R2      0.643","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"Let's inspect function smoothness to gauge whether accuracy gains vs LightGBM can be expected, with the caveat that the different treatment of categoricals makes this comparison less reliable here.","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"avgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data,verbose=true,best_model=false)","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"The categorical feature has a very low average τ, indicating near-linearity. Large gains compared to LightGBM and CatBoost would not be surprising in this case.  ","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"\nRow │ feature      importance  avgtau    sorted_feature  sorted_importance  sorted_avgtau \n     │ String       Float32     Float64   String          Float32            Float64       \n─────┼─────────────────────────────────────────────────────────────────────────────────────\n   1 │ x1             45.4137   0.682357  x2                       46.9929        0.558092\n   2 │ x2             46.9929   0.558092  x1                       45.4137        0.682357\n   3 │ x2_cat_freq     1.78321  1.92352   x2_cat_var                5.81018       1.26407\n   4 │ x2_cat_var      5.81018  1.26407   x2_cat_freq               1.78321       1.92352\n\n Average smoothing parameter τ is 0.7.\n \n ...\n\n At 5-7 or lower, HTBoost should strongly outperform other gradient boosting machines.\n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"Fit LightGBM, at default values. ","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"\n# LightGBM\nif ignore_cat_lightgbm == true\n    estimator = LGBMRegression(objective = \"regression\",num_iterations = 1000,early_stopping_round = 100,max_depth=4)\nelse \n    estimator = LGBMRegression(objective = \"regression\",num_iterations = 1000,early_stopping_round = 100,max_depth=4,\n            categorical_feature = cat_features)\nend \n\nn_train         = Int(round((1-param.sharevalidation)*length(y)))\nx_train,y_train = x[1:n_train,:], Float64.(y[1:n_train])\nx_val,y_val     = x[n_train+1:end,:], Float64.(y[n_train+1:end])\n\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\nyhat_gbm_default = LightGBM.predict(estimator,x)[:,1]\nyf_gbm_default = LightGBM.predict(estimator,x_test)[:,1]\n\nprintln(\"\\n LightGBM default, ignore_cat_lightgbm = $ignore_cat_lightgbm  \" )\nprintln(\"\\n in-sample R2      \", 1 - mean((yhat_gbm_default - y).^2)/var(y)  )\nprintln(\" out-of-sample R2  \", 1 - mean((yf_gbm_default - y_test).^2)/var(y_test) )\n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"LightGBM default, ignore_cat_lightgbm = false\n\n in-sample R2      0.660\n out-of-sample R2  0.635\n ```\n\n**High dimensional categoricals**\n\nIf we re-run the script withn n=10k, n_cat = 1k, LightGBM breaks down, as expected.  \nNote: while LightGBM is not suited to high-dimensional categorical, it may outperform HTBoost in lower dimensional settings in which target encoding is not appropriate or effective.  \n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"markdown  n and number of categories [10000 1000]","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"HTBoost  out-of-samples R2      0.500","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"LightGBM default, ignorecatlightgbm = false  out-of-sample R2  0.095","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"LightGBM default, ignorecatlightgbm = true  out-of-sample R2  0.052","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"\nIn LightGBM, the bottlenecks seems to be n/n_cat rather than n_cat per se:\nhere we set n=100k, n_cat=1k and its performance is again satisfactory.\nThe difference in R2 may seem small, but in this example LightGBM would require a sample of n > 1_000_000 to match HTBoost with n = 100_000. \n(Because of the extremely smooth function used to simulate the data.)\n","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"markdown","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"n and number of unique features [100000 1000]","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"HTBoost  out-of-samples R2      0.648","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"LightGBM default, ignorecatlightgbm = false  out-of-sample R2       0.634","category":"page"},{"location":"tutorials/Categoricals/","page":"-","title":"-","text":"```","category":"page"},{"location":"examples/Basic_use/#Basic-use","page":"Basic use","title":"Basic use","text":"","category":"section"},{"location":"examples/Basic_use/","page":"Basic use","title":"Basic use","text":"Short description:","category":"page"},{"location":"examples/Basic_use/","page":"Basic use","title":"Basic use","text":"Illustrates basic use on a regression problem.\nparam.modality as the most important user's choice, depending on time budget. \nIn default modality, HTBoost performs automatic hyperparameter tuning.","category":"page"},{"location":"examples/Basic_use/","page":"Basic use","title":"Basic use","text":"Extensive description: ","category":"page"},{"location":"examples/Basic_use/","page":"Basic use","title":"Basic use","text":"Simulated iid data, additively nonlinear dgp.","category":"page"},{"location":"examples/Basic_use/","page":"Basic use","title":"Basic use","text":"default loss is :L2. Other options for continuous y are :Huber, :t (recommended in place of :Huber), :gamma, :L2loglink. For zero-inflated continuous y, options are :hurdleGamma, :hurdleL2loglink, :hurdleL2 (see examples/Zero inflated y.jl)   \ndefault is block cross-validation: use randomizecv = true to scramble the data.\nfit, with automatic hyperparameter tuning if modality is :compromise or :accurate\nsave fitted model (upload fitted model)\nfeature importance\naverage τ (smoothness parameter), which is also plotted. (Smoother functions ==> larger gains compared to other GBM)\npartial effects plots","category":"page"},{"location":"examples/Basic_use/","page":"Basic use","title":"Basic use","text":"Options for HTBoost: modality is the key parameter guiding hyperparameter tuning and learning rate. :fast and :fastest only fit one model at default parameters, while :compromise and :accurate perform automatic hyperparameter tuning. In HTBoost, it is not recommended that the user performs  hyperparameter tuning by cross-validation, because this process is done automatically if modality is :compromise or :accurate. The recommended process is to first run in modality=:fast or :fastest, for exploratory analysis and to gauge computing time, and then switch to :compromise (default) or :accurate.","category":"page"},{"location":"examples/Basic_use/","page":"Basic use","title":"Basic use","text":"Block cross-validation: While the default in other GBM is to randomize the allocation to train and validation sets, the default in HTBoost is block cv, which is suitable for time series and panels. Set randomizecv=true to bypass this default.  See examples/Global equity Panel.jl for further options on cross-validation (e.g. sequential cv).","category":"page"},{"location":"examples/Basic_use/","page":"Basic use","title":"Basic use","text":"number_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Plots \n\n# USER'S OPTIONS \n\nRandom.seed!(1)\n\n# Some options for HTBoost\nloss      = :L2            # :L2 is default. Other options for regression are :L2loglink (if y≥0), :t, :Huber\nmodality  = :fastest       # :accurate, :compromise (default), :fast, :fastest \n\npriortype = :hybrid       # :hybrid (default) or :smooth to force smoothness \nnfold     = 1             # number of cv folds. 1 faster (single validation sets), default 4 is slower, but more accurate.\nnofullsample = true       # if nfold=1 and nofullsample=true, the model is not re-fitted on the full sample after validation of the number of trees\nrandomizecv = false       # false (default) to use block-cv. \nverbose     = :Off\nwarnings    = :Off\n \n# options to generate data. y = sum of six additive nonlinear functions + Gaussian noise.\nn,p,n_test  = 10_000,6,100_000\nstde        = 1.0\n\nf_1(x,b)    = @. b*x + 1 \nf_2(x,b)    = @. 2*sin(2.5*b*x)  \nf_3(x,b)    = @. b*x^3\nf_4(x,b)    = @. b*(x < 0.5) \nf_5(x,b)    = @. b/(1.0 + (exp(4.0*x )))\nf_6(x,b)    = @. b*(-0.25 < x < 0.25) \n\nb1,b2,b3,b4,b5,b6 = 1.5,2.0,0.5,4.0,5.0,5.0\n\n# END USER'S OPTIONS\n\n# generate data\nx,x_test = randn(n,p), randn(n_test,p)\n\nf        = f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4) + f_5(x[:,5],b5) +  f_6(x[:,6],b6)\nf_test   = f_1(x_test[:,1],b1) + f_2(x_test[:,2],b2) + f_3(x_test[:,3],b3) + f_4(x_test[:,4],b4) + f_5(x_test[:,5],b5) + f_6(x_test[:,6],b6)\n\ny = f + stde*randn(n)\n\n# set up HTBparam and HTBdata, then fit and predit\nparam  = HTBparam(loss=loss,priortype=priortype,randomizecv=randomizecv,nfold=nfold,verbose=verbose,\n                warnings=warnings,modality=modality,nofullsample=nofullsample)\n\ndata   = HTBdata(y,x,param)\n\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output)  \n\n# feature importance and average smoothing parameter for each feature\navgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data,verbose=true,best_model=false);\nplot(x_plot,g_plot,title=\"avg smoothness of splits\",xlabel=\"standardized x\",label=:none)\n\nprintln(\" \\n modality = $(param.modality), nfold = $nfold \")\nprintln(\" depth = $(output.bestvalue), number of trees = $(output.ntrees), avgtau $avgtau \")\nprintln(\" out-of-sample RMSE from truth \", sqrt(sum((yf - f_test).^2)/n_test) )\n\n# save (load) fitted model\n# using JLD2\n#@save \"output.jld2\" output\n#@load \"output.jld2\" output    # Note: key must be the same, e.g. @load \"output.jld2\" output2 is a KeyError\n\n# feature importance, partial dependence plots and marginal effects\nfnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data,verbose=false);\nq,pdp  = HTBpartialplot(data,output,[1,2,3,4,5,6],predict=:Egamma)\n\n# plot partial dependence\npl   = Vector(undef,6)\nf,b  = [f_1,f_2,f_3,f_4,f_5,f_6],[b1,b2,b3,b4,b5,b6]\n\nfor i in 1:length(pl)\n    pl[i]   = plot( [q[:,i]],[pdp[:,i] f[i](q[:,i],b[i]) - f[i](q[:,i]*0,b[i])],\n           label = [\"HTB\" \"true\"],\n           legend = :bottomright,\n           linecolor = [:blue :red],\n           linestyle = [:solid :dot],\n\n           linewidth = [6 3],\n           titlefont = font(15),\n           legendfont = font(12),\n           xlabel = \"x\",\n           ylabel = \"f(x)\"\n           )\n           \nend\n\ndisplay(plot(pl[1],pl[2],pl[3],pl[4],pl[5],pl[6],layout=(3,2), size=(1300,800)))  \n","category":"page"},{"location":"tutorials/Faster_large_n/#Speeding-up-HTBoost-with-large-n","page":"-","title":"Speeding up HTBoost with large n","text":"","category":"section"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"HTBoost is very slow in comparison with other GBMs. Here we discuss some options to speed up training when n is large. ","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"If HTBoost predominantly chooses hard splits, consider switching to CatBoost","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"If preliminary analysis (e.g. on a subsample and/or with modality=:fastest) suggests that the average value of tau is high (higher than 15-20, see Basic use), HTBoost is effectively fitting symmetric trees with hard rather than smooth splits; CatBoost is then a much more efficient option to fit symmetric trees, if the other features of HTBoost (see index) are not required. For Julia and R users, EvoTrees can also build symmetric trees (tree_type = \"oblivious\"). ","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"Some options to speed up training for HTBoost with large n","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"HTBoost runs much faster (particularly with large n) with multiple cores than with one, after the initial one-off cost. The improvements in speed are roughly linear in the number of cores, up to 8 cores, and still good up to 16 cores, particularly when p/#cores is large. Gains after 16 cores are modest at best.  ","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"Option 1. modality = :fast, nfold = 1, nofullsample = true. ","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"The easiest way to speed up training is by setting nfold=1 (a single validation set), nofullsample=true, and modality=:fast or :fastest. These modalities do not perform cv.  :fast will typically still produces a competitive model in terms of accuracy, particularly if n/p is large. If nfold=1, setting nofullsample=true further reduces computing time by 60% at the cost of fitting the model on a smaller sample. modality = :fastest automatically sets nfold=1, nofullsample=true, and also lambda = 0.2 instead of 0.1. lambda = 0.2 can perform almost as well as 0.1 if the function is smooth and n/p is large.","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"See Option 4 for an alternative.","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"Option 2. Use a coarser grid for feature selection at deeper levels of the tree. (Can be combined with any value of modality) ","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"Examples of use: ","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"    param = HTBparam(depth_coarse_grid =4,depth_coarse_grid2=5,modality=:fast)\n    param = HTBparam(depth_coarse_grid =4,depth_coarse_grid2=5,modality=:compromise)","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"Replacing the defaults (5,7) with (4,5) may speed up computations by 25-33%, with no or little loss of fit in most cases. ","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"Option 3. Don't allow forcing sharp splits (in combination with modality=:fast). Warning: potential for decreased performance!","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"In situations where some features may require imposing sharp splits, the model is estimated twice. To avoid this, run ","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"output = SMARTfit(data,param,cv_hybrid=false)","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"(in combination with modality=:fast or :fastest) then cuts computing times in half. The loss of fit is modest in some cases, but can be substantial in others.","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"Option 4. Cross-validate on a sub-sample, then one run best model on full sample.","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"Setting modality = :fast fits the model at default parameters. If some cross-validation is desired,  the following strategy can be used to speed up cross-validation, typically with only small deterioration in performance if n is large. ","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"When n is very large, and it takes too long to fit HTBoost in modality = :compromise or :accurate, one way to proceed is to cv on a subsample of the data (say 20%) and then fit only one model on the full sample, using the best parameters found in the subsample, except for the number of trees. If the subsample is large enough, the best parameters found in the subsample will be close to the best parameters in the full sample. (Again, the number of trees is optimized on the full sample.) (Of course the subset is more noisy and will prefer simpler models, but the difference should be modest if n is large.)","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"This can be accomplished as follows:","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"Set modality=:compromise or :accurate, take a subsample of the data (20%), and run output=HTBfit() on that.\nSet param=output.bestparam, and then param.modality=:fast, and run HTBfit() on the full data.","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"An example is given below: ","category":"page"},{"location":"tutorials/Faster_large_n/","page":"-","title":"-","text":"number_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Statistics\n\n# USER'S OPTIONS \nRandom.seed!(123)\n\n# Options for data generation \nn         = 500_000\np         = 100         # number of features \ndummies   = true        # if true if x, x_test are 0-1 (much faster training).\nstde      = 1            \n\n# Options for HTBoost: modality is the key parameter guiding hyperparameter tuning and learning rate.\n# :fast only fits one model at default parameters, while :compromise and :accurate perform\n# automatic hyperparameter tuning. \n\nrandomsubset      = 0.2          # e.g. 0.2. Share of observations in the first sub-set \nmodality_subs     = :compromise  # :accurate or :compromise (default)\nmodality_full     = :fast        # :fast\n\nnfold_subs       = 1             # number of cv folds. 1 sufficient if the sub-sample is sufficiently large \nnfold_full       = 1         \nnofullsample_full = true         # if nfold=1 and nofullsample=true, the model is not re-fitted on the full sample after validation          \nrandomizecv       = false       # false (default) to use block-cv.\n\nverbose          = :Off\nwarnings         = :On\n\n# simple f(x), with pstar relevant features.\n\np_star    = 10       # number of relevant features \nβ         = randn(p_star)    # draw linear coefficients from a Gaussian distribution\ndgp(x)    = x[:,1:length(β)]*β\n\n# END USER'S INPUTS \n\nif dummies\n    x,x_test = randn(n,p),randn(200_000,p) \n    x,x_test = Float64.(x .> 0), Float64.(x_test .> 0)\nelse\n    x,x_test = randn(n,p), randn(200_000,p)    \nend     \n\ny       = dgp(x) + stde*randn(n)\nf_test  = dgp(x_test)\n\n# HTBoost on a sub-sample \nparam_subs   = HTBparam(modality=modality_subs,nfold=nfold_subs,nofullsample=true,randomizecv=randomizecv,\n                verbose=verbose,warnings=warnings)\ndata         = HTBdata(y,x,param_subs)\nn            = length(data.y)\n\nind       = randperm(n)[1:convert(Int,round(randomsubset*n))]\ndata_subs = HTBdata(y[ind],x[ind,:],param_subs)\n\noutput_subs = HTBfit(data_subs,param_subs) # performs cv on subset\n\n# HTBoost on full sample \nparam          = output_subs.bestparam        # sets param at best configuration in subset, then modify where appropriate\n\nparam.ntrees   = 2_000                        # number of trees should not be from subsample! Early stopping must be on full sample.\nparam.modality = modality_full      \nparam.nfold    = nfold_full\nparam.nofullsample = nofullsample_full\n\ndata  = HTBdata(y,x,param)\n\nprintln(\"\\n n = $n, p = $p, dummies=$dummies, modality = $(param.modality)\")\n\nprintln(\"\\n Time to train the full model.\")\n@time output = HTBfit(data,param);\n\nyf = HTBpredict(x_test,output,predict=:Ey)  # predict\nprintln(\"\\n RMSE of HTBoost from true E(y|x) \", sqrt(mean((yf-f_test).^2)) )\n","category":"page"},{"location":"tutorials/User_controlled_cv/#User's-controlled-cross-validation-of-parameters","page":"-","title":"User's controlled cross-validation of parameters","text":"","category":"section"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"Summary","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"HTBfit() already parsimoniously cv the most important hyperparameters if modality in [:compromise,:accurate], and stacks the results. This should be sufficient for the majority of applications. \nHTBcv() can be used to select additional (i.e. in addition to the internal cv in HTBfit) hyperparameters to cv. This should improve (or at least not hurt) accuracy in most cases but could be quite expensive.\nHTBcv() can also be used to replace HTBfit() and fully control the hyperparameter selection. (Not recommended.) ","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"Discussion","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"The recommended process for HTBoost is to use modality (:fast, :compromise, :accurate) in HTBfit() rather than HTBcv(). The various modalities in HTBfit() internally control the most important hyperparameters, being as parsimonious as possible due  to the high computational costs of HTB. HTBfit() also stacks the models, which is not done here. All modalities use early stopping to determine the number of trees, so ntrees should not be cv. ","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"The function HTBcv() is provided for users who want to cv additional parameters (not included in HTBfit) and can incur the computational costs. An example is provided below.","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"The function HTBcv() is also provided for advanced users who want to fully control the cross-validation process. In most cases HTBcv() will be less efficient than HTBfit(), and probably less accurate. ","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"Example of use: cv varlntau (strength of smoothness prior) in addition to the internal cv","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"Set up model and data, including aspects of cv such as the number of folds and whether block-cv or randomized.","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"\nparam  = HTBparam(loss=:L2,modality=:accurate,nfold=1,randomize=false)  \ndata   = HTBdata(x,y,param)\n","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"Specify hyperparameters and values for cv as Array{Dictionary}. Here we cv only varlntau (strength of smoothness prior). The resuls is a one-dimensional array. ","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"\nparams_cv = [Dict(\n    :varlntau => varlntau)\n    for\n    varlntau in (0.25^2,0.5^1,1.0)\n    ]\n\nhtbcv = HTBcv(data,param,params_cv)      # cv over dictionary AND internally\n    \n# Some info about the best model \nbestindex = htbcv.bestindex   # params_cv[bestindex] is for best set of cv hyperparameters       \nbestparam = htbcv.bestparam   \n\n# predict using best model\nyf    = HTBpredict(x_oos,htbcv.output)    \n","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"Example of use: fully control cv (not recommended for most users)","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"Specify Array{Dictionary} of hyperparameters to be cv.","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"\nparams_cv = [Dict(\n    :depth => depth,\n    :varlntau => varlntau)    \n    for\n    depth in (2,4,6),\n    varlntau in (0.25^2,0.5^1,1.0^2)\n    ]","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"Call htbc with internal_cv = false","category":"page"},{"location":"tutorials/User_controlled_cv/","page":"-","title":"-","text":"\nhtbcv = HTBcv(data,param,params_cv,internal_cv=false)       \n\n# Some info about the best model \nbestindex = htbcv.bestindex   # params_cv[bestindex] is for best set of cv hyperparameters       \nbestparam = htbcv.bestparam   \noutput    = htbcv.output\n\n# predict using best model\nyf    = HTBpredict(x_oos,htbcv.output)    \n","category":"page"},{"location":"tutorials/GammaPoisson/#Poisson-and-GammaPoisson-for-(possibly-overdisperesed)-count-data","page":"-","title":"Poisson and GammaPoisson for (possibly overdisperesed) count data","text":"","category":"section"},{"location":"tutorials/GammaPoisson/","page":"-","title":"-","text":"XGBoost, LightGBM and CatBoost offer the Poisson loss for count data. The Poisson distribution assumes that var(y|x) = E(y|x), a restriction that is often violated in practice. Besides loss = :Poisson, HTBoost proposes the more general :gammaPoisson (aka negative binomial), a popular generalization of the Poisson in statistics. The gamma Poisson introduces an additional parameter α (the 'overdispersion' parameter), where now E(y)=μ, var(y)=μ(1+αμ). The Poisson is recovered for  α -> 0. ","category":"page"},{"location":"tutorials/GammaPoisson/","page":"-","title":"-","text":"HTBoost estimates α internally by maximum likelihood after each tree. The final value of α can be recovered by HTBcoeff( ) as follows:","category":"page"},{"location":"tutorials/GammaPoisson/","page":"-","title":"-","text":"\nparam  = HTBparam(loss=:gammaPoisson)\ndata   = HTBdata(y,x,param)\noutput = HTBfit(data,param)\ncoeff  = HTBcoeff(output,verbose=false)\n","category":"page"},{"location":"tutorials/GammaPoisson/","page":"-","title":"-","text":"See gammaPoisson for a fully worked-out example, including a comparison with LightGBM (with Poisson loss).   In that example, the data is generated by a gamma Poisson (negative binomial), and HTBoost provides a consistent estimate of both α and E(y|x). ","category":"page"},{"location":"examples/Gamma/#Gamma-loss","page":"Gamma loss","title":"Gamma loss","text":"","category":"section"},{"location":"examples/Gamma/","page":"Gamma loss","title":"Gamma loss","text":"Short description:","category":"page"},{"location":"examples/Gamma/","page":"Gamma loss","title":"Gamma loss","text":"HTBoost with gamma distribution on simulated data (generated by a gamma distribution).\nThe shape parameter k is estimated internally.\nloss=:gamma clearly outperforms loss=:L2 in terms of out-of-sample RMSE,  but :L2loglink approaches the performance of :gamma and is more robust if the density is not gamma. (See examples/L2loglink.jl for more on :L2loglink.)","category":"page"},{"location":"examples/Gamma/","page":"Gamma loss","title":"Gamma loss","text":"Note: The comparison with LightGBM is biased toward HTBoost if the function generating the data is  smooth in some features (this is easily changed by the user). lightGBM is cross-validated over maxdepth and numleaves, with the number of trees set to 1000 and found by early stopping.","category":"page"},{"location":"examples/Gamma/","page":"Gamma loss","title":"Gamma loss","text":"\nnumber_workers  = 8  # desired number of workers\n\nusing Distributed\nnprocs()<number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)\n@everywhere using HybridTreeBoosting\n\nusing Random,Plots,Distributions \nusing LightGBM\n\n# USER'S OPTIONS \n\nRandom.seed!(2)\n\n# Some options for HTBoost\nloss      = :gamma         \nmodality  = :compromise   # :accurate, :compromise (default), :fast, :fastest \n\npriortype = :hybrid       # :hybrid (default) or :smooth to force smoothness \nnfold     = 1             # number of cv folds. 1 faster (single validation sets), default 5 is slower, but more accurate.\nnofullsample = true       # if nfold=1 and nofullsample=true, the model is not re-fitted on the full sample after validation of the number of trees\n \nrandomizecv = false       # false (default) to use block-cv. \nverbose     = :Off\nwarnings    = :On\n\n# options to generate data.\nk           = 5         # shape parameter of the gamma distribution \nn,p,n_test  = 10_000,4,100_000   # sample size and number of features\n\nf_1(x,b)    = b*x  \nf_2(x,b)    = -b*(x.<0.5) + b*(x.>=0.5)   \nf_3(x,b)    = b*x\nf_4(x,b)    = -b*(x.<0.5) + b*(x.>=0.5)\n\nb1,b2,b3,b4 = 0.2,0.2,0.2,0.2\n\n# generate data\nx,x_test = randn(n,p), randn(n_test,p)\n\nc        = -2  \nf        = c .+ f_1(x[:,1],b1) + f_2(x[:,2],b2) + f_3(x[:,3],b3) + f_4(x[:,4],b4)\nf_test   = c .+ f_1(x_test[:,1],b1) + f_2(x_test[:,2],b2) + f_3(x_test[:,3],b3) + f_4(x_test[:,4],b4)\n\nμ        = exp.(f)        # conditional mean \nμ_test   = exp.(f_test)   # conditional mean \n\nscale      = μ/k\nscale_test = μ_test/k\ny          = zeros(n)\n\nfor i in eachindex(y)\n    y[i]  = rand(Gamma.(k,scale[i]))\nend \n\nhistogram(y)\n@show [mean(y), std(y), std(μ), maximum(y)]\n\n# set up HTBparam and HTBdata, then fit and predit\n\n# coefficient estimated internally. \nparam  = HTBparam(loss=loss,priortype=priortype,randomizecv=randomizecv,nfold=nfold,\n                   verbose=verbose,warnings=warnings,modality=modality,nofullsample=nofullsample)\ndata   = HTBdata(y,x,param)\n\noutput = HTBfit(data,param)\nyf     = HTBpredict(x_test,output,predict=:Ey)\n\nprintln(\" \\n loss = $loss, modality = $(param.modality), nfold = $nfold \")\nprintln(\" depth = $(output.bestvalue), number of trees = $(output.ntrees) \")\nprintln(\" out-of-sample RMSE from truth, μ     \", sqrt(sum((yf - μ_test).^2)/n_test) )\n\nprintln(\"\\n true shape = $k, estimated = $(exp(output.bestparam.coeff_updated[1][1])) \")\nprintln(\"\\n For more information about coefficients, use HTBcoeff(output) \")\nHTBcoeff(output)\n\n\n# Repeat for :L2 or :L2loglink loss\n\nparam_L2 = deepcopy(param) \nparam_L2.loss = :L2loglink         # :L2loglink should outperform :L2 with these data\noutput_L2 = HTBfit(data,param_L2)\nyf    = HTBpredict(x_test,output_L2,predict=:Ey)  \n\nprintln(\" \\n loss = $(param_L2.loss), modality = $(param.modality), nfold = $nfold\")\nprintln(\" depth = $(output_L2.bestvalue), number of trees = $(output_L2.ntrees) \")\nprintln(\" out-of-sample RMSE from truth, μ      \", sqrt(sum((yf - μ_test).^2)/n_test) )\n\n# lightGBM\n\nestimator = LGBMRegression(\n    objective = \"gamma\",\n    metric = [\"gamma\"],        # default seems (strangely) \"l2\" regardless of objective: LightGBM.jl bug?  \n    num_iterations = 1000,\n    learning_rate = 0.1,\n    early_stopping_round = 100,\n    num_threads = number_workers,\n    max_depth = 6,      # -1 default\n    min_data_in_leaf = 100,  # 100 default \n    num_leaves = 127         # 127 default  \n)\n\n# Fit lightGBM \n\nn_train = Int(round((1-param.sharevalidation)*length(y)))\nx_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])\nx_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])\n    \n# fit at parameters given by estimator, no cv\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\n    \nyf_gbm_default = LightGBM.predict(estimator,x_test)[:,1]\n\n# parameter search over num_leaves and max_depth\nsplits = (collect(1:n_train),collect(1:min(n_train,100)))  # goes around the problem that at least two training sets are required by search_cv (we want the first)\n\nparams = [Dict(:num_leaves => num_leaves,\n               :max_depth => max_depth) for\n          num_leaves in (4,16,32,64,127,256),\n          max_depth in (2,3,5,6,8)]\n\nlightcv = LightGBM.search_cv(estimator,x,y,splits,params,verbosity=-1)\n\nloss_cv = [lightcv[i][2][\"validation\"][estimator.metric[1]][1] for i in eachindex(lightcv)]\nminind = argmin(loss_cv)\n\nestimator.num_leaves = lightcv[minind][1][:num_leaves]\nestimator.max_depth  = lightcv[minind][1][:max_depth]\n\n# re-fit at cv parameters\nLightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)\n    \nyf_gbm = LightGBM.predict(estimator,x_test)\nyf_gbm = yf_gbm[:,1]    # drop the second dimension or a (n_test,1) matrix \n\nprintln(\"\\n oss RMSE from truth, μ, LightGBM default \", sqrt(sum((yf_gbm_default - μ_test).^2)/n_test) )\nprintln(\" oss RMSE from truth, μ, LightGBM cv      \", sqrt(sum((yf_gbm - μ_test).^2)/n_test) )\n\n# Plot \n\nq,pdp  = HTBpartialplot(data,output,[1,2,3,4],predict=:Egamma)\n\n# plot partial dependence in terms of the natural parameter \npl   = Vector(undef,4)\nf,b  = [f_1,f_2,f_3,f_4],[b1,b2,b3,b4]\n\nfor i in 1:length(pl)\n        pl[i]   = plot( [q[:,i]],[pdp[:,i] f[i](q[:,i],b[i]) - f[i](q[:,i]*0,b[i])],\n           label = [\"HTB\" \"dgp\"],\n           legend = :bottomright,\n           linecolor = [:blue :red],\n           linestyle = [:solid :dot],\n\n           linewidth = [5 5],\n           titlefont = font(15),\n           legendfont = font(12),\n           xlabel = \"x\",\n           ylabel = \"f(x)\",\n           )\nend\n\ndisplay(plot(pl[1], pl[2], pl[3], pl[4], layout=(2,2), size=(1300,800)))  # display() will show it in Plots window.\n","category":"page"},{"location":"tutorials_py/Basic_use/#Basic-use","page":"plot partial dependence","title":"Basic use","text":"","category":"section"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Summary","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Illustrates use of the main functions of HTBoost on a regression problem with simulated data. \nparam.modality as the most important user's choice, depending on time budget. \nIn default modality, HTBoost performs automatic hyperparameter tuning.\nThe Python bindings use juliacall. Please read Installation in Python.\nOnly a few tutorials are provided for Python. For more tutorials and examples, see Julia tutorials and Julia examples, using the guidelines in Installation in Python to adapt the code to Python. ","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Main points ","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"default loss is \"L2\". Other options for continuous y are \"Huber\", \"t\" (recommended in place of \"Huber\"), \"gamma\", \"gammaPoisson\", \"L2loglink\". For zero-inflated continuous y, options are \"hurdleGamma\", \"hurdleL2loglink\", \"hurdleL2\"   \ndefault is block cross-validation with nfolds=4: use randomizecv = TRUE to scramble the data. See Global Equity Panel for further options on cross-validation (e.g. sequential cv, or generally controlling the training and validation sets).","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"fit, with automatic hyperparameter tuning if modality is :compromise or :accurate\nsave fitted model (upload fitted model)\naverage τ (smoothness parameter), which is also plotted. (Smoother functions ==> larger gains compared to other GBM)\nfeature importance\npartial effects plots","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"","category":"page"},{"location":"tutorials_py/Basic_use/#Install-and-load-required-packages.","page":"plot partial dependence","title":"Install and load required packages.","text":"","category":"section"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Follow steps 1-3 in Installation in Python.","category":"page"},{"location":"tutorials_py/Basic_use/#Generate-data.","page":"plot partial dependence","title":"Generate data.","text":"","category":"section"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"y is the sum of six additive nonlinear functions, plus Gaussian noise.","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"import numpy as np\n\n# Define sample size\nn = 10000\np = 6\nstde = 1.0\nn_test = 100000\n\n# Define functions\ndef f_1(x, b):\n    return b * x + 1\n\ndef f_2(x, b):\n    return 2 * np.sin(2.5 * b * x)\n\ndef f_3(x, b):\n    return b * x**3\n\ndef f_4(x, b):\n    return b * (x < 0.5)\n\ndef f_5(x, b):\n    return b / (1.0 + np.exp(4.0 * x))\n\ndef f_6(x, b):\n    return b * ((x > -0.25) & (x < 0.25))\n\n# Define coefficients\nb1 = 1.5\nb2 = 2.0\nb3 = 0.5\nb4 = 4.0\nb5 = 5.0\nb6 = 5.0\n\n# Generate random data\nx = np.random.normal(0, 1, (n, p))\nx_test = np.random.normal(0, 1, (n_test, p))\n\n# Compute f and f_test\nf = f_1(x[:, 0], b1) + f_2(x[:, 1], b2) + f_3(x[:, 2], b3) + f_4(x[:, 3], b4) + f_5(x[:, 4], b5) + f_6(x[:, 5], b6)\nf_test = f_1(x_test[:, 0], b1) + f_2(x_test[:, 1], b2) + f_3(x_test[:, 2], b3) + f_4(x_test[:, 3], b4) + f_5(x_test[:, 4], b5) + f_6(x_test[:, 5], b6)\n\n# Generate y with noise\ny = f + np.random.normal(0, stde, n)\n\n# Define feature names\nfnames = [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\"]\n","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"When y and x are numerical matrices with no missing values, we could feed them to HTBoost directly. However, a more general procedue (which allows strings and missing values), is to transform the data into a dataframe and then into a Julia DataFrame. This is done as follows (here only for x, but the same applies to y if it is not numerical).","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"\nimport pandas as pd\n\n# Create a Pandas dataframe\ndf = pd.DataFrame(x)\ndf_test = pd.DataFrame(x_test)\n\n# Define feature names\nfnames = df.columns.tolist()\n\n# Convert Pandas dataframe to Julia DataFrame\njl.seval(\"using DataFrames\")\nx = jl.DataFrame(df)\nx_test = jl.DataFrame(df_test)\n\n# Describe the Julia DataFrame\njl.seval(\"describe\")(x)","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Options for HTBparam( ).  ","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"I prefer to specify parameter settings separately (here at the top of the script) rather than directly in HTBparam( ), which is of course also possible.   modality is the key parameter: automatic hyperparameter tuning if modality is \"compromise\" or \"accurate\", no tuning (except of #trees) if \"fast\" or \"fastest\".     In HTBoost, it is not recommended that the user performs  hyperparameter tuning by cross-validation, because this process is done automatically if modality is \"compromise\" or \"accurate\". The recommended process is to first run in modality=\"fast\" or \"fastest\", for exploratory analysis and to gauge computing time, and then switch to \"compromise\" (default) or \"accurate\". For a tutorial on user-controlled cross-validation, see User's controlled cross-validation.","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"\nloss      = \"L2\"        # :L2 is default. Other options for regression are :L2loglink (if y≥0), :t, :Huber\nmodality  = \"fast\"      # \"accurate\", \"compromise\" (default), \"fast\", \"fastest\". The first two perform parameter tuning internally by cv.\npriortype = \"hybrid\"    # \"hybrid\" (default) or \"smooth\" to force smoothness \nnfold     = 1           # number of cv folds. 1 faster (single validation sets), default 4 is slower, but more accurate.\nnofullsample = True     # if nfold=1 and nofullsample=TRUE, the model is not re-fitted on the full sample after validation of the number of trees\nrandomizecv = False     # FALSE (default) to use block-cv. \nverbose     = \"Off\"","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Options for cross-validation:","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"While the default in other GBM is to randomize the allocation to train and validation sets, the default in HTBoost is block cv, which is suitable for time series and panels. Set randomizecv=True to bypass this default.  See Timeseriesand_panels for further options on cross-validation (e.g. sequential cv, or generally controlling the training and validation sets).","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"randomizecv = False       # False (default) to use block-cv. ","category":"page"},{"location":"tutorials_py/Basic_use/#Set-up-HTBparam-and-HTBdata,-then-fit.-Optionally,-save-the-model-(or-load-it).-Predict.-Print-some-information.","page":"plot partial dependence","title":"Set up HTBparam and HTBdata, then fit. Optionally, save the model (or load it). Predict. Print some information.","text":"","category":"section"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"\nparam = jl.HTBparam(loss=loss,priortype=priortype,randomizecv=randomizecv,nfold=nfold,verbose=verbose,modality=modality,nofullsample=nofullsample)\n\ndata   = jl.HTBdata(y,x,param)   \noutput = jl.HTBfit(data,param)\n\nyhat = jl.HTBpredict(x, output)      # Fitted values\nyf   = jl.HTBpredict(x_test, output)   # Predicted values\n\n# Print information\nprint(f\"out-of-sample RMSE from truth = {np.sqrt(np.sum((yf - f_test)**2) / n_test)\")\n","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"which prints","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"out-of-sample RMSE from truth 0.3563147\n","category":"page"},{"location":"tutorials_py/Basic_use/#Feature-importance-and-average-smoothing-parameter-for-each-feature.","page":"plot partial dependence","title":"Feature importance and average smoothing parameter for each feature.","text":"","category":"section"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"tuple = jl.HTBweightedtau(output,data,verbose=True)  # output is a named tuple which ","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":" Row │ feature  importance  avgtau     sorted_feature  sorted_importance  sorted_avgtau \n     │ String   Float32     Float64    String          Float32            Float64\n─────┼──────────────────────────────────────────────────────────────────────────────────\n   1 │ x1          14.5666   0.458996  x3                        19.0633       3.30638\n   2 │ x2          12.9643  19.6719    x5                        18.6942       3.72146\n   3 │ x3          19.0633   3.30638   x6                        17.9862      35.1852\n   4 │ x4          16.7254  36.0846    x4                        16.7254      36.0846\n   5 │ x5          18.6942   3.72146   x1                        14.5666       0.458996\n   6 │ x6          17.9862  35.1852    x2                        12.9643      19.6719\n\n Average smoothing parameter τ is 7.3.\n\n In sufficiently large samples, and if modality=:compromise or :accurate\n\n - Values above 20-25 suggest very little smoothness in important features. HTBoost's performance may slightly outperform or slightly underperform other gradient boosting machines.\n - At 10-15 or lower, HTBoost should outperform other gradient boosting machines, or at least be worth including in an ensemble.\n - At 5-7 or lower, HTBoost should strongly outperform other gradient boosting machines.","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"In this case smoothness is a mix of very different values across features: approximate linearity for x1, smooth functions for x3 and x5, and essentially sharp splits for x2, x4, and x6. Note: Variable (feature) importance is computed as in Hastie et al., \"The Elements of Statistical Learning\", second edition, except that the normalization is for sum=100.  ","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Some examples of smoothness corresponding to a few values of tau (for a single split) help to interpret values of avgtau","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"<img src=\"../assets/Sigmoids.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"On simulated data, we can evaluate the RMSE from the True f(x), exluding noise:","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Hybrid trees outperform both smooth and standard trees","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Here is the output for n=10k (nfold=1, nofullsample=True).  Hybrid trees strongly outperform both smooth trees and standard symmetric (aka oblivious) trees. (Note: modality = :sharp is a very inefficient way to run a symmetric tree; use CatBoost or EvoTrees instead!)","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"  modality = fastest, nfold = 1, priortype = hybrid\n depth = 5, number of trees = 141, gavgtau 7.3\n out-of-sample RMSE from truth 0.3136\n\nmodality = fastest, nfold = 1, priortype = smooth \n depth = 5, number of trees = 121, gavgtau 4.5\n out-of-sample RMSE from truth 0.5751\n\n modality = fastest, priortype = sharp\ndepth = 5, number of trees = 183, avgtau 40.0\n out-of-sample RMSE from truth 0.5320","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Partial dependence plots","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Partial dependence assumes (in default) that other features are kept at their mean.","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"fnames,fi,fnames_sorted,fi_sorted,sortedindx = jl.HTBrelevance(output,data,verbose=False);\nq,pdp  = HTBpartialplot(data,output,[1,2,3,4,5,6]) # partial effects for the first 6 variables \n","category":"page"},{"location":"tutorials_py/Basic_use/#plot-partial-dependence","page":"plot partial dependence","title":"plot partial dependence","text":"","category":"section"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"q,pdp = jl.HTBpartialplot(data,output,[1,2,3,4,5,6])","category":"page"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"Here q and pdp are (npoints,6) matrices. Plotting each column (each feature) for different values of n.  Partial plots for n = 1k,10k,100k, with modality = :fastest and nfold = 1.    Notice how plots are smooth only for some features. ","category":"page"},{"location":"tutorials_py/Basic_use/#n-1_000","page":"plot partial dependence","title":"n = 1_000","text":"","category":"section"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"<img src=\"../assets/Minimal1k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"tutorials_py/Basic_use/#n-10_000","page":"plot partial dependence","title":"n = 10_000","text":"","category":"section"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"<img src=\"../assets/Minimal10k.png\" width=\"600\" height=\"400\">","category":"page"},{"location":"tutorials_py/Basic_use/#n-100_000","page":"plot partial dependence","title":"n = 100_000","text":"","category":"section"},{"location":"tutorials_py/Basic_use/","page":"plot partial dependence","title":"plot partial dependence","text":"<img src=\"../assets/Minimal100k.png\" width=\"600\" height=\"400\">","category":"page"}]
}

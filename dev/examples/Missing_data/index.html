<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Internal handling of missing data in HTBoost · HybridTreeBoosting.jl</title><meta name="title" content="Internal handling of missing data in HTBoost · HybridTreeBoosting.jl"/><meta property="og:title" content="Internal handling of missing data in HTBoost · HybridTreeBoosting.jl"/><meta property="twitter:title" content="Internal handling of missing data in HTBoost · HybridTreeBoosting.jl"/><meta name="description" content="Documentation for HybridTreeBoosting.jl."/><meta property="og:description" content="Documentation for HybridTreeBoosting.jl."/><meta property="twitter:description" content="Documentation for HybridTreeBoosting.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">HybridTreeBoosting.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Introduction</a></li><li><a class="tocitem" href="../../Parameters/">Parameters</a></li><li><a class="tocitem" href="../../JuliaAPI/">API</a></li><li><a class="tocitem" href="../../Tutorials/">Tutorials (Julia)</a></li><li><a class="tocitem" href="../../Examples/">Examples (Julia)</a></li><li><a class="tocitem" href="../../Tutorials_R/">Tutorials (R)</a></li><li><a class="tocitem" href="../../Tutorials_py/">Tutorials (Python)</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Internal handling of missing data in HTBoost</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Internal handling of missing data in HTBoost</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Internal-handling-of-missing-data-in-HTBoost"><a class="docs-heading-anchor" href="#Internal-handling-of-missing-data-in-HTBoost">Internal handling of missing data in HTBoost</a><a id="Internal-handling-of-missing-data-in-HTBoost-1"></a><a class="docs-heading-anchor-permalink" href="#Internal-handling-of-missing-data-in-HTBoost" title="Permalink"></a></h1><p><strong>HTBoost handles missing values automatically. Imputation is optional.</strong> </p><p>This example reproduces the set-up in the simulations i) Experiment 1 and ii) Model 2 and Model 3 in Experiment 2 in the paper: &quot;On the consistency of supervised learning with missing values&quot; by Josse et al., 2020.  Data can be missing at random, or missing not at random as a function of x only, or missing not at random as a function of E(y).</p><ul><li>The approach to missing values in HTBoost is Block Propagation (see Josse et al.).</li><li>There is a however a a key difference compared to lightGBM and other GBM when the split is soft (τ &lt; Inf): the value m at which to set all missing is estimated/optimized at each node. With standard trees (sharp splits), it only matters whether missing are sent to the left or right branch, but in HTBoost the allocation of missing values is also smooth (as long as the split is smooth), and the proportion to which missings are sent left AND right is decided by a new split parameter m, distinct from μ.  The result is more efficient inference with missing values. When the split is sharp or the feature takes only two values, HTBoost assigns missing in the same way as LigthGBM (by Block Propagation).</li><li>The native procedure to handle missing values is Bayes consistent (see Josse et al.), i.e. efficient in large samples, and is convenient in that it can handle data of mixed types (continuous, discrete, categorical). When feasible, a good imputation of missing values + mask (see Josse et al. ) can perform better, particularly in small samples, high predictability of missing values from non-missing values, linear or quasi-linear f(x), and missing at random (in line with the results of Josse et al.)  </li></ul><p>The comparison with LightGBM is biased toward HTBoost if the function generating the data is smooth in some features. LightGBM is cross-validated over max<em>depth and num</em>leaves, with the number of trees set to 1000 and found by early stopping.</p><pre><code class="language-julia hljs">
number_workers  = 8  # desired number of workers

using Distributed
nprocs()&lt;number_workers ? addprocs( number_workers - nprocs()  ) : addprocs(0)
@everywhere using HybridTreeBoosting

using DataFrames, Random, Statistics
using LinearAlgebra,Plots, Distributions
using LightGBM

Random.seed!(1)

# Options to generate data. y is the of four additive nonlinear functions + Gaussian noise(0,stde^2)

Experiment      = &quot;1&quot;     # &quot;1&quot; or &quot;2 Friedman&quot; or &quot;2 Linear&quot;  (see Josse et al., 2020, experiment 1, and experiment 2, Friedman or Linear dgp)
missing_pattern = 1       # Only relevant for Experiment = &quot;1&quot;. 1 for MCAR (missing at random), 2 for Censoring MNAR (at 1), 3 for Predictive missingness 

n,p,n_test  = 10_000,10,100_000  # n=1000, p= 9 or 10 in paper. Since this is one run, consider larger n. 
stde        = 0.1                # 0.1 in paper
ρ           = 0.5                # 0.5 in paper, cross-correlation of features 

# Some options for HTBoost
priortype = :hybrid        # :hybrid (default) or :smooth or :sharp
modality  = :compromise    # :accurate, :compromise, :fast, :fastest 

nfold           = 1        # nfold cv. 1 faster (single validation sets), default 4 is slower, but more accurate. Here nfold = 1 for fair comparison with LightGBM.

plot_results = false
mask_missing = false     # default = false. True to introduce an additional feature, a dummy with value &#39;true&#39; if x is missing. Not necessary.

# END USER&#39;S OPTIONS

# generate data

# Missing at random (MCAR)
function model1_missingpattern1(x) 

    prob = 0.2  # probability of miss. 0.2 in their experiments 
    α,β = 1,2  # α,β = 1,2 in their experiments
    i   = 1    # i=1 in their experiments. Which feature is x^2 (miss is for 1st)
    ind = rand(size(x,1)) .&lt; prob
    f   = α*x[:,i].^β  
    x[ind,1] .= NaN

    return f,x
end 


function model1_missingpattern2(x) 

    prob = 0.2                    # probability of miss. 0.2 in their experiments 
    q  = quantile(x[:,1],1-prob)
    α,β = 1,2  # α,β = 1,2 in their experiments
    i   = 1    # i=1 in their experiments. Which feature is x^2 (miss is for 1st)
    ind = x[:,1] .&lt; q
    f   = α*x[:,i].^β  
    x[ind,1] .= NaN

    return f,x
end 



function model1_missingpattern3(x) 

    prob = 0.2   # probability of miss. 0.2 in their experiments 
    α,β = 1,2  # α,β = 1,2 in their experiments
    i   = 1    # i=1 in their experiments. Which feature is x^2 (miss is for 1st)
    ind = rand(size(x,1)) .&lt; prob
    f   = α*x[:,i].^β  + 3*ind 
    x[ind,1] .= NaN

    return f,x
end     


function Friedman(x) 

    prob = 0.2   # probability of miss. 0.2 in their experiments 
    n,p  = size(x)
    f    = @. 10*sin(π*x[:,1]*x[:,2]) + 20*(x[:,3] - 0.5)^2 + 10*x[:,4] + 5*x[:,5]
    MissData = rand(n,p) .&lt; prob
    x[MissData] .= NaN

    return f,x
end     



function Linear(x) 

    prob = 0.2   # probability of miss. 0.2 in their experiments 
    n,p  = size(x)
    f    =  x[:,1] + 2*x[:,2] - x[:,3] + 3*x[:,4] - 0.5*x[:,5] - x[:,6] + 0.3*x[:,7] + 1.7*x[:,8] 
            + 0.4*x[:,9] - 0.3*x[:,10] 
    MissData = rand(n,p) .&lt; prob
    x[MissData] .= NaN

    return f,x
end     



function missing_function(missing_pattern)

    if missing_pattern==1
        return model1_missingpattern1
    end 
    
    if missing_pattern==2
        return model1_missingpattern2
    end 

    if missing_pattern==3
        return model1_missingpattern3
    end 

end 


if Experiment == &quot;1&quot;
    f_pattern = missing_function(missing_pattern)
elseif Experiment == &quot;2 Friedman&quot; 
    f_pattern = Friedman
elseif Experiment == &quot;2 Linear&quot; 
    f_pattern = Linear
else 
    @error &quot;Experiment misspelled.&quot;    
end 


#cross-correlated data
u = ones(p)
μ = ones(p)
V = ρ*(u*u&#39;) + (1-ρ)*I
d = Distributions.MvNormal(μ,V)
x = copy(rand(d,n)&#39;)            # copy() because LightGBM does not copy with Adjoint type  
x_test = copy(rand(d,n_test)&#39;)

f,x            = f_pattern(x)
f_test,x_test  = f_pattern(x_test) 

y      = f + stde*randn(n)
y_test = f_test + stde*randn(n_test)

# set up HTBparam and HTBdata, then fit and predit
param  = HTBparam(priortype=priortype,randomizecv=true,nfold=nfold,modality=modality )
data   = HTBdata(y,x,param)

output = HTBfit(data,param)
yf     = HTBpredict(x_test,output)  # predict

yf  = HTBpredict(x_test,output)  # predict

# Evaluate predictions at a few points
println(&quot;\n E(y|x) with x1 = 1 or missing, and x2 = 0 or 1&quot;) 

for ov in [0.0,1.0]
    x_t    = fill(ov,size(x,2),p)
    x_t[1,1] = 1.0
    yf1     = HTBpredict(x_t,output)  # predict
    x_t[1,1] = NaN
    yf2     = HTBpredict(x_t,output)  # predict

    println(&quot; prediction at x1 = 1 and at x = miss, other variables at $ov &quot;, [yf1[1],yf2[1]])
end 

# LigthGBM 

estimator = LGBMRegression(
    objective = &quot;regression&quot;,
    metric = [&quot;l2&quot;],         
    num_iterations = 1000,
    learning_rate = 0.1,
    early_stopping_round = 100,
    num_threads = number_workers
    )

n_train = Int(round((1-param.sharevalidation)*length(y)))
x_train = x[1:n_train,:]; y_train = Float64.(y[1:n_train])
x_val   = x[n_train+1:end,:]; y_val = Float64.(y[n_train+1:end])

# parameter search over num_leaves and max_depth
splits = (collect(1:n_train),collect(1:min(n_train,100)))  # goes around the problem that at least two training sets are required by search_cv (we want the first)

params = [Dict(:num_leaves =&gt; num_leaves,
           :max_depth =&gt; max_depth) for
      num_leaves in (4,16,32,64,127,256),
      max_depth in (2,3,5,6,8)]

lightcv = LightGBM.search_cv(estimator,x,y,splits,params,verbosity=-1)

loss_cv = [lightcv[i][2][&quot;validation&quot;][estimator.metric[1]][1] for i in eachindex(lightcv)]
minind = argmin(loss_cv)

estimator.num_leaves = lightcv[minind][1][:num_leaves]
estimator.max_depth  = lightcv[minind][1][:max_depth]

# fit at cv parameters
LightGBM.fit!(estimator,x_train,y_train,(x_val,y_val),verbosity=-1)
yf_gbm = LightGBM.predict(estimator,x_test)   # (n_test,num_class) 

println(&quot;\n Experiment = $Experiment, missing_pattern = $missing_pattern, n = $n&quot;)
println(&quot;\n out-of-sample RMSE from truth, HTBoost, modality=:modality  &quot;, sqrt(sum((yf - f_test).^2)/n_test) )
println(&quot; out-of-sample RMSE from truth, LigthGBM cv                     &quot;, sqrt(sum((yf_gbm - f_test).^2)/n_test) )
</code></pre></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Monday 24 February 2025 15:20">Monday 24 February 2025</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>

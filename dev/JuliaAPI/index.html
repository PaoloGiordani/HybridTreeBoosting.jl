<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · HybridTreeBoosting.jl</title><meta name="title" content="API · HybridTreeBoosting.jl"/><meta property="og:title" content="API · HybridTreeBoosting.jl"/><meta property="twitter:title" content="API · HybridTreeBoosting.jl"/><meta name="description" content="Documentation for HybridTreeBoosting.jl."/><meta property="og:description" content="Documentation for HybridTreeBoosting.jl."/><meta property="twitter:description" content="Documentation for HybridTreeBoosting.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">HybridTreeBoosting.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../Parameters/">Parameters</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#Setting-up-the-model"><span>Setting up the model</span></a></li><li><a class="tocitem" href="#Fit-and-predict"><span>Fit and predict</span></a></li><li><a class="tocitem" href="#Post-estimation-analysis"><span>Post-estimation analysis</span></a></li></ul></li><li><a class="tocitem" href="../Tutorials/">Tutorials (Julia)</a></li><li><a class="tocitem" href="../Examples/">Examples (Julia)</a></li><li><a class="tocitem" href="../Tutorials_R/">Tutorials (R)</a></li><li><a class="tocitem" href="../Tutorials_py/">Tutorials (Python)</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API"><a class="docs-heading-anchor" href="#API">API</a><a id="API-1"></a><a class="docs-heading-anchor-permalink" href="#API" title="Permalink"></a></h1><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBinfo" href="#HybridTreeBoosting.HTBinfo"><code>HybridTreeBoosting.HTBinfo</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBinfo()</code></pre><p>Basic information about the main functions in HTBoost (see help on each function for more details)</p><p><strong>Setting up the model</strong></p><ul><li><code>HTBindexes_from_dates</code> builds train and test sets indexes for expanding window cross-validation (if the user wants to over-ride the default block-cv)</li><li><code>HTBparam</code>           parameters, defaults or user-provided.</li><li><code>HTBdata</code>            y,x and, optionally, dates, weights, and names of features</li></ul><p><strong>Fitting and forecasting</strong></p><ul><li><code>HTBfit</code>             fits HTBoost with cv (or validation/early stopping) of number of trees and, optionally, other parameters;                      pre-determined options for more or less extensive cross-validation are controlled by :modality in HTBparam</li><li><code>HTBpredict</code>         predictions for y or natural parameter</li><li><code>HTBcv</code>              user-controlled cross-validation. (Note that the recommended process is to use :modality in HTBfit rather than HTBcv.)            </li></ul><p><strong>POST-ESTIMATION ANALYSIS</strong></p><ul><li><code>HTBcoeff</code>           provides information on constant coefficients, e.g. dispersion and dof for loss=:t</li><li><code>HTBrelevance</code>       computes feature importance (Breiman et al 1984 relevance)</li><li><code>HTBoutput</code>          collects fitted parameters in matrices</li><li><code>HTBweightedtau</code>     computes weighted smoothing parameter to help assess function smoothness</li><li><code>HTBpartialplot</code>     partial dependence plots (keeping all other features fixed, not integrating out)</li><li><code>HTBmarginaleffect</code>  Numerical computation of marginal effects.</li><li><code>HTBplot_tau</code>        plotting sigmoid for a given tau </li><li><code>HTBplot_ppr</code>        provides tau and plot for projection pursuit for a single tree</li></ul><p>Example of use of info function </p><pre><code class="nohighlight hljs">help?&gt; HTBinfo
or ...
julia&gt; HTBinfo()</code></pre><p>To find more information about a specific function, e.g. HTBfit</p><pre><code class="nohighlight hljs">help?&gt; HTBfit</code></pre><p>Example of basic use of HTBoost functions with iid data and default settings</p><pre><code class="nohighlight hljs">param  = HTBparam()                                 
data   = HTBdata(y,x,param)
output = HTBfit(data,param)
yf     = HTBpredict(x_test,output)</code></pre><p>See the examples and tutorials for illustrations of use. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><h2 id="Setting-up-the-model"><a class="docs-heading-anchor" href="#Setting-up-the-model">Setting up the model</a><a id="Setting-up-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-up-the-model" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBparam" href="#HybridTreeBoosting.HTBparam"><code>HybridTreeBoosting.HTBparam</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBparam(;)</code></pre><p>Parameters for HTBoost</p><p>Note: all Julia symbols can be replaced by strings. e.g. :L2 can be replaced by &quot;L2&quot;.</p><p><strong>Parameters that are most likely to be modified by user (all inputs are keywords with default values)</strong></p><ul><li><p><code>loss</code>             [:L2] Supported distributions:</p><ul><li>:L2 (Gaussian), aliases :l2,:mse,:Gaussian,:normal</li><li>:logistic, aliases :binary (binary classification)</li><li>:multiclass (multiclass classification)</li><li>:t, aliases :student (student-t, robust alternative to :L2)</li><li>:Huber, aliases :huber </li><li>:gamma</li><li>:lognormal, aliases :logL2, :logl2 (positive continuous data) </li><li>:Poisson (count data)</li><li>:gammaPoisson, aliases :gamma<em>Poisson,:gamma</em>poisson,:negbin,:negative_binomial (aka negative binomial, count data)</li><li>:L2loglink, aliases :l2loglink (alternative to :L2 if y≥0)</li><li>:hurdleGamma (zero-inflated y)</li><li>:hurdleL2loglink (zero-inflated y)</li><li>:hurdleL2 (zero-inflated y)</li></ul></li><li><p>See the examples for uses of each loss function. Fixed coefficients (such as shape for :gamma, dispersion and dof for :t, and overdispersion for :gammaPoisson) are computed internally by maximum likelihood. Inspect them using <em>HTBcoeff()</em>. In <em>HTBpredict()</em>, predictions are for E(y) if predict=:Ey (default), while predict=:Egamma forecasts the fitted parameter ( E(logit(prob) for :logistic, log(E(y)) for :gamma etc ... )</p></li><li><p><code>modality</code>         [:compromise] Options are: :accurate, :compromise (default), :fast, :fastest.                    These options are meant to replace the need for the user to cross-validate parameters. Advanced users with a big computational budget can still do so.                    :fast and :fastest run only one model, while :compromise and :accurate cross-validate the most important parameters.                    :fast runs only one model (only cv number of trees) at values defined in param = HTBparam().                     :fastest runs only one model, setting lambda=0.2, nfold=1 and nofullsample=true (does not re-estimate on the full sample after cv).                     Recommended for faster preliminary analysis only.                     In most cases, :fast and :fastest also use the quadratic approximation to the loss for large samples.                     :compromise and :accurate cross-validates several models at the most important parameters (see HTBfit() for details),                     then stack all the cv models.</p></li><li><p><code>priortype</code>               [:hybrid] :hybrid encourages smoothness, but allows both smooth and sharp splits, :smooth forces smooth splits,                           :disperse is :hybrid but with no penalization encouraging smooth functions (not recommended).                           Set to :smooth if you want to force derivatives to be defined everywhere. </p></li><li><p><code>randomizecv</code>       [false] default is block-cv (aka purged cv); a time series or panel structure is automatically detected (see HTBdata)                           if a date column is provided. Set to true for standard cv.</p></li><li><p><code>nfold</code>              [4] n in n-fold cv. Set nfold = 1 for a single validation set (by default the last param.sharevalidation share of the sample).                           nfold, sharevalidation, and randomizecv are disregarded if train and test observations are provided by the user.</p></li><li><p><code>sharevalidation:</code>        [0.30] Can be: a) Integer, size of the validation set, or b) Float, share of validation set.                           Relevant only if nfold = 1.</p></li><li><p><code>indtrain_a:Vector{Vector{I}}</code> [] for user&#39;s provided array of indices of train sets. e.g. vector of 5 vectors, each with indices of train set observations</p></li><li><p><code>indtest_a:Vector{Vector{I}}</code>  [] for user&#39;s provided array of indices of test sets. e.g. vector of 5 vectors, each with indices of train set observations</p></li><li><p><code>nofullsample</code>      [false] if true and nfold=1, HTBoost is not re-estimated on the full sample after validation.                           Reduces computing time by roughly 60%, at the cost of a modest loss of accuracy.                           Useful for very large datasets, in preliminary analysis, in simulations, and when instructions specify a train/validation                           split with no re-estimation on full sample. Activated by default when modality=:fastest.     </p></li><li><p><code>cat_features</code>            [] vector of indices of categorical features, e.g. [2] or [2,5], or vector of names in DataFrame,                           e.g. [:wage,:age] or [&quot;wage&quot;,&quot;age&quot;]. If empty, categoricals are automatically detected as non-numerical features.</p></li><li><p><code>cv_categoricals</code>     [:default] whether to run preliminary cross-validation on parameters related to categorical features.                       :none uses default parameters                        :penalty runs a rough cv the penalty associated to the number of categories; recommended if n/n_cat if low for any feature, particularly if SNR is low                                                    :n0 runs a rough of cv the strength of the prior shrinking categorical values to the overall mean; recommended with highly unequal number of observations in different categories                       :both runs a rough cv or penalty and n0                        :default uses :none for modality in [:fastest,:fast], :penalty for :compromise, and :both for :accurate        </p></li><li><p><code>overlap:</code>            [0] number of overlaps in time series and panels. Typically overlap = h-1, where y(t) = Y(t+h)-Y(t). Used for purged-CV.</p></li><li><p><code>verbose</code>         [:Off] verbosity :On or :Off</p></li><li><p><code>warnings</code>        [:On] or :Off</p></li></ul><p><strong>Parameters that may sometimes be be modified by user</strong></p><ul><li><p><code>lambda</code>           [0.1 or 0.2] Learning rate. 0.1 for (nearly) best performance. 0.2 can be almost as accurate, particularly if the function is smooth and p is small.                    The default is 0.1, except in modality = :fastest, where it&#39;s 0.2. Modality = :compromise carries out the cv at lambda=0.2 and then fits the best model at 0.1.                    Consider 0.05 if tiny improvements in accuracy are important and computing time is not a concern.</p></li><li><p><code>depth</code>              [5] tree depth. Unless modality = :fast or :fastest, this is over-written as depth is cross-validated. See HTBfit() for more options.</p></li><li><p><code>weights</code>                 NOTE: weights for weighted likelihood are set in HTBdata, not in HTBparam.</p></li><li><p><code>offset</code>                  NOTE: offsets (aka exposures) are set in HTBdata, not in HTBparam. See examples/Offset or exposure.jl     </p></li><li><p><code>sparsity_penalization</code>   [0.3] positive numbers encourage sparsity. The range [0.0-1.5] should cover most scenarios.                            Automatically cv in modality=:compromise and :accurate. Increase to obtain a more parsimonious model, set to 0 for standard boosting.</p></li><li><p><code>ntrees</code>             [3000] Maximum number of trees. HTBfit will automatically stop when cv loss stops decreasing.</p></li><li><p><code>sharevs</code>                 [1.0] row subsampling in variable selection phase (only to choose feature on which to split.) Default is no subsampling.                           sharevs = :Auto sets the subsample size to min(n,50k*sqrt(n/50k)).                           At high n, sharevs&lt;1 speeds up computations, but can reduce accuracy, particularly in sparse setting with low SNR.         </p></li><li><p><code>subsampleshare_columns</code>  [1.0] column subsampling (aka feature subsampling) by level.</p></li><li><p><code>min_unique</code>              [:default] sharp splits are imposed on features with less than min_unique values (default is 5 for modality=:compromise or :accurate, else 10)</p></li><li><p><code>mixed_dc_sharp</code>          [false] true to force sharp splits on discrete and mixed discrete-continuous features (defined as having over 20% obs on a single value)</p></li><li><p><code>delete_missing</code>          [false] true to delete rows with missing values in any feature, false to handle missing internally (recommended).</p></li><li><p><code>theta</code>                   [1]  numbers larger than 1 imply tighter penalization on β (final leaf values) compared to default.</p></li><li><p><code>meanlntau</code>               [1.0] prior mean of log(τ). Set to higher numbers to suggest less smooth functions.        </p></li><li><p><code>mugridpoints</code>       [11] number of points at which to evaluate μ during variable selection. 5 is sufficient on simulated data with normal or uniform distributions, but actual data may benefit from more (due to with highly non-Gaussian features).                           For extremely complex and nonlinear features, more than 10 may be needed.        </p></li><li><p><code>force_sharp_splits</code>      [] optionally, a p vector of Bool, with j-th value set to true if the j-th feature is forced to enter with a sharp split.</p></li><li><p><code>force_smooth_splits</code>     [] optionally, a p vector of Bool, with j-th value set to true if the j-th feature is forced to enter with a smooth split (high values of τ not allowed).</p></li><li><p><code>cat_representation_dimension</code>  [4 (2 for classification)] 1 for mean encoding, 2 adds frequency, 3 adds variance, 4 adds robust skewness, 5 adds robust kurtosis</p></li><li><p><code>losscv</code>                  [:default] loss function for cross-validation (:mse,:mae,:logistic,:sign). </p></li><li><p><code>n_refineOptim</code>      [10^6] MAXIMUM number of observations to use fit μ and τ (split point and smoothness).                           Lower numbers can provide speed-ups with very large n at some cost in terms of fit.</p></li><li><p><code>loglikdivide</code>         [1.0] Higher numbers increase the strength or all priors. The defaults sets it internally using HTBloglikdivide(),                           when it detects a dates series in HTBdata().</p></li><li><p><code>tau_threshold</code>         [10.0] lowest threshold for imposing sharp splits. Lower numbers give more sharp splits.</p></li><li><p><code>multiplier_stdtau</code>    [5.0] The default priors suggest smoother splits on features whose unconditional distribution (appropriately transformed according to the link function) is closer to the unconditional distribution of <em>y</em> or, when not applicable, to a Gaussian. To disengage this feature, set <em>multiplier_stdtau</em> = 0</p></li></ul><p><strong>Additional parameters to control the cross-validation process can be set in HTBfit(), but keeping the defaults is generally encouraged.</strong></p><p><strong>Example</strong></p><pre><code class="nohighlight hljs">param = HTBparam()</code></pre><p><strong>Example</strong></p><pre><code class="nohighlight hljs">param = HTBparam(nfold=1,nofullsample=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBdata" href="#HybridTreeBoosting.HTBdata"><code>HybridTreeBoosting.HTBdata</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">    HTBdata(y,x,param,[dates];weights=[],fnames=[],offset=[])</code></pre><p>Collects and pre-processes data in preparation for fitting HTBoost</p><p><strong>Inputs</strong></p><ul><li><code>y::Vector</code>              Vector of responses. Can be a vector of lables, or a dataframe with one column. </li><li><code>x</code>                      Matrix of features. Can be a vector or matrix of floats, or a dataframe. Converted internally to a Matrix{T}, T as defined in HTBparam</li><li><code>param::HTBparam</code></li></ul><p><strong>Optional Inputs</strong></p><ul><li><code>dates::AbstractVector</code>  [1:n] Typically Vector{Date} or Vector{Int}. Used in cross-validation to determine splits.                           If not supplied, the default 1:n assumes a cross-section of independent realizations (conditional on x) or a single time series.</li><li>&#39;weights&#39;                [ones(T,n)] vector of floats or Floats, weights for weighted likelhood</li><li><code>fnames::Vector{String}</code> [x1, x2, ... ] feature names</li><li>&#39;offset&#39;                 vector of offsets (exposure), in logs if the loss adopts a loss link (:gamma,:gammaPoisson,:L2loglink,....) </li></ul><p><strong>Examples of use</strong></p><pre><code class="nohighlight hljs">data = HTBdata(y,x,param)
data = HTBdata(y,x,param,dates,fnames=names)
data = HTBdata(y,df[:,[:CAPE, :momentum ]],param,df.dates,fnames=df.names)
data = HTBdata(y,df[:,3:end],param)</code></pre><p><strong>Notes</strong></p><ul><li>When dates are provided, the data will be ordered chronologically (for cross-validation functions) unless the user has provided explicit training and validation sets.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBindexes_from_dates" href="#HybridTreeBoosting.HTBindexes_from_dates"><code>HybridTreeBoosting.HTBindexes_from_dates</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>HTBindexes<em>from</em>dates(df::DataFrame,datesymbol::Symbol,first<em>date::Date,n</em>reestimate::Int)</p><p>Computes indexes of training set and test set for cumulative CV and pseudo-real-time forecasting exercises in time series and panel data. The function is inefficient for large datasets, but does not require the data to be sorted.</p><ul><li><p>INPUTS</p></li><li><p>df                    DataFrame with dates and other variables</p></li><li><p>datesymbol            symbol (or string) name of the date</p></li><li><p>first_date            when the first training set ENDS (end date of the first training set)</p></li><li><p>n_reestimate          every how many periods to re-estimate (update the training set)</p></li><li><p>OUTPUT indtrain<em>a,indtest</em>a are arrays of arrays of indexes of train and test samples</p></li><li><p>Example of use</p><p>first<em>date = Date(&quot;2017-12-31&quot;, Dates.DateFormat(&quot;y-m-d&quot;))   indtrain</em>a,indtest<em>a = HTBindexes</em>from<em>dates(df,:date,first</em>date,12)</p><p>or as a named tuple with elements indtrain<em>a,indtest</em>a)</p><p>t = HTBindexes<em>from</em>dates(df,:date,first_date,12)</p></li><li><p>NOTES</p></li><li><p>Inefficient for large datasets</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><h2 id="Fit-and-predict"><a class="docs-heading-anchor" href="#Fit-and-predict">Fit and predict</a><a id="Fit-and-predict-1"></a><a class="docs-heading-anchor-permalink" href="#Fit-and-predict" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBfit" href="#HybridTreeBoosting.HTBfit"><code>HybridTreeBoosting.HTBfit</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBfit(data,param;cv_grid=[],cv_sparsity=:Auto,cv_depthppr=:Auto)</code></pre><p>Fits HTBoost with with k-fold cross-validation of number of trees and depth, and possibly a few more models.</p><p>If param.modality is :fast or :fastest, fits one model, at param, and if needed a second where sharp splits are  forced on features with high average values of τ. For param.modality=:accurate or :compromise,   may then cross-validate the following hyperparamters:</p><ol><li>Parameters for categorical features, if any.</li><li>depth in the range 1-6 (1-7 for :accurate) (starts with 3 and 5, then moves up or down based on results, breaking the loop as soon as the cv loss increases)</li><li>A penalization to encourage sparsity (fewer relevant features). Whether this is performed or not depends on the effective sample size (n and signal-to-noise ratio).</li><li>A model without projection pursuit nonlinear expansion of trees (only if modality=:accurate). </li><li>One or two models with column subsampling and slightly lower learning rate. One in :compromise, two in :accurate if the first reduced the cv loss. </li></ol><p>A maximum of 10 models are fitted in modality = :accurate, and of 7 in :compromise.</p><p>The default range for depth can be replaced by providing a vector cv_grid, e.g. </p><pre><code class="nohighlight hljs">HTBfit(data,param,cv_grid = [2,4,6])</code></pre><p>The sparsity penalization can be de-activated by setting cv_sparsity=false, e.g. </p><pre><code class="nohighlight hljs">    HTBfit(data,param,cv_sparsity=false)</code></pre><p>If param.modality=:accurate, the learning rate lambda for all models is left at param.lambda (0.1 in default). If modality=:compromise, lambda=0.2 is used in cv, and the best model is then refitted with lambda = param.lambda. </p><p>Finally, all the estimated models considered are stacked, with weights chosen to minimize the cross-validated (original) loss.    Unless modality=:accurate, this stacking will typically be equivalent to the best model.</p><p><strong>Inputs</strong></p><ul><li><code>data::HTBdata</code></li><li><code>param::HTBparam</code></li></ul><p><strong>Optional inputs</strong></p><ul><li><p><code>cv_grid::Vector</code>         Defaul [2,3,5,6]. The code performs a search in the space depth in [2,3,5,6], trying to fit few models if possible. Provide a                           vector to over-ride (e.g. [2,4])</p></li><li><p><code>cv_sparsity</code>             Default :Auto. Set cv<em>sparsity = true to guarantee search over sparsity penalization or cv</em>sparsity=false to disactivate (and save computing time.)                           In :Auto, whether the cv is performed or not depends on :modality and on the n/p ratio. (Not implemented yet: it should ideally also depend on the signal-to-noise ratio). </p></li><li><p><code>cv_depthppr</code>             true to cv whether to remove projection pursuit regression. Default is true for modality in [:compromise,:accurate], else false.</p></li><li><p><code>cv_col_subsample</code>        Default:Auto. If true, fits one or two models with column subsampling. </p></li></ul><p><strong>Output (named tuple, or vector of named tuple for hurdle models)</strong></p><ul><li><code>indtest::Vector{Vector{Int}}</code>  indexes of validation samples</li><li><code>bestvalue::Float</code>              best value of depth in cv_grid</li><li><code>bestparam::SAMRTparam</code>         param for best model  </li><li><code>ntrees::Int</code>                   number of trees (best value of param.ntrees) for best model</li><li><code>loss::Float</code>                   best cv loss</li><li><code>lossw::Float</code>                  loss of stacked models</li><li><code>meanloss:Vector{Float}</code>        mean cv loss at bestvalue of param for param.ntrees = 1,2,....</li><li><code>stdeloss:Vector{Float}</code>        standard errror of cv loss at bestvalue of param for param.ntrees = 1,2,....</li><li><code>lossgrid::Vector{Float}</code>       cv loss for best tree size for each grid value </li><li><code>loglikdivide:Float</code>            loglikdivide. effective sample size = n/loglikvide. Roughly accounts for cross-correlation and serial correlation </li><li><code>HTBtrees::HTBoostTrees</code>        for the best cv value of param and ntrees</li><li><code>HTBtrees_a</code>                    length(cv_grid) vector of HTBtrees</li><li><code>i</code>                             vector (ntrees) of vectors (depth of each tree) of threshold features for best model</li><li><code>mu</code>                            vector (ntrees) of vectors (depth of each tree) of threshold points  for best model</li><li><code>tau</code>                           vector (ntrees) of vectors (depth of each tree) of sigmoid parameters for best model</li><li><code>fi2</code>                           vector (ntrees) of vectors (depth of each tree) of feature importance, increase in R2 at each split, for best model</li><li><code>w</code>                             length(cv_grid) vector of stacked weights</li><li><code>ratio_actual_max</code>              ratio of actual number of candidate features over potential maximum. Relevant if sparsevs=:On: indicates sparsevs should be switched off if too high (e.g. higher than 0.5).</li><li><code>problems</code>                      true if there were computational problems in any of the models: NaN loss or loss jumping up</li></ul><p><strong>Notes</strong></p><ul><li>The following options for cross-validation are specified in param: randomizecv, nfold, sharevalidation, stderulestop</li></ul><p><strong>Examples of use:</strong></p><pre><code class="nohighlight hljs">param = HTBparam()
data   = HTBdata(y,x,param)
output = HTBfit(data,param)
ntrees = output.ntrees 
best_depth = output.bestvalue 

Example for hudle models (loss in [:hurdleGamma,:hurdleL2loglink,:hurdleL2])
ntrees_0    = output[1].ntrees   # number of trees for logistic regression, 0-not0
ntrees_not0 = output[2].ntrees   # number of trees for gamma or L2 loss</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBpredict" href="#HybridTreeBoosting.HTBpredict"><code>HybridTreeBoosting.HTBpredict</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBpredict(x,output)</code></pre><p>Forecasts from HTBoost, for y or the natural parameter.</p><p><strong>Inputs</strong></p><ul><li><code>x</code>                           (n,p) DataFrame or Float matrix of forecast origins (type&lt;:real) or p vector of forecast origin                               In the same format as the x given as input is HTBdata(y,x,...). May contain missing or NaN.</li><li><code>output</code>                      output from HTBfit</li></ul><p><strong>Optional inputs</strong></p><ul><li><code>predict</code>                    [:Ey], :Ey or :Egamma. :Ey returns the forecast of y, :Egamma returns the forecast of the natural parameter                              (e.g. logit(prob) for :logistic and E(log(y)|x) for :lognormal).</li><li><code>best_model</code>                 [false] true to use only the single best model, false to use stacked weighted average</li><li><code>offset</code>                     (n) vector, offset (or exposure), in terms of gamma (log exposure if the loss has a log-link)     </li><li><code>cutoff_paralellel</code>          [20_000] if x has more than these rows, a parallellized algorithm is called (which is slower for few forecasts)</li></ul><p><strong>Output for standard models</strong></p><ul><li><code>yf</code>                         (n) vector of forecasts of y (or, outside regression, of the natural parameter), or scalar forecast if n = 1</li></ul><p><strong>Output for hurdle models</strong></p><ul><li><code>yf</code>                         (n) vector of forecasts of E(y|x) for the combined model</li><li><code>prob0</code>                      (n) vector of forecasts of prob(y=0|x)</li><li><code>yf_not0</code>                    (n) vector of forecasts of E(y|x,y /=0)</li></ul><p><strong>Output for loss = :multiclass</strong></p><ul><li><code>yf</code>                        (n,num_class) matrix, yf[i,j] if the probability that observation i belongs to class j</li><li><code>class_values</code>              (num<em>clas) vector, class</em>value[j] is the value of y associated with class j</li><li><code>ymax</code>                      (n) vector, ymax[i] is the class value with highest probability at observation i. </li></ul><p><strong>Examples of use</strong></p><pre><code class="nohighlight hljs">output = HTBfit(data,param)
yf     = HTBpredict(x_oos,output)
yf     = HTBpredict(x_oos,output,best_model=true)
yf     = HTBpredict(x_oos,output,offset = log.(exposure) )

yf,prob0,yf_not0 = HTBpredict(x_oos,output)  # for hurdle models. Or as a tuple t = HTBpredict(x_oos,output) 
yf,class_value,ymax = HTBpredict(x_oos,output)  # for multiclass. Or as a tuple t = HTBpredict(x_oos,output)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBcv" href="#HybridTreeBoosting.HTBcv"><code>HybridTreeBoosting.HTBcv</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBcv(data,param,params_cv;internal_cv=true)</code></pre><p>User&#39;s controlled cross-validation for HTBoost.</p><p>The recommended process for HTBoost is to use <em>modality</em> (:fastest, :fast, :compromise, :accurate) in HTBfit() rather than HTBcv(). The various modalities in HTBfit() internally control the most important hyperparameters, being as parsimonious as possible due  to the high computational costs of HTB. HTBfit() also stacks the models, which is not done here. All modalities use early stopping to determine the number of trees, so ntrees should not be cv. </p><p>The function HTBcv() is provided for advanced users who want to fully control the cross-validation process and can incur the costs. HTBcv() has the option to set internal_cv=true, in which case cv is also performed internally (at increase cost).</p><p>A good use of HTBcv() is to cv a parameter that is not included in the modality (e.g. the strength of the smoothness prior), in conjunction with internal_cv = true. (At increase computational cost.)</p><p><strong>Inputs</strong></p><ul><li><code>data</code>                    HTBdata type </li><li><code>param</code>                   HTBparam type. Includes number of folds, randomization (or block-cv) and, optionally, indexes of train and validation folds</li><li><code>params_cv</code>               Array of Dictionaries. Each dictionary contains the hyperparameters to be cv&#39;ed (keys) and the values to be cv&#39;ed (values). </li></ul><p><strong>Optional Inputs</strong></p><ul><li><code>internal_cv</code>             [true] If true, it will perform internal cv (as dictated by param.modality) for each element of the dictionary.                           This could be very slow. Set to false if you want to fully control the cv process.                           NOTE: if false, param.modality is irrelevant (set to :fast)</li></ul><p><strong>Output (named tuple)</strong></p><ul><li><code>bestindex</code>               Index of the best model in params_cv </li><li><code>bestparam</code>               param (type HTBparam) of the best model</li><li><code>output</code>                  output of the best model</li><li><code>loss</code>                    loss of the best model</li><li><code>output_a</code>                Array of outputs for all models</li><li><code>loss_a</code>                  Array of losses for all models</li></ul><p><strong>Example of use: cv over varlntau (strength of smoothness prior), and also cv internally.</strong></p><pre><code class="nohighlight hljs"># Set up model and data 
param  = HTBparam(loss=:L2,modality=:accurate,nfold=4)  # number of folds, randomization (or block-cv) and, optionally, indexes of train and validation folds
data   = HTBdata(x,y,param)

# Specify hyperparameters and values for cv as a Dictionary. Here we cv only varlntau (strength of smoothness prior).
# The resuls is a one-dimensional array. More dimensions can be added in the same way. 
params_cv = [Dict(
    :varlntau =&gt; varlntau)    # strength of prior on log(tau) (smoothness). Default is 0.5^2. Smaller numbers are stronger priors.
    for
    varlntau in (0.25^2,0.5^1,1.0^2)
    ]

htbcv = HTBcv(data,param,params_cv)      # cv over dictionary and internally

# Some info about the best model 
bestindex = htbcv.bestindex   # params_cv[bestindex] is for best set of cv hyperparameters       
bestparam = htbcv.bestparam   

# predict using best model
yf    = HTBpredict(x_oos,htbcv.output)</code></pre><p><strong>Example of use: replace HTBfit() (not recommended) by setting internal_cv=false</strong></p><pre><code class="nohighlight hljs"># Set up model and data 
param  = HTBparam(loss=:L2,randomizecv=false,nfold=4)  # number of folds, randomization (or block-cv) and, optionally, indexes of train and validation folds
data   = HTBdata(x,y,param)

# Specify hyperparameters and values for cv as a Dictionary. Here we cv depth and varlntau (strength of smoothness prior).
# The resuls is a two-dimensional array. More dimensions can be added in the same way. 
params_cv = [Dict(
    :depth =&gt; depth,
    :varlntau =&gt; varlntau)    # strength of prior on log(tau) (smoothness). Default is 0.5^2. Smaller numbers are stronger priors.
    for
    depth in (2,4,6),
    varlntau in (0.25^2,0.5^1,1.0^2)
    ]

htbcv = HTBcv(data,param,params_cv,internal_cv=false)       

# Some info about the best model 
bestindex = htbcv.bestindex   # params_cv[bestindex] is for best set of cv hyperparameters       
bestparam = htbcv.bestparam   
output    = htbcv.output

# predict using best model
yf    = HTBpredict(x_oos,htbcv.output)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><h2 id="Post-estimation-analysis"><a class="docs-heading-anchor" href="#Post-estimation-analysis">Post-estimation analysis</a><a id="Post-estimation-analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Post-estimation-analysis" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBcoeff" href="#HybridTreeBoosting.HTBcoeff"><code>HybridTreeBoosting.HTBcoeff</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBcoeff(output;verbose=true)</code></pre><p>Provides some information on constant coefficients for best model (in the form of a tuple.) For example, error variance for :L2, dispersion and dof for :t.</p><p><strong>Inputs</strong></p><ul><li><code>output</code>                      output from HTBfit</li></ul><p><strong>Output</strong></p><ul><li><code>coeff</code>                      named tuple with information on fixed coefficients (e.g. variance for :L2, dispersion and dof for :t)</li></ul><p><strong>Example of use</strong></p><pre><code class="nohighlight hljs">output = HTBfit(data,param)
coeff  = HTBcoeff(output,verbose=false)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBrelevance" href="#HybridTreeBoosting.HTBrelevance"><code>HybridTreeBoosting.HTBrelevance</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBrelevance(output,data::HTBdata;verbose=true,best_model=false)</code></pre><p>Computes feature importance (summing to 100), defined by the relevance measure of Breiman et al. (1984), equation 10.42 in Hastie et al., &quot;The Elements of Statistical Learning&quot;, second edition, except that the normalization is for sum = 100, not for largest = 100. Relevance is defined on the fit of the trees on pseudo-residuals. best<em>model=true for single model with lowest CV loss, best</em>model= false for weighted average (weights optimized by stacking)</p><p><strong>Output</strong></p><ul><li><code>fnames::Vector{String}</code>         feature names, same order as in data</li><li><code>fi::Vector{Float}</code>              feature importance, same order as in data</li><li><code>fnames_sorted::Vector{String}</code>  feature names, sorted from highest to lowest importance</li><li><code>fi_sorted::Vector{Float}</code>       feature importance, sorted from highest to lowest</li><li><code>sortedindx::Vector{Int}</code>        feature indices, sorted from highest to lowest importance</li></ul><p><strong>Example of use</strong></p><pre><code class="nohighlight hljs">output = HTBfit(data,param)
fnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data,verbose = false)
fnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output,data,best_model=true)

or as a named tuple 

t = HTBrelevance(output,data,verbose = false)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBpartialplot" href="#HybridTreeBoosting.HTBpartialplot"><code>HybridTreeBoosting.HTBpartialplot</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBpartialplot(data::HTBdata,output,features::Vector{Int64};,predict=:Egamma,best_model=false,other_xs::Vector=[],q1st=0.01,npoints=1000))</code></pre><p>Partial dependence plot for selected features. Notice: Default is for natural parameter (gamma) rather than y. For feature i, computes gamma(x<em>i) - gamma(x</em>i=mean(x<em>i)) for x</em>i between q1st and 1-q1st quantile, with all other features at their mean.</p><p><strong>Inputs</strong></p><ul><li><code>data::HTBdata</code></li><li><code>output</code></li><li><code>features::Vector{Int}</code>        position index (in data.x) of features to compute partial dependence plot for</li><li><code>other_xs::Vector{Float}</code>      (keyword), a size(data.x)[2] vector of values at which to evaluate the responses. []                                Note: other<em>xs should be expressed in standardized units, i.e. for (x</em>i-mean(x<em>i))/std(x</em>i)   </li><li><code>q1st::Float</code>                  (keyword) first quantile to compute, e.g. 0.001. Last quantile is 1-q1st. [0.01]</li><li>`npoints::Int&#39;                 (keyword) number of points at which to evalute f(x). [1000]</li></ul><p><strong>Optional inputs</strong></p><ul><li><code>predict</code>                    [:Egamma], :Ey or :Egamma. :Ey returns the impact on the forecast of y, :Egamma on the natural parameter.  </li><li><code>best_model</code>                 [false]  true for single model with lowest CV loss, =false for weighted average (by stacking)</li></ul><p><strong>Output</strong></p><ul><li><code>q::Matrix</code>                   (npoints,length(features)), values of x<em>i at which f(x</em>i) is evaluated</li><li><code>pdp::Matrix</code>                 (npoints,length(features)), values of f(x_i)</li></ul><p><strong>Example of use</strong></p><pre><code class="nohighlight hljs">output = HTBfit(data,param)
q,pdp  = HTBpartialplot(data,output.HTBtrees,sortedindx[1,2],q1st=0.001)

or as a named tuple 

t      = HTBpartialplot(data,output.HTBtrees,sortedindx[1,2],q1st=0.001)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBmarginaleffect" href="#HybridTreeBoosting.HTBmarginaleffect"><code>HybridTreeBoosting.HTBmarginaleffect</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBmarginaleffect(data::HTBdata,output,features::Vector{Int64};predict=:Egamma,best_model=false,other_xs::Vector =[],q1st=0.01,npoints=50,epsilon=0.02)</code></pre><p>APPROXIMATE Computation of marginal effects using NUMERICAL derivatives (default ϵ=0.01)</p><p><strong>Inputs</strong></p><ul><li><code>data::HTBdata</code></li><li><code>HTBtrees::HTBoostTrees</code></li><li><code>features::Vector{Int}</code>        position index (in data.x) of features to compute partial dependence plot for</li><li><code>other_xs::Vector{Float}</code>      (keyword), a size(data.x)[1] vector of values at which to evaluate the marginal effect. []                                Note: other<em>xs should be expressed in standardized units, i.e. for (x</em>i-mean(x<em>i))/std(x</em>i)   </li><li><code>q1st::Float</code>                  (keyword) first quantile to compute, e.g. 0.001. Last quantile is 1-q1st. [0.01]</li><li>`npoints::Int&#39;                 (keyword) number of points at which to evalute df(x<em>i)/dx</em>i. [50]</li><li>`epsilon::Float&#39;               (keyword) epsilon for numerical derivative, [0.01]</li></ul><p><strong>Optional inputs</strong></p><ul><li><code>predict</code>                     [:Egamma], :Ey or :Egamma. :Ey returns the impact on the forecast of y, :Egamma on the natural parameter.  </li><li><code>best_model</code>                  [false]  true for single model with lowest CV loss, =false for weighted average (by stacking)</li></ul><p><strong>Output</strong></p><ul><li><code>q::Matrix</code>                   (npoints,length(features)), values of x<em>i at which f(x</em>i) is evaluated, or vector if npoints = 1</li><li><code>d::Matrix</code>                   (npoints,length(features)), values of marginal effects, or vector if npoints = 1</li></ul><p><strong>NOTE: Provisional! APPROXIMATE Computation of marginal effects using NUMERICAL derivatives. (Analytical derivatives are available)</strong></p><p><strong>NOTE: To compute marginal effect at one point x0 rather than over a grid, set npoints = 1 and other_xs = x0 (a p vector, p the number of features)</strong></p><p><strong>Example</strong></p><pre><code class="nohighlight hljs">output = HTBfit(data,param)
q,m    = HTBmarginaleffect(data,output.HTBtrees,[1,3])
t      = HTBmarginaleffect(data,output.HTBtrees,[1,3])   # as named tuple</code></pre><p><strong>Example</strong></p><pre><code class="nohighlight hljs">q,m  = HTBmarginaleffect(data,output.HTBtrees,[1,2,3,4],other_xs = zeros(p),npoints = 1)</code></pre><p><strong>Example</strong></p><pre><code class="nohighlight hljs">output = HTBfit(data,param)
fnames,fi,fnames_sorted,fi_sorted,sortedindx = HTBrelevance(output.HTBtrees,data,verbose=false)
q,m  = HTBmarginaleffect(data,output,sortedindx[1,2],q1st=0.001)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBoutput" href="#HybridTreeBoosting.HTBoutput"><code>HybridTreeBoosting.HTBoutput</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBoutput(HTBtrees::HTBoostTrees;exclude_pp = true)</code></pre><p>Output fitted parameters estimated from each tree, collected in matrices. Excluded projection pursuit regression parameters.</p><p><strong>Inputs</strong></p><ul><li>The default exclude_pp does not give μ and τ for projection pursuit regression. </li></ul><p><strong>Output</strong></p><ul><li><code>i</code>         vector (ntrees) of vectors (depth of tree i) of threshold features</li><li><code>μ</code>         vector (ntrees) of vectors (depth of tree i) of threshold points</li><li><code>τ</code>         vector (ntrees) of vectors (depth of tree i) of smoothing parameters</li><li><code>fi2</code>       vector (ntrees) of vectors (depth of tree i) of feature importance, increase in R2 at each split</li><li><code>m</code>         vector (ntrees) of vectors (depth of tree i) of threshold points for missing values </li><li><code>β</code>         vector (ntrees) of vectors (2^depth of tree i) of leaf coefficients (1st phase, excluding ppr)</li></ul><p><strong>Example of use</strong></p><pre><code class="nohighlight hljs">output = HTBfit(data,param)
i,μ,τ,fi2,m,β = HTBoutput(output.HTBtrees)
t = HTBoutput(output.HTBtrees)  # named tuple</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBweightedtau" href="#HybridTreeBoosting.HTBweightedtau"><code>HybridTreeBoosting.HTBweightedtau</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBweightedtau(output,data;verbose=true,best_model=false)</code></pre><p>Computes weighted (by variance importance gain at each split) smoothing parameter τ for each feature, and for the entire model (features are averaged by variance importance) statistics for each feature, averaged over all trees. Sharp thresholds (τ=Inf) are bounded at 40. best<em>model=true for single model with lowest CV loss, best</em>model= false for weighted average (weights optimized by stacking)</p><p><strong>Input</strong></p><ul><li><code>output</code>   output from HTBfit</li><li><code>data</code>     data input to HTBfit</li></ul><p><strong>Optional inputs</strong></p><ul><li><code>verbose</code>   [true]  prints out the results to screen as DataFrame</li><li><code>max_tau</code>   [40]    values of τ at or above this are set to Inf (hard splits) for plotting purposes</li></ul><p><strong>Output. Named tuple with the following fields</strong></p><ul><li><code>avgtau</code>         scalar, average importance weighted τ over all features (also weighted by variance importance) </li><li><code>gavgtau</code>        scalar, geometric average of importance weighted log τ over all features (also weighted by variance importance).                  NB: Default in printed output and plot. (Arguably the more informative measure.) </li><li><code>avgtau_a</code>       p-vector of avg importance weighted τ for each feature </li><li><code>df</code>             dataframe collecting avgtau_a information (only if verbose=true)</li><li><code>x_plot</code>         x-axis to plot sigmoid for gavgtau, in range [-2 2] for standardized feature </li><li><code>g_plot</code>         y-axis to plot sigmoid for gavgtau </li><li><code>fnames</code>         feature names</li><li><code>fi</code>             feature importance, summing to 100</li><li><code>fnames_sorted</code>  sorted feature names, from highest to lowest importance</li><li><code>fi_sorted</code>      sorted feature importance</li><li><code>sortedindx</code>     sorted feature indices</li></ul><p><strong>Example of use</strong></p><pre><code class="nohighlight hljs">output = HTBfit(data,param)
avgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data)
avgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data,verbose=false,plot_tau=false,best_model=true)

using Plots
plot(x_plot,g_plot,title=&quot;avg smoothness of splits&quot;,xlabel=&quot;standardized x&quot;,label=:none,legend=:bottomright)    

Or as a named tuple 

t = HTBweightedtau(output,data)
plot(t.x_plot,t.g_plot,title=&quot;avg smoothness of splits&quot;,xlabel=&quot;standardized x&quot;,label=:none,legend=:bottomright)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBplot_tau" href="#HybridTreeBoosting.HTBplot_tau"><code>HybridTreeBoosting.HTBplot_tau</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBplot_tau(tau;mu=0,sigmoid=:sigmoidsqrt,range=2)</code></pre><p>Produces x and y to plot sigmoid function for a given τ (typically the average τ for a feature).  </p><p><strong>Input</strong></p><ul><li><code>tau</code>                       smoothing parameter τ</li></ul><p><strong>Optional inputs</strong></p><ul><li><code>mu</code>        [0]             location parameter for sigmoid</li><li><code>sigmoid</code>   [sigmoidsqrt]   :sigmoidsqrt or :sigmoidlogistic</li><li><code>range</code>     [2]             x range for plot, [-range range]. x is standardized</li><li><code>max_tau</code>   [40]            tau at which a hard split is assumed</li></ul><p><strong>Output</strong></p><ul><li><code>x_plot</code>         x-axis to plot sigmoid for tau, in range [-2 2] for standardized feature </li><li><code>g_plot</code>         y-axis to plot sigmoid for tau </li></ul><p><strong>Note</strong></p><ul><li>tau ≥ 40 is interpreted as a hard split. </li></ul><p><strong>Example of use</strong></p><pre><code class="nohighlight hljs">output = HTBfit(data,param)
avgtau,gavgtau,avgtau_a,dftau,x_plot,g_plot = HTBweightedtau(output,data)
x_plot,g_plot = HTBplot_tau(avgtau[1])     # tau of first feature
using Plots
plot(x_plot,g_plot,title=&quot;avg tau of feature 1&quot;,xlabel=&quot;standardized x&quot;,label=:none,legend=:bottomright)

or as a named tuple

t = HTBplot_tau(avgtau[1])     # tau of first feature
plot(t.x_plot,t.g_plot,title=&quot;avg tau of feature 1&quot;,xlabel=&quot;standardized x&quot;,label=:none,legend=:bottomright)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HybridTreeBoosting.HTBplot_ppr" href="#HybridTreeBoosting.HTBplot_ppr"><code>HybridTreeBoosting.HTBplot_ppr</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HTBplot_ppr(output;which_tree=1)</code></pre><p>Visualize impact of projection pursuit for an individual tree.</p><p><strong>Example of use</strong></p><pre><code class="nohighlight hljs">yf1,yf0,tau = HTBplot_ppr(output,which_tree=1)
plot(yf0,yf1)

where yf0 is the standardized prediction from the tree, and yf1 is the (non-standardized) prediction after ppr</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PaoloGiordani/HybridTreeBoosting.jl.git">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Parameters/">« Parameters</a><a class="docs-footer-nextpage" href="../Tutorials/">Tutorials (Julia) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.10.2 on <span class="colophon-date" title="Wednesday 30 April 2025 10:13">Wednesday 30 April 2025</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
